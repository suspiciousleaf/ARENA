# %%
import os
import sys
import math
import numpy as np
import einops
import torch as t
from pathlib import Path

# Make sure exercises are in the path
chapter = r"chapter0_fundamentals"
exercises_dir = Path(f"{os.getcwd().split(chapter)[0]}/{chapter}/exercises").resolve()
section_dir = exercises_dir / "part0_prereqs"
if str(exercises_dir) not in sys.path:
    sys.path.append(str(exercises_dir))

from plotly_utils import imshow, line, bar
from part0_prereqs.utils import display_array_as_img
import part0_prereqs.tests as tests

MAIN = __name__ == "__main__"
# %%
arr = np.load(section_dir / "numbers.npy")
# %%
import utils

for img in arr:
    utils.display_array_as_img(img)

# %%
# Exercise 1: Display concatenated images.

arr1 = einops.rearrange(arr, "i c h w -> c h (i w)")
utils.display_array_as_img(arr1)

# %%
# Exercise 2: Vertically concatenate two 0 images.
arr2 = einops.repeat(arr[0], "c h w -> c (2 h) w")
utils.display_array_as_img(arr2)

# %%
arr3 = einops.repeat(arr[0:2], "i c h w -> c (i h) (rep_w w)", rep_w=2)
utils.display_array_as_img(arr3)

# %%

arr4 = einops.repeat(arr[0], "c h w -> c (h 2) w")
utils.display_array_as_img(arr4)

# %%
# Exercise 5: Splitting color channels.
arr5 = einops.rearrange(arr[0], "c h w -> h (c w)")
utils.display_array_as_img(arr5)

# %%
# Exercise 6: Wrap images across two rows.
arr6 = einops.rearrange(arr, "(i1 i2) c h w -> c (i1 h) (i2 w)", i1=2)
utils.display_array_as_img(arr6)

# %%
# Exercise 7: Max of color channels
arr7 = einops.reduce(arr, "i c h w -> h (i w)", "max")
utils.display_array_as_img(arr7)

# %%
# Exercise 8: Stack images and take minimum.
arr8 = einops.reduce(arr, "i c h w -> h w", "min")
utils.display_array_as_img(arr8)

# %%
# Exercise 9: Transpose
arr9 = einops.rearrange(arr[1], "c h w -> c w h")
utils.display_array_as_img(arr9)

# %%
# Exercise 10: Splitting i, concatenating v and h, scaling down by 2

arr10 = einops.reduce(
    arr.astype(float), "(i1 i2) c (h 2) (w 2) -> c (i1 h) (i2 w)", "mean", i1=2
)
utils.display_array_as_img(arr10)


# %%
# Einops exercises - helper functions
def assert_all_equal(actual: t.Tensor, expected: t.Tensor) -> None:
    assert actual.shape == expected.shape, f"Shape mismatch, got: {actual.shape}"
    assert (actual == expected).all(), f"Value mismatch, got: {actual}"
    print("Passed!")


def assert_all_close(
    actual: t.Tensor, expected: t.Tensor, rtol=1e-05, atol=0.0001
) -> None:
    assert actual.shape == expected.shape, f"Shape mismatch, got: {actual.shape}"
    assert t.allclose(actual, expected, rtol=rtol, atol=atol)
    print("Passed!")


# %%
# Exercise A.1 - rearrange
def rearrange_1() -> t.Tensor:
    """Return the following tensor using only torch.arange and einops.rearrange:

    [[3, 4],
     [5, 6],
     [7, 8]]
    """
    tmp = t.arange(3, 9)
    result = einops.rearrange(tmp, "(b1 b2) -> b1 b2", b1=3)
    return result


expected = t.tensor([[3, 4], [5, 6], [7, 8]])
assert_all_equal(rearrange_1(), expected)


# %%
# Exercise A.2 - arange and rearrange
def rearrange_2() -> t.Tensor:
    """Return the following tensor using only torch.arange and einops.rearrange:

    [[1, 2, 3],
     [4, 5, 6]]
    """
    tmp = t.arange(1, 7)
    result = einops.rearrange(tmp, "(b1 b2) -> b1 b2", b1=2)
    return result


assert_all_equal(rearrange_2(), t.tensor([[1, 2, 3], [4, 5, 6]]))


# %%
# Exercise A.3 - rearrange
def rearrange_3() -> t.Tensor:
    """Return the following tensor using only torch.arange and einops.rearrange:

    [[[1], [2], [3], [4], [5], [6]]]
    """
    tmp = t.arange(1, 7)
    result = einops.rearrange(tmp, "b -> 1 b 1")
    return result


assert_all_equal(rearrange_3(), t.tensor([[[1], [2], [3], [4], [5], [6]]]))


# %%
# Exercise B.1 - temperature average
def temperatures_average(temps: t.Tensor) -> t.Tensor:
    """Return the average temperature for each week.

    temps: a 1D temperature containing temperatures for each day.
    Length will be a multiple of 7 and the first 7 days are for the first week, second 7 days for the second week, etc.

    You can do this with a single call to reduce.
    """
    assert len(temps) % 7 == 0
    return einops.reduce(temps, "(n 7) -> n", "mean")


temps = t.Tensor(
    [71, 72, 70, 75, 71, 72, 70, 68, 65, 60, 68, 60, 55, 59, 75, 80, 85, 80, 78, 72, 83]
)
expected = t.tensor([71.5714, 62.1429, 79.0])
assert_all_close(temperatures_average(temps), expected)


# %%
# Exercise B.2 - temperature difference
def temperatures_differences(temps: t.Tensor) -> t.Tensor:
    """For each day, subtract the average for the week the day belongs to.

    temps: as above
    """
    assert len(temps) % 7 == 0
    means = einops.repeat(einops.reduce(temps, "(n 7) -> n", "mean"), "n -> (n 7)")
    return temps - means


expected = t.tensor(
    [
        -0.5714,
        0.4286,
        -1.5714,
        3.4286,
        -0.5714,
        0.4286,
        -1.5714,
        5.8571,
        2.8571,
        -2.1429,
        5.8571,
        -2.1429,
        -7.1429,
        -3.1429,
        -4.0,
        1.0,
        6.0,
        1.0,
        -1.0,
        -7.0,
        4.0,
    ]
)
actual = temperatures_differences(temps)
assert_all_close(actual, expected)


# %%
# Exercise B.3 - temperature normalized
def temperatures_normalized(temps: t.Tensor) -> t.Tensor:
    """For each day, subtract the weekly average and divide by the weekly standard deviation.

    temps: as above

    Pass torch.std to reduce.
    """
    means = einops.reduce(temps, "(t 7) -> t", "mean")
    diffs = temps - einops.repeat(means, "t -> (t 7)")
    stds = einops.reduce(diffs, "(d 7) -> d", t.std)
    return diffs / einops.repeat(stds, "s -> (s 7)")


expected = t.tensor(
    [
        -0.3326,
        0.2494,
        -0.9146,
        1.9954,
        -0.3326,
        0.2494,
        -0.9146,
        1.1839,
        0.5775,
        -0.4331,
        1.1839,
        -0.4331,
        -1.4438,
        -0.6353,
        -0.8944,
        0.2236,
        1.3416,
        0.2236,
        -0.2236,
        -1.5652,
        0.8944,
    ]
)
actual = temperatures_normalized(temps)
assert_all_close(actual, expected)


# %%
# Exercise C - identity matrix
def identity_matrix(n: int) -> t.Tensor:
    """Return the identity matrix of size nxn.

    Don't use torch.eye or similar.

    Hint: you can do it with arange, rearrange, and ==.
    Bonus: find a different way to do it.
    """
    assert n >= 0
    seq = t.arange(n)
    seq_1 = einops.repeat(seq, "i -> n i", n=n)
    seq_2 = einops.repeat(seq, "i -> i n", n=n)
    result = seq_1 == seq_2
    return result


assert_all_equal(identity_matrix(3), t.Tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))
assert_all_equal(identity_matrix(0), t.zeros((0, 0)))


# %%
# Exercise D - sample distribution
def sample_distribution(probs: t.Tensor, n: int) -> t.Tensor:
    """Return n random samples from probs, where probs is a normalized probability distribution.

    probs: shape (k,) where probs[i] is the probability of event i occurring.
    n: number of random samples

    Return: shape (n,) where out[i] is an integer indicating which event was sampled.

    Use torch.rand and torch.cumsum to do this without any explicit loops.

    Note: if you think your solution is correct but the test is failing, try increasing the value of n.
    """
    assert abs(probs.sum() - 1.0) < 0.001
    assert (probs >= 0).all()
    rs = t.rand(n)
    cs = t.cumsum(probs, dim=0)
    is_bigger = rs[:, None] > cs
    return einops.reduce(is_bigger, "r i -> r", "sum")


n = 10000000
probs = t.tensor([0.05, 0.1, 0.1, 0.2, 0.15, 0.4])
freqs = t.bincount(sample_distribution(probs, n)) / n
assert_all_close(freqs, probs, rtol=0.001, atol=0.001)


# %%
# Exercise E - classifier accuracy
def classifier_accuracy(scores: t.Tensor, true_classes: t.Tensor) -> t.Tensor:
    """Return the fraction of inputs for which the maximum score corresponds to the true class for that input.

    scores: shape (batch, n_classes). A higher score[b, i] means that the classifier thinks class i is more likely.
    true_classes: shape (batch, ). true_classes[b] is an integer from [0...n_classes).

    Use torch.argmax.

    Find index of max value of each sub list( [0.75, 0.5, 0.25] max index is 0), see if it matches the index in true_classes, return the fraction of correct / total
    """
    assert true_classes.max() < scores.shape[1]
    ans = (scores.argmax(dim=1) == true_classes).float().mean()

    return ans


scores = t.tensor([[0.75, 0.5, 0.25], [0.1, 0.5, 0.4], [0.1, 0.7, 0.2]])
true_classes = t.tensor([0, 1, 0])
expected = 2.0 / 3.0
assert classifier_accuracy(scores, true_classes) == expected


# %%
# Exercise F.1 - total price indexing
def total_price_indexing(prices: t.Tensor, items: t.Tensor) -> float:
    """Given prices for each kind of item and a tensor of items purchased, return the total price.

    prices: shape (k, ). prices[i] is the price of the ith item.
    items: shape (n, ). A 1D tensor where each value is an item index from [0..k). Each value is the index of a single item purchased, return total price of order.

    Use integer array indexing. The below document describes this for NumPy but it's the same in PyTorch:

    https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing
    """
    assert items.max() < prices.shape[0]

    # return sum([prices[item] for item in items])[0]
    ans = prices[items].sum().item()

    return ans


prices = t.tensor([0.5, 1, 1.5, 2, 2.5])
items = t.tensor([0, 0, 1, 1, 4, 3, 2])
assert total_price_indexing(prices, items) == 9.0

# %%
# Exercise F.2 - gather 2D


def gather_2d(matrix: t.Tensor, indexes: t.Tensor) -> t.Tensor:
    """Perform a gather operation along the second dimension.

    matrix: shape (m, n)
    indexes: shape (m, k)

    Return: shape (m, k). out[i][j] = matrix[i][indexes[i][j]]

    For this problem, the test already passes and it's your job to write at least three asserts relating the arguments and the output. This is a tricky function and worth spending some time to wrap your head around its behavior.

    See: https://pytorch.org/docs/stable/generated/torch.gather.html?highlight=gather#torch.gather
    For diagram see (note diagram uses 1 index, code uses 0 index) https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms
    """

    assert matrix.ndim == indexes.ndim
    assert indexes.shape[0] <= matrix.shape[0]
    out = matrix.gather(1, indexes)
    assert out.shape == indexes.shape
    return out


matrix = t.arange(15).view(3, 5)
indexes = t.tensor([[4], [3], [2]])
expected = t.tensor([[4], [8], [12]])
assert_all_equal(gather_2d(matrix, indexes), expected)
indexes2 = t.tensor([[2, 4], [1, 3], [0, 2]])
expected2 = t.tensor([[2, 4], [6, 8], [10, 12]])
assert_all_equal(gather_2d(matrix, indexes2), expected2)

# %%
# Exercise F.3 - total price gather


def total_price_gather(prices: t.Tensor, items: t.Tensor) -> float:
    """Compute the same as total_price_indexing, but use torch.gather."""
    assert items.max() < prices.shape[0]
    ans = prices.gather(0, items).sum().item()
    return ans


prices = t.tensor([0.5, 1, 1.5, 2, 2.5])
items = t.tensor([0, 0, 1, 1, 4, 3, 2])
assert total_price_gather(prices, items) == 9.0


# %%
# Einsum
def einsum_trace(mat: np.ndarray):
    """
    Returns the same as `np.trace`.
    """
    # ident = t.eye(mat.shape[0], mat.shape[1])
    # return einops.einsum(mat * ident.numpy(), "r c -> ")
    return einops.einsum(mat, "i i -> ")


def einsum_mv(mat: np.ndarray, vec: np.ndarray):
    """
    Returns the same as `np.matmul`, when `mat` is a 2D array and `vec` is 1D.
    """
    return einops.einsum(mat, vec[:, None], "a b, c d -> a d")


def einsum_mm(mat1: np.ndarray, mat2: np.ndarray):
    """
    Returns the same as `np.matmul`, when `mat1` and `mat2` are both 2D arrays.
    """
    pass


def einsum_inner(vec1: np.ndarray, vec2: np.ndarray):
    """
    Returns the same as `np.inner`.
    """
    pass


def einsum_outer(vec1: np.ndarray, vec2: np.ndarray):
    """
    Returns the same as `np.outer`.
    """
    pass


tests.test_einsum_trace(einsum_trace)
tests.test_einsum_mv(einsum_mv)
tests.test_einsum_mm(einsum_mm)
tests.test_einsum_inner(einsum_inner)
tests.test_einsum_outer(einsum_outer)
# %%
