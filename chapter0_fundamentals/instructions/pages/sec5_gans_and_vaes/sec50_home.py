import streamlit as st



def section():

    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#introduction'>Introduction</a></li>
    <li class='margtop'><a class='contents-el' href='#content-learning-objectives'>Content & Learning Objectives</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#gans'>GANs</a></li>
        <li><a class='contents-el' href='#autoencoders-vaes'>Autoencoders & VAEs</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#setup'>Setup</a></li>
</ul>""", unsafe_allow_html=True)

    st.markdown(
r"""
# [0.5] - GANs & VAEs

### Colab: [exercises](https://colab.research.google.com/drive/1WpSxK44Kpnuic6dC0uWwQFcMnG8J4FRr) | [solutions](https://colab.research.google.com/drive/1RG8HPjlmxyXjHUXebCgnpojNH3lLZUAI)

Please send any problems / bugs on the `#errata` channel in the [Slack group](https://join.slack.com/t/arena-uk/shared_invite/zt-28h0xs49u-ZN9ZDbGXl~oCorjbBsSQag), and ask any questions on the dedicated channels for this chapter of material.

If you want to change to dark mode, you can do this by clicking the three horizontal lines in the top-right, then navigating to Settings â†’ Theme.

Links to other chapters: [**(1) Transformers & Mech Interp**](https://arena3-chapter1-transformer-interp.streamlit.app/), [**(2) RL**](https://arena3-chapter2-rl.streamlit.app/).

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/paint.png" width="350">

## Introduction

Today, we're studying two important classes of generative image models: **Generative Adversarial Networks (GANs)** and **Variational Autoencoders (VAEs)**. Although these generally aren't SOTA any more (thanks in part to the rise of diffusion models), there are some deep conceptual insights which can be gleaned from studying these models (VAEs in particular) which help lay the groundwork for more advanced models.

These exercises will also hopefully bring much of this chapter full-circle:

* We'll cover transposed convolutions, which will serve as a refresher on some of the ideas behind convolutions **(day 2: CNNs & ResNets)**
* We'll be assembling NNet architectures from scratch, like in the ResNets exercises **(day 2: CNNs & ResNets)**
* We'll work with different loss functions, and think intuitively about what it means to optimize them **(day 3: Optimization & Hyperparameters)**
* We'll be working with `wandb`, and will learn how to log outputs produced by our models **(day 3: Optimization & Hyperparameters)**
* We'll have to think carefully about how gradient propagation works between different parts of our model **(day 4: Backpropagation)**

## Content & Learning Objectives

### GANs

We start with some reading material on GANs and transposed convolutions (which are often used in parallel with convolutions, to take a latent space and map it back into a full-size image).

Next, you'll actually implement and train your own GANs, to generate celebrity pictures. By the time you're done, you'll hopefully have produced output like this (and you'll really be giving Midjourney a run for their money):
                
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan-last-output.png" width="1100">

> ### Learning Objectives
> 
> * Learn about the transposed convolution operation.
> * Understand the loss function used in GANs, and why it can be expected to result in the generator producing realistic outputs.
> * Implement the DCGAN architecture from the paper, with relatively minimal guidance.
> * Learn how to identify and fix bugs in your GAN architecture, to improve convergence properties.

### Autoencoders & VAEs

Autoencoders are a pretty simple architecture: you learn a compressed representation of your data (mainly using linear layers and convolutions), then reconstruct it back into an image (with linear layers and transposed convolutions).

Although autoencoders can learn some interesting low-dimensional representations, they are less good for generating images because their latent spaces aren't generally meaningful. This leads to VAEs, which solve this problem by having their encoders map to a distribution over latent vectors, rather than a single latent vector. This incentivises the latent space to be more meaningful, and we can more easily generate images from sample vectors in this space.

> ### Learning Objectives
>
> * Understand the basic architecture of autoencoders and VAEs
> * Learn about the reparameterization trick for VAEs
> * Implement your own autoencoder
> * Implement your own VAE, and use it to generate realistic MNIST images
> * (optional) Dive deeper into the mathematical underpinnings of VAEs, and learn about the ELBO loss function

### Bonus - Transposed Convolutions

In this section, you'll implement the transposed convolution operation manually. This is similar to a regular convolution, but designed for upsampling rather than downsampling (i.e. producing an image from a latent vector rather producing output from an image). These are very important in many generative algorithms. Once you implement this, you'll be able to build your own GANs and VAEs from scratch, without using any pre-built layers.

*Note - the bonus section from the CNNs day is a prerequisite for these bonus exercises. If you haven't completed that section, you'll need to do so before attempting these.*

> ### Learning Objectives
> 
> * Learn about & implement the transposed convolution operation.
> * Implement GANs and/or VAEs entirely from scratch.

## Setup

### Imports

```python
import os
import sys
import torch as t
from torch import nn, optim
import einops
from einops.layers.torch import Rearrange
from tqdm import tqdm
from dataclasses import dataclass, field
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Dataset, Subset
import numpy as np
from typing import Optional, Tuple, List, Literal, Union
import plotly.express as px
import torchinfo
import time
import wandb
from PIL import Image
import pandas as pd
from pathlib import Path
from datasets import load_dataset

# Make sure exercises are in the path
chapter = r"chapter0_fundamentals"
exercises_dir = Path(f"{os.getcwd().split(chapter)[0]}/{chapter}/exercises").resolve()
section_dir = exercises_dir / "part5_gans_and_vaes"
if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))

from part2_cnns.utils import print_param_count
import part5_gans_and_vaes.tests as tests
import part5_gans_and_vaes.solutions as solutions
from plotly_utils import imshow

from part2_cnns.solutions import (
    Linear,
    ReLU,
    Sequential,
    BatchNorm2d,
)
from part2_cnns.solutions_bonus import (
    pad1d,
    pad2d,
    conv1d_minimal,
    conv2d_minimal,
    Conv2d,
    Pair,
    IntOrPair,
    force_pair,
)

device = t.device("cuda" if t.cuda.is_available() else "cpu")

MAIN = __name__ == "__main__"
```
""", unsafe_allow_html=True)
