import streamlit as st

def section():

    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#batched-ray-segment-intersection'>Batched Ray-Segment Intersection</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#tip-ellipsis'>Tip - Ellipsis</a></li>
        <li><a class='contents-el' href='#tip-elementwise-logical-operations-on-tensors'>Tip - Elementwise Logical Operations on Tensors</a></li>
        <li><a class='contents-el' href='#tip-einops'>Tip - <code>einops</code></a></li>
        <li><a class='contents-el' href='#tip-logical-reductions'>Tip - Logical Reductions</a></li>
        <li><a class='contents-el' href='#tip-broadcasting'>Tip - Broadcasting</a></li>
        <li><a class='contents-el' href='#summary-of-all-these-tips'>Summary of all these tips</a></li>
        <li><a class='contents-el' href='#exercise-implement-intersect-rays-1d'><b>Exercise</b> - implement <code>intersect_rays_1d</code></a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#using-gpt-to-understand-code'>Using GPT to understand code</a></li>
    <li class='margtop'><a class='contents-el' href='#10d-rays'>2D Rays</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-make-rays-2d'><b>Exercise</b> - implement <code>make_rays_2d</code></a></li>
    </ul></li>
</ul>""", unsafe_allow_html=True)

    st.markdown(
r"""
# Batched Operations

## Batched Ray-Segment Intersection

Next, implement a batched version that takes multiple rays, multiple line segments, and returns a boolean for each ray indicating whether **any** segment intersects with that ray.

Note - in the batched version, we don't want the solver to throw an exception just because some of the equations don't have a solution - these should just return False. 


### Tip - Ellipsis

You can use an ellipsis `...` in an indexing expression to avoid repeated `:` and to write indexing expressions that work on varying numbers of input dimensions. 

For example, `x[..., 0]` is equivalent to `x[:, :, 0]` if `x` is 3D, and equivalent to `x[:, :, :, 0]` if `x` is 4D.

### Tip - Elementwise Logical Operations on Tensors

For regular booleans, the keywords `and`, `or`, and `not` are used to do logical operations and the operators `&`, `|`, and `~` do and, or and not on each bit of the input numbers. For example `0b10001 | 0b11000` is `0b11001` or 25 in base 10.

Tragically, Python doesn't allow classes to overload keywords, so if `x` and `y` are of type `torch.Tensor`, then `x and y` does **not** do the natural thing that you probably expect, which is compute `x[i] and y[i]` elementwise. It actually tries to coerce `x` to a regular boolean, which throws an exception.

As a workaround, PyTorch (and NumPy) have chosen to overload the bitwise operators but have them actually mean logical operations, since you usually don't care to do bitwise operations on tensors. So the correct expression would be `x & y` to compute `x[i] and y[i]` elementwise.

Another gotcha when it comes to logical operations - **operator precedence**. For instance, `v >= 0 & v <= 1` is actually evaluated as `(v >= (0 & v)) <= 1` (because `&` has high precedence). When in doubt, use parentheses to force the correct parsing: `(v >= 0) & (v <= 1)`.

### Tip - `einops`

Einops is a useful library which we'll dive deeper with tomorrow. For now, the only important function you'll need to know is `einops.repeat`. This takes as arguments a tensor and a string, and returns a new tensor which has been repeated along the specified dimensions. For example, the following code shows how we can repeat a 2D tensor along the last dimension:

```python
x = t.randn(2, 3)
x_repeated = einops.repeat(x, 'a b -> a b c', c=4)

assert x_repeated.shape == (2, 3, 4)
for c in range(4):
    t.testing.assert_close(x, x_repeated[:, :, c])
```

*(The function `t.testing.assert_close` checks that two tensors are the same shape, and all elements are the same value, within some very small numerical error.)*

### Tip - Logical Reductions

In plain Python, if you have a list of lists and want to know if any element in a row is `True`, you could use a list comprehension like `[any(row) for row in rows]`. The efficient way to do this in PyTorch is with `torch.any()` or equivalently the `.any()` method of a tensor, which accept the dimension to reduce over. Similarly, `torch.all()` or `.all()` method. Both of these methods accept a `dim` argument, which is the dimension to reduce over.

<details>
<summary>Aside - tensor methods</summary>

Most functions like `torch.any(tensor, ...)` (which take a tensor as first argument) have an equivalent tensor method `tensor.any(...)`. We'll see many more examples of functions like this as we go through the course.
</details>

### Tip - Broadcasting

Broadcasting is what happens when you perform an operation on two tensors, and one is a smaller size, but is copied along the dimensions of the larger one in order to apply to it. Below is an example (where `B` is copied along the first dimension of `A`):

```python
A = t.randn(2, 3)
B = t.randn(3)
AplusB = A + B

assert AplusB.shape == (2, 3)
for i in range(2):
    t.testing.assert_close(AplusB[i], A[i] + B)
```

Broadcasting sematics are a bit messy, and we'll go into it in more detail later in the course. If you want to get a full picture of it then click on the dropdown below, but for now here's the important thing to know - *dimensions get appended to the **start** of the smaller tensor `B`, and it is copied along those dimensions until its shape matches the larger tensor `A`*.

<details>
<summary>Aside - details of broadcasting</summary>

If you try to broadcast tensors `A` and `B`, then the following happens:

* The tensor with fewer dimensions is padded with dimensions of size one on the left.
* Once the tensors have the same number of dimensions, they are checked for compatibility.
    * Two dimensions are compatible if they are equal, or if one of them is one (in the latter case, we repeat the size-1 tensor along that dimension until it's the same size as the larger one).
    * If they are not compatible, then broadcasting is not allowed.

For instance, in the example above, first `B` is left-padded to shape `(1, 3)`, then it is copied along the first dimension to shape `(2, 3)`, when it can be added to `A`.

On the other hand, if `B` had shape `(2,)` then broadcasting would fail, because dimensions can't be added to the right of a tensor.
</details>


### Summary of all these tips

> * Use `...` to avoid repeated `:` in indexing expressions.
> * Use `&`, `|`, and `~` for elementwise logical operations on tensors.
> * Use parentheses to force the correct operator precedence.
> * Use `torch.any()` or `.any()` to do logical reductions (you can do this over a single dimension, with the `dim` argument).
> * If you're trying to broadcast tensors `A` and `B` (where `B` has fewer dimensions), this will work if the shape of `B` matches the ***last*** dimensions of `A` (in this case, `B` will get copied along the earlier dimensions of `A`).


### Exercise - implement `intersect_rays_1d`

```yaml
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª
Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª

You should spend up to 25-30 minutes on this exercise.

It will be one of the most difficult today; certainly it has the biggest step in difficulty.
```

With all these tips in mind, now you should implement `intersect_rays_1d`, the batched version of `intersect_ray_1d` (which returns True for each ray, if it intersects *any* segment).


```python
def intersect_rays_1d(rays: Float[Tensor, "nrays 2 3"], segments: Float[Tensor, "nsegments 2 3"]) -> Bool[Tensor, "nrays"]:
    '''
    For each ray, return True if it intersects any segment.
    '''
    pass


tests.test_intersect_rays_1d(intersect_rays_1d)
tests.test_intersect_rays_1d_special_case(intersect_rays_1d)
```

<details>
<summary>Help - I'm not sure how to implment this function without a for loop.</summary>

Initially, `rays.shape == (NR, 2, 3)` and `segments.shape == (NS, 2, 3)`. Try performing `einops.repeat` on them, so both their shapes are `(NR, NS, 2, 3)`. Then you can formulate and solve the batched system of matrix equations.
</details>

<details>
<summary>Help - I'm not sure how to deal with the cases of zero determinant.</summary>

You can use `t.linalg.det` to compute the determinant of a matrix, or batch of matrices *(gotcha: the determinant won't be exactly zero, but you can check that it's very close to zero, e.g. `det.abs() < 1e-6`)*. This will give you a boolean mask for which matrices are singular.

You can set all singular matrices to the identity (this avoids errors), and then at the very end you can use your boolean mask again to set the intersection to `False` for the singular matrices.
</details>

<details>
<summary>Help - I'm still stuck on the zero determinant cases.</summary>

After formulating the matrix equation, you should have a batch of matrices of shape `(NR, NS, 2, 2)`, i.e. `mat[i, j, :, :]` is a matrix which looks like:

$$
\begin{pmatrix} D_x & (L_1 - L_2)_x \\ D_y & (L_1 - L_2)_y \\ \end{pmatrix}
$$

Calling `t.linalg.det(mat)` will return an array of shape `(NR, NS)` containing the determinants of each matrix. You can use this to construct a mask for the singular matrices *(gotcha: the determinant won't be exactly zero, but you can check that it's very close to zero, e.g. `det.abs() < 1e-6`)*.

Indexing `mat` by this mask will return an array of shape `(x, 2, 2)`, where the zeroth axis indexes the singular matrices. As we discussed in the broadcasting section earlier, this means we can use broadcasting to set all these singular matrices to the identity:

```python
mat[is_singular] = t.eye(2)
```
</details>

<details>
<summary>Solution</summary>


```python
def intersect_rays_1d(rays: Float[Tensor, "nrays 2 3"], segments: Float[Tensor, "nsegments 2 3"]) -> Bool[Tensor, "nrays"]:
    '''
    For each ray, return True if it intersects any segment.
    '''
    # SOLUTION
    NR = rays.size(0)
    NS = segments.size(0)

    # Get just the x and y coordinates
    rays = rays[..., :2]
    segments = segments[..., :2]

    # Repeat rays and segments so that we can compuate the intersection of every (ray, segment) pair
    rays = einops.repeat(rays, "nrays p d -> nrays nsegments p d", nsegments=NS)
    segments = einops.repeat(segments, "nsegments p d -> nrays nsegments p d", nrays=NR)

    # Each element of `rays` is [[Ox, Oy], [Dx, Dy]]
    O = rays[:, :, 0]
    D = rays[:, :, 1]
    assert O.shape == (NR, NS, 2)

    # Each element of `segments` is [[L1x, L1y], [L2x, L2y]]
    L_1 = segments[:, :, 0]
    L_2 = segments[:, :, 1]
    assert L_1.shape == (NR, NS, 2)

    # Define matrix on left hand side of equation
    mat = t.stack([D, L_1 - L_2], dim=-1)
    # Get boolean of where matrix is singular, and replace it with the identity in these positions
    dets = t.linalg.det(mat)
    is_singular = dets.abs() < 1e-8
    assert is_singular.shape == (NR, NS)
    mat[is_singular] = t.eye(2)

    # Define vector on the right hand side of equation
    vec = L_1 - O

    # Solve equation, get results
    sol = t.linalg.solve(mat, vec)
    u = sol[..., 0]
    v = sol[..., 1]

    # Return boolean of (matrix is nonsingular, and solution is in correct range implying intersection)
    return ((u >= 0) & (v >= 0) & (v <= 1) & ~is_singular).any(dim=-1)
```
</details>


## Using GPT to understand code

*Note, the world of LLMs moves fast, so this section is likely to get out of date at some point!*

Next week we'll start learning about transformers and how to build them, but it's not too early to start using them to accelerate your own learning!

We'll be discussing more advanced ways to use GPT 3 and 4 as coding partners / research assistants in the coming weeks, but for now we'll look at a simple example: **using GPT to understand code**. You're recommended to read the recent [LessWrong post](https://www.lesswrong.com/posts/ptY6X3BdW4kgqpZFo/using-gpt-4-to-understand-code) by Siddharth Hiregowdara in which he explains his process. This works best on GPT-4, but I've found GPT-3.5 works equally well for reasonably straightforward problems (see the section below).

Firstly, you should get an account to use GPT with if you haven't already. Next, try asking GPT-3.5 / 4 for an explanation of the function above. You can do this e.g. via the following prompt:

```
Explain this Python function, line by line. You should break up your explanation by inserting sections of the code.

def intersect_rays_1d(rays: Float[Tensor, "nrays 2 3"], segments: Float[Tensor, "nsegments 2 3"]) -> Bool[Tensor, "nrays"]:
    NR = rays.size(0)
    NS = segments.size(0)
    rays = rays[..., :2]
    ...
```

I've found removing comments is often more helpful, because then GPT will answer in its own words rather than just repeating the comments (and the comments can sometimes confuse it). 

Once you've got a response, here are a few more things you might want to consider asking:

* Can you suggest ways to improve the code?
    * GPT-4 recommended using a longer docstring and more descriptive variable names, among other things.
* Can you explain why the line `mat[is_singular] = t.eye(2)` works?
    * GPT-4 gave me a correct and very detailed explanation involving broadcasting and tensor shapes.

Is using GPT in this way cheating? It can be, if your first instinct is to jump to GPT rather than trying to understand the code yourself. But it's important here to bring up the distinction of [playing in easy mode](https://www.lesswrong.com/posts/nJPtHHq6L7MAMBvRK/play-in-easy-mode) vs [playing in hard mode](https://www.lesswrong.com/posts/7hLWZf6kFkduecH2g/play-in-hard-mode). There are situations where it's valuable for you to think about a problem for a while before moving forward because that deliberation will directly lead to you becoming a better researcher or engineer (e.g. when you're thinking of a hypothesis for how a circuit works while doing mechanistic interpretability on a transformer, or you're pondering which datastructure best fits your use case while implementing some RL algorithm). But there are also situations (like this one) where you'll get more value from speedrunning towards an understanding of certain code or concepts, and apply your understanding in subsequent exercises. It's important to find a balance!

#### When to use GPT-3.5 and GPT-4

GPT-3.5 and 4 both have advantages and disadvantages in different situations. GPT-3.5 has a large advantage in speed over GPT-4, and works equally well on simple problems or functions. If it's anything that Copilot is capable of writing, then you're likely better off using it instead of GPT-4.

On the other hand, GPT-4 has an advantage at generating coherent code (although we don't expect you to be using it for code generation much at this stage in the program), and is generally better at responding to complex tasks with less prompt engineering.

#### Additional notes on using GPT (from Joseph Bloom)

* ChatGPT is overly friendly. If you give it bad code it won't tell you it's shit so you need to encourage it to give feedback and/or show you examples of great code. Especially for beginner coders using it, it's important to realise how *under* critical it is. 
* GPT is great at writing tests (asking it to write a test for a function is often better than asking it if a function is correct), refactoring code (identifying repeated tasks and extracting them) and naming variables well. These are specific things worth doing a few times to see how useful they can be. 
* GPT-4 does well with whole modules/scripts so don't hesitate to add those. When you start managing repos on GitHub, use tracked files so that when you copy-paste edited code back, all the changes are highlighted for you as if you'd made them. (VSCode highlights changes with small blue bars next to the line numbers in your python files.)

Here are some things you can play around with:

* Ask GPT it to write tests for the function. You can give more specific instructions (e.g. asking it to use / not to use the `unittests` library, or to print more informative error messages).
* Ask GPT how to refactor the function above. (When I did this, it suggested splitting the function up into subfunctions which performed the discrete tasks of "compute intersection points")


## 2D Rays

Now we're going to make use of the z dimension and have rays emitted from the origin in both y and z dimensions.


### Exercise - implement `make_rays_2d`

```yaml
Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª
Importance: ðŸ”µðŸ”µâšªâšªâšª

You should spend up to 10-15 minutes on this exercise.
```

Implement `make_rays_2d` analogously to `make_rays_1d`. The result should look like a pyramid with the tip at the origin.


```python
def make_rays_2d(num_pixels_y: int, num_pixels_z: int, y_limit: float, z_limit: float) -> Float[t.Tensor, "nrays 2 3"]:
    '''
    num_pixels_y: The number of pixels in the y dimension
    num_pixels_z: The number of pixels in the z dimension

    y_limit: At x=1, the rays should extend from -y_limit to +y_limit, inclusive of both.
    z_limit: At x=1, the rays should extend from -z_limit to +z_limit, inclusive of both.

    Returns: shape (num_rays=num_pixels_y * num_pixels_z, num_points=2, num_dims=3).
    '''
    pass


rays_2d = make_rays_2d(10, 10, 0.3, 0.3)
render_lines_with_plotly(rays_2d)
```

<details>
<summary>Help - I'm not sure how to implement this function.</summary>

Don't write it as a function right away. The most efficient way is to write and test each line individually in the REPL to verify it does what you expect before proceeding.

You can either build up the output tensor using `torch.stack`, or you can initialize the output tensor to its final size and then assign to slices like `rays[:, 1, 1] = ...`. It's good practice to be able to do it both ways.

Each y coordinate needs a ray with each corresponding z coordinate - in other words this is an outer product. The most elegant way to do this is with two calls to `einops.repeat`. You can also accomplish this with `unsqueeze`, `expand`, and `reshape` combined.
</details>

<details>
<summary>Solution</summary>


```python
def make_rays_2d(num_pixels_y: int, num_pixels_z: int, y_limit: float, z_limit: float) -> Float[t.Tensor, "nrays 2 3"]:
    '''
    num_pixels_y: The number of pixels in the y dimension
    num_pixels_z: The number of pixels in the z dimension

    y_limit: At x=1, the rays should extend from -y_limit to +y_limit, inclusive of both.
    z_limit: At x=1, the rays should extend from -z_limit to +z_limit, inclusive of both.

    Returns: shape (num_rays=num_pixels_y * num_pixels_z, num_points=2, num_dims=3).
    '''
    # SOLUTION
    n_pixels = num_pixels_y * num_pixels_z
    ygrid = t.linspace(-y_limit, y_limit, num_pixels_y)
    zgrid = t.linspace(-z_limit, z_limit, num_pixels_z)
    rays = t.zeros((n_pixels, 2, 3), dtype=t.float32)
    rays[:, 1, 0] = 1
    rays[:, 1, 1] = einops.repeat(ygrid, "y -> (y z)", z=num_pixels_z)
    rays[:, 1, 2] = einops.repeat(zgrid, "z -> (y z)", y=num_pixels_y)
    return rays
```
</details>


""", unsafe_allow_html=True)