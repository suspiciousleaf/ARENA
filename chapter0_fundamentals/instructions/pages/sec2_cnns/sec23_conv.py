import streamlit as st


def section():

    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#reading'>Reading</a></li>
    <li class='margtop'><a class='contents-el' href='#convolutions'>Convolutions</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-conv2d'><b>Exercise</b> - implement <code>Conv2d</code></a></li>
        <li><a class='contents-el' href='#exercise-implement-maxpool2d'><b>Exercise</b> - implement <code>MaxPool2d</code></a></li>
    </ul></li>
</ul>""", unsafe_allow_html=True)

    st.markdown(
r"""
# Convolutions

> ### Learning Objectives
>
> * Learn how convolutions work, and why they are useful for vision models
> * Implement your own convolutions, and maxpooling layers

## Reading

* [But what is a convolution?](https://www.youtube.com/watch?v=KuXjwB4LzSA) by 3Blue1Brown
* [A Comprehensive Guide to Convolutional Neural Networks (TowardsDataScience)](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)

## Convolutions

Here are some questions about convolutions to make sure you've understood the material. Once you finish the article above, you should try and answer these questions without referring back to the original article.

<details>
<summary>Why would convolutional layers be less likely to overfit data than standard linear (fully connected) layers?</summary>

Convolutional layers require significantly fewer weights to be learned. This is because the same kernel is applied all across the image, rather than every pair of `(input, output)` nodes requiring a different weight to be learned.
</details>

<details>
<summary>Suppose you fixed some random permutation of the pixels in an image, and applied this to all images in your dataset, before training a convolutional neural network for classifying images. Do you expect this to be less effective, or equally effective?</summary>

It will be less effective, because CNNs work thanks to **spatial locality** - groups of pixels close together are more meaningful. For instance, CNNs will often learn convolutions at an early layer which recognise gradients or simple shapes. If you permute the pixels (even if you permute in the same way for every image), you destroy locality. 
</details>

<details>
<summary>If you have a 28x28 image, and you apply a 3x3 convolution with stride 1, padding 1, what shape will the output be?</summary>

It will be the same shape, i.e. `28x28`. In the post linked above, this is described as **same padding**. Tomorrow, we'll build an MNIST classifier which uses these convolutions.
</details>

A note on terminology - you might see docs and docstrings sometimes use `num_features`, sometimes use `channels`, and sometimes just `C`. Often these two terms interchangeable. Our neural network inputs will often be RGB images and so we will have `channels=3` corresponding to the 3 colors red/green/blue. As we pass our initial image through convolutional layers, the number of channels will change. In the context of convolutions, the number of features and number of channels usually refer to the same value.

### Exercise - implement `Conv2d`

```yaml
Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª
Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª

You should spend up to ~20 minutes on this exercise.

Make sure you understand what operation is taking place here, and how the dimensions are changing.
```

Rather than implementing the `conv2d` function from scratch, we'll allow you to use `t.nn.functional.conv2d`. In the exercise below, you should use this function to implement the `nn.Conv2d` layer. In other words, you should:

* Initialize the weights for the convolutional layer in the `__init__` function.
    * You should look at the PyTorch page for `nn.Conv2d` [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) to understand what the shape of the weights should be.
    * We assume `bias=False`, so the only `nn.Parameter` object we need to define is `weight`.
    * You should use **Xavier initialization**, as described near the end of the documentation page linked above. (There's a hint on this below, if you get stuck.)
* Implement the `forward` method, which should apply the convolutional layer to the input.
    * In other words, it should implement the `torch.nn.functional.conv2d` function (documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html)), using the weights and biases (and other layer parameters) that you initialized in `__init__`.
* Fill in the `extra_repr` method, to print out the convolutional layer's parameters.

```python
class Conv2d(nn.Module):
    def __init__(
        self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0
    ):
        '''
        Same as torch.nn.Conv2d with bias=False.

        Name your weight field `self.weight` for compatibility with the PyTorch version.
        '''
        super().__init__()
        pass

    def forward(self, x: t.Tensor) -> t.Tensor:
        '''Apply the functional conv2d, which you can import.'''
        pass

    def extra_repr(self) -> str:
        pass


tests.test_conv2d_module(Conv2d)
m = Conv2d(in_channels=24, out_channels=12, kernel_size=3, stride=2, padding=1)
print(f"Manually verify that this is an informative repr: {m}")
```

<details>
<summary>Help - I don't know what to use as number of inputs, when doing Xavier initialisation.</summary>

In the case of convolutions, each value in the output is computed by taking the product over `in_channels * kernel_width * kernel_height` elements. So this should be our value for $N_{in}$.
</details>

<details>
<summary>Solution</summary>

```python
class Conv2d(nn.Module):
    def __init__(
        self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0
    ):
        '''
        Same as torch.nn.Conv2d with bias=False.

        Name your weight field `self.weight` for compatibility with the PyTorch version.
        '''
        super().__init__()
        # SOLUTION
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding

        kernel_height = kernel_width = kernel_size
        sf = 1 / np.sqrt(in_channels * kernel_width * kernel_height)
        weight = sf * (2 * t.rand(out_channels, in_channels, kernel_height, kernel_width) - 1)
        self.weight = nn.Parameter(weight)

    def forward(self, x: t.Tensor) -> t.Tensor:
        '''Apply the functional conv2d, which you can import.'''
        # SOLUTION
        return t.nn.functional.conv2d(x, self.weight, stride=self.stride, padding=self.padding)

    def extra_repr(self) -> str:
        # SOLUTION
        keys = ["in_channels", "out_channels", "kernel_size", "stride", "padding"]
        return ", ".join([f"{key}={getattr(self, key)}" for key in keys])
```
</details>

<br>

### Exercise - implement `MaxPool2d`

```yaml
Difficulty: ðŸ”´ðŸ”´âšªâšªâšª
Importance: ðŸ”µðŸ”µâšªâšªâšª

You should spend up to ~10 minutes on this exercise.
```

Next, you should implement `MaxPool2d`. This module is often applied after a convolutional layer, to reduce the spatial dimensions of the output. It works by taking the maximum value in each kernel-sized window, and outputting that value. For instance, if we have a 2x2 kernel, then we take the maximum of each 2x2 window in the input.

You should use `torch.nn.functional.max_pool2d` to implement this layer. The documentation page can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html), and the documentation for `nn.MaxPool2d` can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html).

```python
class MaxPool2d(nn.Module):
    def __init__(self, kernel_size: int, stride: Optional[int] = None, padding: int = 1):
        super().__init__()
        pass

    def forward(self, x: t.Tensor) -> t.Tensor:
        '''Call the functional version of max_pool2d.'''
        pass

    def extra_repr(self) -> str:
        '''Add additional information to the string representation of this class.'''
        pass


tests.test_maxpool2d_module(MaxPool2d)
m = MaxPool2d(kernel_size=3, stride=2, padding=1)
print(f"Manually verify that this is an informative repr: {m}")
```

<details>
<summary>Help - I'm really confused about what to do here!</summary>

Your `forward` method should just implement the `maxpool2d` function. In order to get the parameters for this function like `kernel_size` and `stride`, you'll need to initialise them in `__init__`. 

Remember that `MaxPool2d` has no weights - it's just a wrapper for the `maxpool2d` function.

---

Ideally, the `extra_repr` method should output something like:
```python
"kernel_size=3, stride=2, padding=1"
```

so that when you print the module, it will look like this:

```python
MaxPool2d(kernel_size=3, stride=2, padding=1)
```
</details>

<details>
<summary>Solution</summary>


```python
class MaxPool2d(nn.Module):
    def __init__(self, kernel_size: int, stride: Optional[int] = None, padding: int = 1):
        super().__init__()
        # SOLUTION
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding

    def forward(self, x: t.Tensor) -> t.Tensor:
        '''Call the functional version of maxpool2d.'''
        # SOLUTION
        return F.max_pool2d(x, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)

    def extra_repr(self) -> str:
        '''Add additional information to the string representation of this class.'''
        # SOLUTION
        return ", ".join([f"{key}={getattr(self, key)}" for key in ["kernel_size", "stride", "padding"]])
```
</details>
""", unsafe_allow_html=True)
