import streamlit as st

def section():

    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#exercise-build-your-own-nn-parameter'><b>Exercise</b> - build your own <code>nn.Parameter</code></a></li>
    <li class='margtop'><a class='contents-el' href='#exercise-build-your-own-nn-module'><b>Exercise</b> - build your own <code>nn.Module</code></a></li>
    <li class='margtop'><a class='contents-el' href='#build-your-own-linear-layer'>Build Your Own Linear Layer</a></li>
    <li class='margtop'><a class='contents-el' href='#build-your-own-cross-entropy-loss'>Build Your Own Cross-Entropy Loss</a></li>
    <li class='margtop'><a class='contents-el' href='#build-your-own-nograd-context-manager'>Build your own <code>NoGrad</code> context manager</a></li>
    <li class='margtop'><a class='contents-el' href='#training-your-network'>Training Your Network</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#training-loop'>Training Loop</a></li>
</ul>""", unsafe_allow_html=True)

    st.markdown(
r"""

# Putting everything together


> ### Learning Objectives
>
> * Complete the process of building up a neural network from scratch and training it via gradient descent.


## Exercise - build your own `nn.Parameter`

```yaml
Difficulty: ðŸ”´ðŸ”´âšªâšªâšª
Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª

You should spend up to 10-15 minutes on this exercise.
```

We've now written enough backwards passes that we can go up a layer and write our own `nn.Parameter` and `nn.Module`.
We don't need much for `Parameter`. It is itself a `Tensor`, shares storage with the provided `Tensor` and requires_grad is `True` by default - that's it!


```python
class Parameter(Tensor):
    def __init__(self, tensor: Tensor, requires_grad=True):
        '''Share the array with the provided tensor.'''
        pass

    def __repr__(self):
        pass


x = Tensor([1.0, 2.0, 3.0])
p = Parameter(x)
assert p.requires_grad
assert p.array is x.array
assert repr(p) == "Parameter containing:\nTensor(array([1., 2., 3.], dtype=float32), requires_grad=True)"
x.add_(Tensor(np.array(2.0)))
assert np.allclose(
    p.array, np.array([3.0, 4.0, 5.0])
), "in-place modifications to the original tensor should affect the parameter"
```

<details>
<summary>Solution </summary>

```python
class Parameter(Tensor):
    def __init__(self, tensor: Tensor, requires_grad=True):
        '''Share the array with the provided tensor.'''
        return super().__init__(tensor.array, requires_grad=requires_grad)

    def __repr__(self):
        return f"Parameter containing:\n{super().__repr__()}"
```
</details>


## Exercise - build your own `nn.Module`

```yaml
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª
Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª

You should spend up to 25-30 minutes on this exercise.
```

`nn.Module` is like `torch.Tensor` in that it has a lot of functionality, most of which we don't care about today. We will just implement enough to get our network training. 

Implement the indicated methods (i.e. the ones which are currently just `pass`). We've defined `_modules` and `_parameters` dict for you (note that, because this isn't a dataclass, we need to manually define them inside the `__init__` method).

Tip: you can bypass `__getattr__` by accessing `self.__dict__` inside a method.

*Note - some of these methods are difficult and non-obvious to implement, so don't worry if you need to look at the solutions.*


```python
class Module:
    _modules: Dict[str, "Module"]
    _parameters: Dict[str, Parameter]

    def __init__(self):
        self._modules = {}
        self._parameters = {}

    def modules(self):
        '''Return the direct child modules of this module.'''
        return self.__dict__["_modules"].values()

    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:
        '''
        Return an iterator over Module parameters.

        recurse: if True, the iterator includes parameters of submodules, recursively.
        '''
        pass

    def __setattr__(self, key: str, val: Any) -> None:
        '''
        If val is a Parameter or Module, store it in the appropriate _parameters or _modules dict.
        Otherwise, call __setattr__ from the superclass.
        '''
        pass

    def __getattr__(self, key: str) -> Union[Parameter, "Module"]:
        '''
        If key is in _parameters or _modules, return the corresponding value.
        Otherwise, raise KeyError.
        '''
        pass

    def __call__(self, *args, **kwargs):
        return self.forward(*args, **kwargs)

    def forward(self):
        raise NotImplementedError("Subclasses must implement forward!")

    def __repr__(self):
        def _indent(s_, numSpaces):
            return re.sub("\n", "\n" + (" " * numSpaces), s_)
        lines = [f"({key}): {_indent(repr(module), 2)}" for key, module in self._modules.items()]
        return "".join([
            self.__class__.__name__ + "(",
            "\n  " + "\n  ".join(lines) + "\n" if lines else "", ")"
        ])


class TestInnerModule(Module):
    def __init__(self):
        super().__init__()
        self.param1 = Parameter(Tensor([1.0]))
        self.param2 = Parameter(Tensor([2.0]))

class TestModule(Module):
    def __init__(self):
        super().__init__()
        self.inner = TestInnerModule()
        self.param3 = Parameter(Tensor([3.0]))


mod = TestModule()
assert list(mod.modules()) == [mod.inner]
assert list(mod.parameters()) == [
    mod.param3,
    mod.inner.param1,
    mod.inner.param2,
], "parameters should come before submodule parameters"
print("Manually verify that the repr looks reasonable:")
print(mod)
```

<details>
<summary>Solution</summary>


```python
class Parameter(Tensor):
    def __init__(self, tensor: Tensor, requires_grad=True):
        '''Share the array with the provided tensor.'''
        # SOLUTION
        return super().__init__(tensor.array, requires_grad=requires_grad)

    def __repr__(self):
        # SOLUTION
        return f"Parameter containing:\n{super().__repr__()}"

class Module:
    _modules: Dict[str, "Module"]
    _parameters: Dict[str, Parameter]

    def __init__(self):
        self._modules = {}
        self._parameters = {}

    def modules(self):
        '''Return the direct child modules of this module.'''
        return self.__dict__["_modules"].values()

    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:
        '''
        Return an iterator over Module parameters.

        recurse: if True, the iterator includes parameters of submodules, recursively.
        '''
        # SOLUTION
        parameters_list = list(self.__dict__["_parameters"].values())
        if recurse:
            for mod in self.modules():
                parameters_list.extend(list(mod.parameters(recurse=True)))
        return iter(parameters_list)

    def __setattr__(self, key: str, val: Any) -> None:
        '''
        If val is a Parameter or Module, store it in the appropriate _parameters or _modules dict.
        Otherwise, call __setattr__ from the superclass.
        '''
        # SOLUTION
        if isinstance(val, Parameter):
            self.__dict__["_parameters"][key] = val
        elif isinstance(val, Module):
            self.__dict__["_modules"][key] = val
        else:
            super().__setattr__(key, val)

    def __getattr__(self, key: str) -> Union[Parameter, "Module"]:
        '''
        If key is in _parameters or _modules, return the corresponding value.
        Otherwise, raise KeyError.
        '''
        # SOLUTION
        if key in self.__dict__["_parameters"]:
            return self.__dict__["_parameters"][key]
        
        if key in self.__dict__["_modules"]:
            return self.__dict__["_modules"][key]
        
        raise KeyError(key)

        
    def __call__(self, *args, **kwargs):
        return self.forward(*args, **kwargs)

    def forward(self):
        raise NotImplementedError("Subclasses must implement forward!")

    def __repr__(self):
        def _indent(s_, numSpaces):
            return re.sub("\n", "\n" + (" " * numSpaces), s_)
        lines = [f"({key}): {_indent(repr(module), 2)}" for key, module in self._modules.items()]
        return "".join([
            self.__class__.__name__ + "(",
            "\n  " + "\n  ".join(lines) + "\n" if lines else "", ")"
        ])
```
</details>


## Build Your Own Linear Layer

```yaml
Difficulty: ðŸ”´ðŸ”´âšªâšªâšª
Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª

You should spend up to 20-25 minutes on this exercise.
```

You may have a `Linear` written already that you can adapt to use our own `Parameter`, `Module`, and `Tensor`. If your `Linear` used `einsum`, use a `matmul` instead. You can implement a backward function for `einsum` in the bonus section.


```python
class Linear(Module):
    weight: Parameter
    bias: Optional[Parameter]
    
    def __init__(self, in_features: int, out_features: int, bias=True):
        '''
        A simple linear (technically, affine) transformation.

        The fields should be named `weight` and `bias` for compatibility with PyTorch.
        If `bias` is False, set `self.bias` to None.
        '''
        super().__init__()
        pass

    def forward(self, x: Tensor) -> Tensor:
        '''
        x: shape (*, in_features)
        Return: shape (*, out_features)
        '''
        pass

    def extra_repr(self) -> str:
        # note, we need to use `self.bias is not None`, because `self.bias` is either a tensor or None, not bool
        return f"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}"



linear = Linear(3, 4)
assert isinstance(linear.weight, Tensor)
assert linear.weight.requires_grad

input = Tensor([[1.0, 2.0, 3.0]])
output = linear(input)
assert output.requires_grad

expected_output = input @ linear.weight.T + linear.bias
np.testing.assert_allclose(output.array, expected_output.array)

print("All tests for `Linear` passed!")
```

<details>
<summary>Solution</summary>


```python
class Linear(Module):
    weight: Parameter
    bias: Optional[Parameter]
    
    def __init__(self, in_features: int, out_features: int, bias=True):
        '''
        A simple linear (technically, affine) transformation.

        The fields should be named `weight` and `bias` for compatibility with PyTorch.
        If `bias` is False, set `self.bias` to None.
        '''
        super().__init__()
        # SOLUTION
        self.in_features = in_features
        self.out_features = out_features
        
        # sf needs to be a float
        sf = in_features ** -0.5
        
        weight = sf * Tensor(2 * np.random.rand(out_features, in_features) - 1)
        self.weight = Parameter(weight)
        
        if bias:
            bias = sf * Tensor(2 * np.random.rand(out_features,) - 1)
            self.bias = Parameter(bias)
        else:
            self.bias = None

    def forward(self, x: Tensor) -> Tensor:
        '''
        x: shape (*, in_features)
        Return: shape (*, out_features)
        '''
        # SOLUTION
        out = x @ self.weight.T
        # Note, transpose has been defined as .permute(-1, -2) in the Tensor class
        if self.bias is not None: 
            out = out + self.bias
        return out

    def extra_repr(self) -> str:
        # note, we need to use `self.bias is not None`, because `self.bias` is either a tensor or None, not bool
        return f"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}"
```
</details>


Finally, for the sake of completeness, we'll define a `ReLU` module:


```python
class ReLU(Module):
    def forward(self, x: Tensor) -> Tensor:
        return relu(x)

```

Now we can define a MLP suitable for classifying MNIST, with zero PyTorch dependency!


```python
class MLP(Module):
    def __init__(self):
        super().__init__()
        self.linear1 = Linear(28 * 28, 64)
        self.linear2 = Linear(64, 64)
        self.relu1 = ReLU()
        self.relu2 = ReLU()
        self.output = Linear(64, 10)

    def forward(self, x: Tensor) -> Tensor:
        x = x.reshape((x.shape[0], 28 * 28))
        x = self.relu1(self.linear1(x))
        x = self.relu2(self.linear2(x))
        x = self.output(x)
        return x

```

## Build Your Own Cross-Entropy Loss

```yaml
Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 10-15 minutes on this exercise.
```

Make use of your integer array indexing to implement `cross_entropy`. See the documentation page [here](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).

Note - if you have a tensor `X` of shape `(a, b)`, and you want to take the `Y[i]`-th element of each row (where `Y` is a tensor of length `a`), the easiest way to do this is with:

```python
X[range(a), Y]
```

since this is read as:

```python
X[0, Y[0]], X[1, Y[1]], ..., X[a-1, Y[a-1]]
```

In place of `range` here, you should use the function `arange` which we've provided for you (this works in exactly the same way, but it returns a `Tensor` instead of a `range` object).


```python
def cross_entropy(logits: Tensor, true_labels: Tensor) -> Tensor:
    '''Like torch.nn.functional.cross_entropy with reduction='none'.

    logits: shape (batch, classes)
    true_labels: shape (batch,). Each element is the index of the correct label in the logits.

    Return: shape (batch, ) containing the per-example loss.
    '''
    pass


tests.test_cross_entropy(Tensor, cross_entropy)
```

<details>
<summary>Solution</summary>


```python
def cross_entropy(logits: Tensor, true_labels: Tensor) -> Tensor:
    '''Like torch.nn.functional.cross_entropy with reduction='none'.

    logits: shape (batch, classes)
    true_labels: shape (batch,). Each element is the index of the correct label in the logits.

    Return: shape (batch, ) containing the per-example loss.
    '''
    # SOLUTION
    n_batch, n_class = logits.shape
    true = logits[arange(0, n_batch), true_labels]
    return -log(exp(true) / exp(logits).sum(1))
```
</details>


## Build your own `NoGrad` context manager

```yaml
Difficulty: ðŸ”´ðŸ”´âšªâšªâšª
Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª

You should spend up to 10-15 minutes on this exercise.
```

The last thing our backpropagation system needs is the ability to turn it off completely like `torch.no_grad`. 

Below, you should implement the `NoGrad` context manager so that it reads and writes the `grad_tracking_enabled` flag from the top of the file.

You should use the global variable `grad_tracking_enabled` **global variables**  for this. To make sure you're accessing the global variable rather than just a local variable, you can use the `global` keyword. In general, using mutable global variables is not ideal because multiple threads will be a problem, but we will leave that for another day.


```python
class NoGrad:
    '''Context manager that disables grad inside the block. Like torch.no_grad.'''

    was_enabled: bool

    def __enter__(self):
        '''
        Method which is called whenever the context manager is entered, i.e. at the 
        start of the `with NoGrad():` block.
        '''
        pass

    def __exit__(self, type, value, traceback):
        '''
        Method which is called whenever we exit the context manager.
        '''
        pass


```

<details>
<summary>Help - I'm not sure what to do here.</summary>

You should put `global grad_tracking_enabled` at the top of both methods.

In the `__enter__` method, you should disable gradient tracking (i.e. set the global variable to False).

In the `__exit__` method, you should set gradient tracking to whatever its global value was *before* you entered the context manager (this is why we need to use the `was_enabled` variable, so you can record what the global value was before you entered the context manager).
</details>

<details>
<summary>Solution</summary>


```python
class NoGrad:
    '''Context manager that disables grad inside the block. Like torch.no_grad.'''

    was_enabled: bool

    def __enter__(self):
        '''
        Method which is called whenever the context manager is entered, i.e. at the 
        start of the `with NoGrad():` block.
        '''
        # SOLUTION
        global grad_tracking_enabled
        self.was_enabled = grad_tracking_enabled
        grad_tracking_enabled = False

    def __exit__(self, type, value, traceback):
        '''
        Method which is called whenever we exit the context manager.
        '''
        # SOLUTION
        global grad_tracking_enabled
        grad_tracking_enabled = self.was_enabled
```
</details>


## Training Your Network

We've already looked at data loading and training loops earlier in the course, so we'll provide a minimal version of these today as well as the data loading code.


```python
train_loader, test_loader = get_mnist()
visualize(train_loader)
```

And here's a basic optimizer & training/testing loop:


```python
class SGD:
    def __init__(self, params: Iterable[Parameter], lr: float):
        '''Vanilla SGD with no additional features.'''
        self.params = list(params)
        self.lr = lr
        self.b = [None for _ in self.params]

    def zero_grad(self) -> None:
        for p in self.params:
            p.grad = None

    def step(self) -> None:
        with NoGrad():
            for (i, p) in enumerate(self.params):
                assert isinstance(p.grad, Tensor)
                p.add_(p.grad, -self.lr)


def train(model: MLP, train_loader: DataLoader, optimizer: SGD, epoch: int, train_loss_list: Optional[list] = None):
    print(f"Epoch: {epoch}")
    progress_bar = tqdm(enumerate(train_loader))
    for (batch_idx, (data, target)) in progress_bar:
        data = Tensor(data.numpy())
        target = Tensor(target.numpy())
        optimizer.zero_grad()
        output = model(data)
        loss = cross_entropy(output, target).sum() / len(output)
        loss.backward()
        progress_bar.set_description(f"Train set: Avg loss: {loss.item():.3f}")
        optimizer.step()
        if train_loss_list is not None: train_loss_list.append(loss.item())


def test(model: MLP, test_loader: DataLoader, test_loss_list: Optional[list] = None):
    test_loss = 0
    correct = 0
    with NoGrad():
        for (data, target) in test_loader:
            data = Tensor(data.numpy())
            target = Tensor(target.numpy())
            output: Tensor = model(data)
            test_loss += cross_entropy(output, target).sum().item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += (pred == target.reshape(pred.shape)).sum().item()
    test_loss /= len(test_loader.dataset)
    print(f"Test set:  Avg loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({correct / len(test_loader.dataset):.1%})")
    if test_loss_list is not None: test_loss_list.append(test_loss)

```

### Training Loop

To finish the day, let's see if everything works correctly and our MLP learns to classify MNIST. It's normal to encounter some bugs and glitches at this point - just go back and fix them until everything runs.


```python
num_epochs = 5
model = MLP()
start = time.time()
train_loss_list = []
test_loss_list = []
optimizer = SGD(model.parameters(), 0.01)
for epoch in range(num_epochs):
    train(model, train_loader, optimizer, epoch, train_loss_list)
    test(model, test_loader, test_loss_list)
    optimizer.step()
print(f"\nCompleted in {time.time() - start: .2f}s")

line(
    train_loss_list,
    yaxis_range=[0, max(train_loss_list) + 0.1],
    labels={"x": "Batches seen", "y": "Cross entropy loss"},
    title="ConvNet training on MNIST",
    width=800,
    hovermode="x unified",
    template="ggplot2", # alternative aesthetic for your plots (-:
)
```

Note - this training loop (if done correctly) will look to the one we used in earlier sections is that we're using SGD rather than Adam. You can try adapting your Adam code from the previous day's exercises, and get the same results as you have in earlier sections.




""", unsafe_allow_html=True)

