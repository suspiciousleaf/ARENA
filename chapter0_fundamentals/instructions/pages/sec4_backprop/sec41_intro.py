import streamlit as st


def section():

    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#reading'>Reading</a></li>
    <li class='margtop'><a class='contents-el' href='#computing-gradients-with-backpropagation'>Computing Gradients with Backpropagation</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#backward-functions'>Backward Functions</a></li>
        <li><a class='contents-el' href='#topological-ordering'>Topological Ordering</a></li>
        <li><a class='contents-el' href='#backpropagation'>Backpropagation</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#backward-function-of-log'>Backward function of log</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-log-back'><b>Exercise</b> - implement <code>log_back</code></a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#backward-functions-of-two-tensors'>Backward functions of two tensors</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#broadcasting-rules'>Broadcasting Rules</a></li>
        <li><a class='contents-el' href='#why-do-we-need-broadcasting-for-backprop'>Why do we need broadcasting for backprop?</a></li>
        <li><a class='contents-el' href='#exercise-implement-unbroadcast'><b>Exercise</b> - implement <code>unbroadcast</code></a></li>
        <li><a class='contents-el' href='#backward-function-for-elementwise-multiply'>Backward Function for Elementwise Multiply</a></li>
        <li><a class='contents-el' href='#exercise-implement-both-multiply-back-functions'><b>Exercise</b> - implement both <code>multiply_back</code> functions</a></li>
        <li><a class='contents-el' href='#exercise-implement-forward-and-back'><b>Exercise</b> - implement <code>forward_and_back</code></a></li>
</ul>""", unsafe_allow_html=True)

    st.markdown(
r"""

# Introduction to backprop

> ### Learning Objectives
> 
> * Understand what a computational graph is, and how it can be used to calculate gradients.
> * Start to implement backwards versions of some basic functions.

## Reading

* [Calculus on Computational Graphs: Backpropagation (Chris Olah)](https://colah.github.io/posts/2015-08-Backprop/)


## Computing Gradients with Backpropagation

This section will briefly review the backpropagation algorithm, but focus mainly on the concrete implementation in software.

To train a neural network, we want to know how the loss would change if we slightly adjust one of the learnable parameters.

One obvious and straightforward way to do this would be just to add a small value  to the parameter, and run the forward pass again. This is called finite differences, and the main issue is we need to run a forward pass for every single parameter that we want to adjust. This method is infeasible for large networks, but it's important to know as a way of sanity checking other methods.

A second obvious way is to write out the function for the entire network, and then symbolically take the gradient to obtain a symbolic expression for the gradient. This also works and is another thing to check against, but the expression gets quite complicated.

Suppose that you have some **computational graph**, and you want to determine the derivative of the some scalar loss L with respect to NumPy arrays a, b, and c:

<img src="https://raw.githubusercontent.com/callummcdougall/Fundamentals/main/images/abc_de_L.png" width="400">

This graph corresponds to the following Python:

```python
d = a * b
e = b + c
L = d + e
```

The goal of our system is that users can write ordinary looking Python code like this and have all the book-keeping needed to perform backpropagation happen behind the scenes. To do this, we're going to wrap each array and each function in special objects from our library that do the usual thing plus build up this graph structure that we need.

### Backward Functions

We've drawn our computation graph from left to right and the arrows pointing to the right, so that in the forward pass, boxes to the right depend on boxes to the left. In the backwards pass, the opposite is true: the gradient of boxes on the left depends on the gradient of boxes on the right.

If we want to compute the derivative of $L$ wrt all other variables (as was described in the reading), we should traverse the graph from right to left. Each time we encounter an instance of function application, we can use the chain rule from calculus to proceed one step further to the left. For example, if we have $d = a \times b$, then:

$$
\frac{dL}{da} = \frac{dL}{dd}\times \frac{dd}{da} = \frac{dL}{dd}\times b
$$

Suppose we are working from right to left, trying to calculate $\frac{dL}{da}$. If we already know the values of the variables $a$, $b$ and $d$, as well as the value of $\frac{dL}{dd}$, then we can use the following function to find $\frac{dL}{da}$:

$$
F(a, b, d, \frac{\partial L}{\partial d}) = \frac{\partial L}{\partial d}\cdot b
$$

and we can do something similar when trying to calculate $\frac{dL}{db}$.

In other words, we can take the **"forward function"** $(a, b) \to a \cdot b$, and for each of its parameters, we can define an associated **"backwards function"** which tells us how to compute the gradient wrt this argument using only known quantities as inputs.

Ignoring issues of unbroadcasting (which we'll cover later), we could write the backward with respect to the first argument as:

```python
def multiply_back(grad_out, out, a, b):
    '''
    Inputs:
        grad_out = dL/d(out)
        out = a * b

    Returns:
        dL/da
    '''
    return grad_out * b
```

where `grad_out` is the gradient of the loss with respect to the output of the function (i.e. $\frac{dL}{dd}$), `out` is the output of the function (i.e. $d$), and `a` and `b` are our inputs.


### Topological Ordering

When we're actually doing backprop, how do we guarantee that we'll always know the value of our backwards functions' inputs? For instance, in the example above we couldn't have computed $\frac{dL}{da}$ without first knowing $\frac{dL}{dd}$.

The answer is that we sort all our nodes using an algorithm called [topological sorting](https://en.wikipedia.org/wiki/Topological_sorting), and then do our computations in this order. After each computation, we store the gradients in our nodes for use in subsequent calculations.

When described in terms of the diagram above, topological sort can be thought of as an ordering of nodes from right to left. Crucially, this sorting has the following property: if there is a directed path in the computational graph going from node `x` to node `y`, then `x` must follow `y` in the sorting. 

There are many ways of proving that a cycle-free directed graph contains a topological ordering. You can try and prove this for yourself, or click on the expander below to reveal the outline of a simple proof.


<details>
<summary>Click to reveal proof</summary>

We can prove by induction on the number of nodes $N$. 
    
If $N=1$, the problem is trivial.

If $N>1$, then pick any node, and follow the arrows until you reach a node with no directed arrows going out of it. Such a node must exist, or else you would be following the arrows forever, and you'd eventually return to a node you previously visited, but this would be a cycle, which is a contradiction. Once you've found this "root node", you can put it first in your topological ordering, then remove it from the graph and apply the topological sort on the subgraph created by removing this node. By induction, your topological sorting algorithm on this smaller graph should return a valid ordering. If you append the root node to the start of this ordering, you have a topological ordering for the whole graph.
</details>


A quick note on some potentially confusing terminology. We will refer to the "end node" as the **root node**, and the "starting nodes" as **leaf nodes**. For instance, in the diagram at the top of the section, the left nodes `a`, `b` and `c` are the leaf nodes, and `L` is the root node. This might seem odd given it makes the leaf nodes come before the root nodes, but the reason is as follows: *when we're doing the backpropagation algorithm, we start at `L` and work our way back through the graph*. So, by our notation, we start at the root node and work our way out to the leaves.

Another important piece of terminology here is **parent node**. This means the same thing as it does in most other contexts - the parents of node `x` are all the nodes `y` with connections `y -> x` (so in the diagram, `L`'s parents are `d` and `e`).


<details>
<summary>Question - can you think of a reason it might be important for a node to store a list of all of its parent nodes?</summary>

During backprop, we're moving from right to left in the diagram. If a node doesn't store its parent, then there will be no way to get access to that parent node during backprop, so we can't propagate gradients to it.
</details>



The very first node in our topological sort will be $L$, the root node.



### Backpropagation

After all this setup, the backpropagation mechanism becomes pretty straightforward. We sort the nodes topologically, then we iterate over them and call each backward function exactly once in order to accumulate the gradients at each node.

It's important that the grads be accumulated instead of overwritten in a case like value $b$ which has two outgoing edges, since $\frac{dL}{db}$ will then be the sum of two terms. Since addition is commutative it doesn't matter whether we `backward()` the Mul or the Add that depend on $b$ first.

During backpropagation, for each forward function in our computational graph we need to find the partial derivative of the output with respect to each of its inputs. Each partial is then multiplied by the gradient of the loss with respect to the forward functions output (`grad_out`) to find the gradient of the loss with respect to each input. We'll handle these calculations using backward functions.


## Backward function of log

First, we'll write the backward function for `x -> out = log(x)`. This should be a function which, when fed the values `x, out, grad_out = dL/d(out)` returns the value of `dL/dx` just from this particular computational path.


<img src="https://raw.githubusercontent.com/callummcdougall/Fundamentals/main/images/x_log_out.png" width="400">


Note - it might seem strange at first why we need `x` and `out` to be inputs, `out` can be calculated directly from `x`. The answer is that sometimes it is computationally cheaper to express the derivative in terms of `out` than in terms of `x`.


<details>
<summary>Question - can you think of an example function where it would be computationally cheaper to use 'out' than to use 'x'?</summary>

The most obvious answer is the exponential function, `out = e ^ x`. Here, the gradient `d(out)/dx` is equal to `out`. We'll see this when we implement a backward version of `torch.exp` later today.
</details>


### Exercise - implement `log_back`

```yaml
Difficulty: 🔴🔴⚪⚪⚪
Importance: 🔵🔵🔵⚪⚪

You should spend up to 5-10 minutes on this exercise.
```


You should fill in this function below. Don't worry about division by zero or other edge cases - the goal here is just to see how the pieces of the system fit together.

*This should just be a short, one-line function.*


```python
def log_back(grad_out: Arr, out: Arr, x: Arr) -> Arr:
    '''Backwards function for f(x) = log(x)

    grad_out: Gradient of some loss wrt out
    out: the output of np.log(x).
    x: the input of np.log.

    Return: gradient of the given loss wrt x
    '''
    pass


tests.test_log_back(log_back)
```

<details>
<summary>Help - I'm not sure what the output of this backward function for log should be.</summary>

By the chain rule, we have:

$$
\frac{dL}{dx} = \frac{dL}{d(\text{out})} \cdot \frac{d(\text{out})}{dx} = \frac{dL}{d(\text{out})} \cdot \frac{d(\log{x})}{dx} = \frac{dL}{d(\text{out})} \cdot \frac{1}{x}
$$

---

(Note - technically, $\frac{d(\text{out})}{dx}$ is a tensor containing the derivatives of each element of $\text{out}$ with respect to each element of $x$, and we should matrix multiply when we use the chain rule. However, since $\text{out} = \log x$ is an elementwise function of $x$, our application of the chain rule here will also be an elementwise multiplication: $\frac{dL}{dx_{ij}} = \frac{dL}{d(\text{out}_{ij})} \cdot \frac{d(\text{out}_{ij})}{dx_{ij}}$. When we get to things like matrix multiplication later, we'll have to be a bit more careful!)
</details>

<details>
<summary>Solution</summary>


```python
def log_back(grad_out: Arr, out: Arr, x: Arr) -> Arr:
    '''Backwards function for f(x) = log(x)

    grad_out: Gradient of some loss wrt out
    out: the output of np.log(x).
    x: the input of np.log.

    Return: gradient of the given loss wrt x
    '''
    # SOLUTION
    return grad_out / x
```
</details>


## Backward functions of two tensors

Now we'll implement backward functions for multiple tensors. To do so, we first need to understand broadcasting.


### Broadcasting Rules

Both NumPy and PyTorch have the same rules for broadcasting. The shape of the arrays being operated on is compared element-wise, starting from the rightmost dimension and working left. Two dimensions are compatible when

* they are equal, or
* one of them is 1 (in which case the array is repeated along this dimension to fit into the other one).

Two arrays with a different number of dimensions can be operated on, provided the one with fewer dimensions is compatible with the rightmost elements of the one with more dimensions. Another way to picture this is that NumPy appends dimensions of size 1 to the start of the smaller-dimensional array until they both have the same dimensionality, and then their sizes are checked for compatibility.

As a warm-up exercise, below are some examples of broadcasting. Can you figure out which are valid, and which will raise errors?


```python
x = np.ones((3, 1, 5))
y = np.ones((1, 4, 5))

z = x + y
```

<details>
<summary>Answer</summary>

This is valid, because the 0th dimension of `y` and the 1st dimension of `x` can both be copied so that `x` and `y` have the same shape: `(3, 4, 5)`. The resulting array `z` will also have shape `(3, 4, 5)`.
</details>


```python
x = np.ones((8, 2, 6))
y = np.ones((8, 2))

z = x + y
```

<details>
<summary>Answer</summary>

This is not valid. We first need to expand `y` by appending a dimension to the front, and the last two dimensions of `x` are `(2, 6)`, which won't broadcast with `y`'s `(8, 2)`.
</details>


```python
x = np.ones((8, 2, 6))
y = np.ones((2, 6))

z = x + y
```

<details>
<summary>Answer</summary>

This is valid. Once NumPy expands `y` by appending a single dimension to the front, it can then be broadcast with `x`.
</details>


### Why do we need broadcasting for backprop?


Often, a tensor $x$ gets broadcasted to produce another tensor $x_{broadcasted}$, when being used to create $out$. It might be easy to define the derivative wrt $out$ and $x_{broadcasted}$, but we need to know how to go from this to calculating the derivative wrt $x$.

To take an example:

```python
x = t.ones(4,)
y = t.ones(3, 4)
out = x + y # = x_broadcasted + y
L = out[0, 0] + out[1, 1] + out[2, 1]
```


<img src="https://raw.githubusercontent.com/callummcdougall/Fundamentals/main/images/xy_add_out.png" width="400">


<img src="https://raw.githubusercontent.com/callummcdougall/Fundamentals/main/images/broadcast-2.png" width="400">


In this case, we have:

$$
\frac{dL}{d(out)} = \frac{dL}{dx_{broadcasted}} = \begin{bmatrix}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 1 & 0 & 0
\end{bmatrix}
$$

How do we get from this to $\frac{dL}{dx}$? Well, we can write $L$ as a function of $x$ (ignoring $y$ for now):

$$
\begin{aligned}
L &= x_{broadcasted}[0, 0] + x_{broadcasted}[1, 1] + x_{broadcasted}[2, 1] \\
&= x[0] + x[1] + x[1] \\
&= x[0] + 2x[1]
\end{aligned}
$$

meaning the derivative with respect to $x$ is:

$$
\frac{dL}{dx} = \begin{bmatrix}
1 & 2 & 0 & 0
\end{bmatrix}
$$

Note how we got this by taking $\frac{dL}{dx_{broadcasted}}$, and summing it over the dimension along which $x$ was broadcasted. This leads to our general rule for handling broadcasted operations:


> ##### Summary
> 
> If we know $\frac{dL}{d(out)}$, and want to know $\frac{dL}{dx}$ (where $x$ was broadcasted to produce $out$) then there are two steps:
> 
> 1. Compute $\frac{dL}{dx_{broadcasted}}$ in the standard way, i.e. using one of your backward functions (no broadcasting involved here).
> 2. ***Unbroadcast*** $\frac{dL}{dx_{broadcasted}}$, by summing it over the dimensions along which $x$ was broadcasted.


We used the term "unbroadcast" because the way that our tensor's shape changes will be the reverse of how it changed during broadcasting. If `x` was broadcasted from `(4,) -> (3, 4)`, then unbroadcasting will have to take a tensor of shape `(3, 4)` and sum over it to return a tensor of shape `(4,)`. Similarly, if `x` was broadcasted from `(1, 4) -> (3, 4)`, then we need to sum over the zeroth dimension, but leave it as a 2D tensor (with zeroth dimension of size 1).


### Exercise - implement `unbroadcast`

```yaml
Difficulty: 🔴🔴🔴⚪⚪
Importance: 🔵🔵⚪⚪⚪

You should spend up to 15-20 minutes on this exercise.

This can be finnicky to implement, so you should be willing to read the solution and move on if you get stuck.
```

Below, you should implement this function. `broadcasted` is the array you want to sum over, and `original` is the array with the shape you want to return. Your function should:

* Compare the shapes of `broadcasted` and `original`, and deduce (using broadcasting rules) how `original` was broadcasted to get `broadcasted`.
* Sum over the dimensions of `broadcasted` that were added by broadcasting, and return a tensor of shape `original.shape`.

Hint - the `.sum()` method (for NumPy arrays) takes arguments `axis` and `keepdims`. The `axis` argument is an int or list of ints to sum over, and `keepdims` is a boolean that determines whether you want to remove the dims you're summing over (if `False`) or leave them as dims of size 1 (if `True`). You'll need to use both arguments when implementing this function.


```python
def unbroadcast(broadcasted: Arr, original: Arr) -> Arr:
    '''
    Sum 'broadcasted' until it has the shape of 'original'.

    broadcasted: An array that was formerly of the same shape of 'original' and was expanded by broadcasting rules.
    '''
    pass


tests.test_unbroadcast(unbroadcast)
```

<details>
<summary>Help - I'm confused about implementing unbroadcast!</summary>

Recall that broadcasting `original -> broadcasted` has 2 steps:

1. Append dims of size 1 to the start of `original`, until it has the same number of dims as `broadcasted`.
2. Copy `original` along each dimension where it has size 1.

Similarly, your `unbroadcast` function should have 2 steps:

1. Sum over the dimensions at the start of `broadcasted`, until the result has the same number of dims as `original`. 
    * Here you should use `keepdims=False`, because you're trying to reduce the dimensionality of `broadcasted`.
2. Sum over the dimensions of `broadcasted` wherever `original` has size 1.
    * Here you should use `keepdims=True`, because you want to leave these dimensions having size 1 (so that the result has the same shape as `original`).
</details>
<details>
<summary>Solution</summary>


```python
def unbroadcast(broadcasted: Arr, original: Arr) -> Arr:
    '''
    Sum 'broadcasted' until it has the shape of 'original'.

    broadcasted: An array that was formerly of the same shape of 'original' and was expanded by broadcasting rules.
    '''
    # SOLUTION
    
    # Step 1: sum and remove prepended dims, so both arrays have same number of dims
    n_dims_to_sum = len(broadcasted.shape) - len(original.shape)
    broadcasted = broadcasted.sum(axis=tuple(range(n_dims_to_sum)))
    
    # Step 2: sum over dims which were originally 1 (but don't remove them)
    dims_to_sum = tuple([
        i for i, (o, b) in enumerate(zip(original.shape, broadcasted.shape))
        if o == 1 and b > 1
    ])
    broadcasted = broadcasted.sum(axis=dims_to_sum, keepdims=True)
    
    return broadcasted
```
</details>


### Backward Function for Elementwise Multiply

Functions that are differentiable with respect to more than one input tensor are straightforward given that we already know how to handle broadcasting.

- We're going to have two backwards functions, one for each input argument.
- If the input arguments were broadcasted together to create a larger output, the incoming `grad_out` will be of the larger common broadcasted shape and we need to make use of `unbroadcast` from earlier to match the shape to the appropriate input argument.
- We'll want our backward function to work when one of the inputs is an float. We won't need to calculate the grad_in with respect to floats, so we only need to consider when y is an float for `multiply_back0` and when x is an float for `multiply_back1`.


### Exercise - implement both `multiply_back` functions

```yaml
Difficulty: 🔴🔴🔴⚪⚪
Importance: 🔵🔵⚪⚪⚪

You should spend up to 10-15 minutes on these exercises.
```


Below, you should implement both `multiply_back0` and `multiply_back1`. 

You might be wondering why we need two different functions, rather than just having a single function to serve both purposes. This will become more important later on, once we deal with functions with more than one argument, which is not symmetric in its arguments. For instance, the derivative of $x / y$ wrt $x$ is not the same as the expression you get after differentiating this wrt $y$ then swapping the labels around.

The first part of each function has been provided for you (this makes sure that both inputs are arrays).

<details>
<summary>Help - I'm not sure how to use the <code>unbroadcast</code> function.</summary>

First, do the calculation assuming no broadcasting. Then, use `unbroadcast` to make sure the result has the same shape as the array you're trying to calculate the derivative with respect to.
</details>


```python
def multiply_back0(grad_out: Arr, out: Arr, x: Arr, y: Union[Arr, float]) -> Arr:
    '''Backwards function for x * y wrt argument 0 aka x.'''
    if not isinstance(y, Arr):
        y = np.array(y)
    pass

def multiply_back1(grad_out: Arr, out: Arr, x: Union[Arr, float], y: Arr) -> Arr:
    '''Backwards function for x * y wrt argument 1 aka y.'''
    if not isinstance(x, Arr):
        x = np.array(x)
    pass


tests.test_multiply_back(multiply_back0, multiply_back1)
tests.test_multiply_back_float(multiply_back0, multiply_back1)
```

<details>
<summary>Solution</summary>


```python
def multiply_back0(grad_out: Arr, out: Arr, x: Arr, y: Union[Arr, float]) -> Arr:
    '''Backwards function for x * y wrt argument 0 aka x.'''
    if not isinstance(y, Arr):
        y = np.array(y)
    # SOLUTION
    return unbroadcast(y * grad_out, x)

def multiply_back1(grad_out: Arr, out: Arr, x: Union[Arr, float], y: Arr) -> Arr:
    '''Backwards function for x * y wrt argument 1 aka y.'''
    if not isinstance(x, Arr):
        x = np.array(x)
    # SOLUTION
    return unbroadcast(x * grad_out, y)
```
</details>


Now we'll use our backward functions to do backpropagation manually, for the following computational graph:

<img src="https://raw.githubusercontent.com/callummcdougall/Fundamentals/main/images/abcdefg.png" width=550>


### Exercise - implement `forward_and_back`

```yaml
Difficulty: 🔴🔴🔴⚪⚪
Importance: 🔵🔵🔵🔵⚪

You should spend up to 15-20 minutes on these exercises.

This function is very useful for getting a hands-on sense of the backpropagation algorithm.
```

Below, you should implement the `forward_and_back` function. This is an opportunity for you to practice using the backward functions you've written so far, and should hopefully give you a better sense of how the full backprop function will eventually work.

Note - we're assuming all arrays in this graph have size 1, i.e. they're just scalars.


```python
def forward_and_back(a: Arr, b: Arr, c: Arr) -> Tuple[Arr, Arr, Arr]:
    '''
    Calculates the output of the computational graph above (g), then backpropogates the gradients and returns dg/da, dg/db, and dg/dc
    '''
    pass


tests.test_forward_and_back(forward_and_back)
```

<details>
<summary>Help - I'm not sure what my first 'grad_out' argument should be!</summary>

`grad_out` is $\frac{dL}{d(\text{out})}$, where $L$ is the node at the end of your graph and $\text{out}$ is the output of the function you're backpropagating through. For your first function (which is `log : f -> g`), the output is `g`, so your `grad_out` should be $\frac{dg}{dg} = 1$. (You should make this an array of size 1, since `g` is a scalar.)

The output of this function is $\frac{dg}{df}$, which you can use as the `grad_out` argument in subsequent backward funcs.
</details>
<details>
<summary>Solution</summary>


```python
def forward_and_back(a: Arr, b: Arr, c: Arr) -> Tuple[Arr, Arr, Arr]:
    '''
    Calculates the output of the computational graph above (g), then backpropogates the gradients and returns dg/da, dg/db, and dg/dc
    '''
    # SOLUTION
    d = a * b
    e = np.log(c)
    f = d * e
    g = np.log(f)
    
    final_grad_out = np.ones_like(g)
    dg_df = log_back(grad_out=final_grad_out, out=g, x=f)
    dg_dd = multiply_back0(dg_df, f, d, e)
    dg_de = multiply_back1(dg_df, f, d, e)
    dg_da = multiply_back0(dg_dd, d, a, b)
    dg_db = multiply_back1(dg_dd, d, a, b)
    dg_dc = log_back(dg_de, e, c)
    
    return (dg_da, dg_db, dg_dc)
```
</details>


In the next section, you'll build up to full automation of this backpropagation process, in a way that's similar to PyTorch's `autograd`.




""", unsafe_allow_html=True)

