import streamlit as st


def section():

    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#wrapping-arrays-tensor'>Wrapping Arrays (Tensor)</a></li>
    <li class='margtop'><a class='contents-el' href='#recipe'>Recipe</a></li>
    <li class='margtop'><a class='contents-el' href='#registering-backwards-functions'>Registering backwards functions</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-backwardfunclookup'><b>Exercise</b> - implement <code>BackwardFuncLookup</code></a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#tensors'>Tensors</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#requires-grad'>requires_grad</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#forward-pass:-building-the-computational-graph'>Forward Pass: Building the Computational Graph</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-log-forward'><b>Exercise</b> - implement <code>log_forward</code></a></li>
        <li><a class='contents-el' href='#exercise-implement-multiply-forward'><b>Exercise</b> - implement <code>multiply_forward</code></a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#forward-pass-generic-version'>Forward Pass - Generic Version</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-wrap-forward-fn'><b>Exercise</b> - implement <code>wrap_forward_fn</code></a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#backpropagation'>Backpropagation</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#topological-sort'>Topological Sort</a></li>
        <li><a class='contents-el' href='#exercise-implement-topological-sort'><b>Exercise</b> - implement <code>topological_sort</code></a></li>
        <li><a class='contents-el' href='#the-backward-method'>The <code>backward</code> method</a></li>
        <li><a class='contents-el' href='#end-grad'>End grad</a></li>
        <li><a class='contents-el' href='#leaf-nodes'>Leaf nodes</a></li>
        <li><a class='contents-el' href='#exercise-implement-backprop'><b>Exercise</b> - implement <code>backprop</code></a></li>
</ul>""", unsafe_allow_html=True)

    st.markdown(
r"""

# Autograd


> ### Learning Objectives
>
> * Perform a topological sort of a computational graph (and understand why this is important).
> * Implement a the `backprop` function, to calculate and store gradients for all tensors in a computational graph.


Now, rather than figuring out which backward functions to call, in what order, and what their inputs should be, we'll write code that takes care of that for us. We'll implement this with a few major components:
- Tensor
- Recipe
- wrap_forward_fn


## Wrapping Arrays (Tensor)

We're going to wrap each array with a wrapper object from our library which we'll call `Tensor` because it's going to behave similarly to a `torch.Tensor`.

Each Tensor that is created by one of our forward functions will have a `Recipe`, which tracks the extra information need to run backpropagation.

`wrap_forward_fn` will take a forward function and return a new forward function that does the same thing while recording the info we need to do backprop in the `Recipe`.


## Recipe

Let's start by taking a look at `Recipe`.

`@dataclass` is a handy class decorator that sets up an `__init__` function for the class that takes the provided attributes as arguments and sets them as you'd expect.

The class `Recipe` is designed to track the forward functions in our computational graph, so that gradients can be calculated during backprop. Each tensor created by a forward function has its own `Recipe`. We're naming it this because it is a set of instructions that tell us which ingredients went into making our tensor: what the function was, and what tensors were used as input to the function to produce this one as output.


```python
@dataclass(frozen=True)
class Recipe:
    '''Extra information necessary to run backpropagation. You don't need to modify this.'''

    func: Callable
    "The 'inner' NumPy function that does the actual forward computation."
    "Note, we call it 'inner' to distinguish it from the wrapper we'll create for it later on."

    args: tuple
    "The input arguments passed to func."
    "For instance, if func was np.sum then args would be a length-1 tuple containing the tensor to be summed."

    kwargs: Dict[str, Any]
    "Keyword arguments passed to func."
    "For instance, if func was np.sum then kwargs might contain 'dim' and 'keepdims'."
    
    parents: Dict[int, "Tensor"]
    "Map from positional argument index to the Tensor at that position, in order to be able to pass gradients back along the computational graph."

```

Note that `args` just stores the values of the underlying arrays, but `parents` stores the actual tensors. This is because they serve two different purposes: `args` is required for computing the value of gradients during backpropagation, and `parents` is required to infer the structure of the computational graph (i.e. which tensors were used to produce which other tensors).

Here are some examples, to build intuition for what the four fields of `Recipe` are, and why we need all four of them to fully describe a tensor in our graph and how it was created:

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/recipe-fixed-2.png" width="800">


## Registering backwards functions

The `Recipe` takes care of tracking the forward functions in our computational graph, but we still need a way to find the backward function corresponding to a given forward function when we do backprop (or possibly the set of backward functions, if the forward function takes more than one argument).


### Exercise - implement `BackwardFuncLookup`

```yaml
Difficulty: ðŸ”´ðŸ”´âšªâšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 10-15 minutes on these exercises.

These exercises should be very short, once you understand what is being asked.
```


We will define a class `BackwardFuncLookup` in order to find the backward function for a given forward function. Details of the implementation are left up to you.

The implementation today can be done very simply. We won't support backprop wrt keyword arguments and will raise an exception if the user tries to pass a Tensor by keyword. You can remove this limitation later if you have time.

We do need to support functions with multiple positional arguments like multiplication so we'll also provide the positional argument index when setting and getting back_fns.

If you're confused as to what this question is asking you to implement, you can look at the code below the class definition (which shows how a class instance can should be used to store and access backward functions).


```python
class BackwardFuncLookup:
    def __init__(self) -> None:
        pass

    def add_back_func(self, forward_fn: Callable, arg_position: int, back_fn: Callable) -> None:
        pass

    def get_back_func(self, forward_fn: Callable, arg_position: int) -> Callable:
        pass


BACK_FUNCS = BackwardFuncLookup()
BACK_FUNCS.add_back_func(np.log, 0, log_back)
BACK_FUNCS.add_back_func(np.multiply, 0, multiply_back0)
BACK_FUNCS.add_back_func(np.multiply, 1, multiply_back1)

assert BACK_FUNCS.get_back_func(np.log, 0) == log_back
assert BACK_FUNCS.get_back_func(np.multiply, 0) == multiply_back0
assert BACK_FUNCS.get_back_func(np.multiply, 1) == multiply_back1

print("Tests passed - BackwardFuncLookup class is working as expected!")
```

<details>
<summary>Example implementation</summary>

This implementation uses the useful [`collections.defaultdict`](https://docs.python.org/3/library/collections.html#collections.defaultdict) item.

Also, note the use of type annotations. This allows you to auto-fill appropriate methods when you work with these objects.

```python
class BackwardFuncLookup:
    def __init__(self) -> None:
        self.back_funcs: defaultdict[Callable, dict[int, Callable]] = defaultdict(dict)

    def add_back_func(self, forward_fn: Callable, arg_position: int, back_fn: Callable) -> None:
        self.back_funcs[forward_fn][arg_position] = back_fn

    def get_back_func(self, forward_fn: Callable, arg_position: int) -> Callable:
        return self.back_funcs[forward_fn][arg_position]
```
</details>


## Tensors

Our Tensor object has these fields:
- An `array` field of type `np.ndarray`.
- A `requires_grad` field of type `bool`.
- A `grad` field of the same size and type as the value.
- A `recipe` field, as we've already seen.


### requires_grad

The meaning of `requires_grad` is that when doing operations using this tensor, the recipe will be stored and it and any descendents will be included in the computational graph.

Note that `requires_grad` does not necessarily mean that we will save the accumulated gradients to this tensor's `.grad` parameter when doing backprop: we will follow pytorch's implementation of backprop and only save gradients to leaf tensors (see `Tensor.is_leaf`, below).

---

There is a lot of repetitive boilerplate involved which we have done for you. You don't need to modify anything in this class: the methods here will delegate to functions that you will implement throughout the day. You should read the code for the `Tensor` class up to `__init__`, and make sure you understand it. Most of the methods beyond this are just replicating the basic functionality of PyTorch tensors.

<br>

<details>
<summary><span style="font-size:18px; font-weight:bold">There's a lot of code in the block below, which is why it's been put in a dropdown (otherwise it's a pain to scroll past!). You should copy this code into your answers file and run it.</span></summary> 



```python
Arr = np.ndarray

class Tensor:
    '''
    A drop-in replacement for torch.Tensor supporting a subset of features.
    '''

    array: Arr
    "The underlying array. Can be shared between multiple Tensors."
    requires_grad: bool
    "If True, calling functions or methods on this tensor will track relevant data for backprop."
    grad: Optional["Tensor"]
    "Backpropagation will accumulate gradients into this field."
    recipe: Optional[Recipe]
    "Extra information necessary to run backpropagation."

    def __init__(self, array: Union[Arr, list], requires_grad=False):
        self.array = array if isinstance(array, Arr) else np.array(array)
        if self.array.dtype == np.float64:
            self.array = self.array.astype(np.float32)
        self.requires_grad = requires_grad
        self.grad = None
        self.recipe = None
        "If not None, this tensor's array was created via recipe.func(*recipe.args, **recipe.kwargs)."

    def __neg__(self) -> "Tensor":
        return negative(self)

    def __add__(self, other) -> "Tensor":
        return add(self, other)

    def __radd__(self, other) -> "Tensor":
        return add(other, self)

    def __sub__(self, other) -> "Tensor":
        return subtract(self, other)

    def __rsub__(self, other):
        return subtract(other, self)

    def __mul__(self, other) -> "Tensor":
        return multiply(self, other)

    def __rmul__(self, other) -> "Tensor":
        return multiply(other, self)

    def __truediv__(self, other) -> "Tensor":
        return true_divide(self, other)

    def __rtruediv__(self, other) -> "Tensor":
        return true_divide(other, self)

    def __matmul__(self, other) -> "Tensor":
        return matmul(self, other)

    def __rmatmul__(self, other) -> "Tensor":
        return matmul(other, self)

    def __eq__(self, other) -> "Tensor":
        return eq(self, other)

    def __repr__(self) -> str:
        return f"Tensor({repr(self.array)}, requires_grad={self.requires_grad})"

    def __len__(self) -> int:
        if self.array.ndim == 0:
            raise TypeError
        return self.array.shape[0]

    def __hash__(self) -> int:
        return id(self)

    def __getitem__(self, index) -> "Tensor":
        return getitem(self, index)

    def add_(self, other: "Tensor", alpha: float = 1.0) -> "Tensor":
        add_(self, other, alpha=alpha)
        return self

    @property
    def T(self) -> "Tensor":
        return permute(self, axes=(-1, -2))

    def item(self):
        return self.array.item()

    def sum(self, dim=None, keepdim=False):
        return sum(self, dim=dim, keepdim=keepdim)

    def log(self):
        return log(self)

    def exp(self):
        return exp(self)

    def reshape(self, new_shape):
        return reshape(self, new_shape)

    def expand(self, new_shape):
        return expand(self, new_shape)

    def permute(self, dims):
        return permute(self, dims)

    def maximum(self, other):
        return maximum(self, other)

    def relu(self):
        return relu(self)

    def argmax(self, dim=None, keepdim=False):
        return argmax(self, dim=dim, keepdim=keepdim)

    def uniform_(self, low: float, high: float) -> "Tensor":
        self.array[:] = np.random.uniform(low, high, self.array.shape)
        return self

    def backward(self, end_grad: Union[Arr, "Tensor", None] = None) -> None:
        if isinstance(end_grad, Arr):
            end_grad = Tensor(end_grad)
        return backprop(self, end_grad)

    def size(self, dim: Optional[int] = None):
        if dim is None:
            return self.shape
        return self.shape[dim]

    @property
    def shape(self):
        return self.array.shape

    @property
    def ndim(self):
        return self.array.ndim

    @property
    def is_leaf(self):
        '''Same as https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html'''
        if self.requires_grad and self.recipe and self.recipe.parents:
            return False
        return True

    def __bool__(self):
        if np.array(self.shape).prod() != 1:
            raise RuntimeError("bool value of Tensor with more than one value is ambiguous")
        return bool(self.item())

def empty(*shape: int) -> Tensor:
    '''Like torch.empty.'''
    return Tensor(np.empty(shape))

def zeros(*shape: int) -> Tensor:
    '''Like torch.zeros.'''
    return Tensor(np.zeros(shape))

def arange(start: int, end: int, step=1) -> Tensor:
    '''Like torch.arange(start, end).'''
    return Tensor(np.arange(start, end, step=step))

def tensor(array: Arr, requires_grad=False) -> Tensor:
    '''Like torch.tensor.'''
    return Tensor(array, requires_grad=requires_grad)
```

</details>

<br>

## Forward Pass: Building the Computational Graph

Let's start with a simple case: our `log` function. `log_forward` is a wrapper, which should implement the functionality of `np.log` but work with tensors rather than arrays.


### Exercise - implement `log_forward`

```yaml
Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 15-20 minutes on this exercise.
```

Our `log` function must do the following:

- Call `np.log` on the input *array* (i.e. the array attribute of the tensor).
- Create a new `Tensor` containing the output.
- If grad tracking is enabled globally AND (the input requires grad, OR has a recipe), then the output requires grad and we fill out the recipe of our output, as a `Recipe` object.

Later we'll redo this in a generic and reusable way, but for now just get it working.

Note - in these exercises, we've used the global variable `grad_tracking_enabled` to indicate whether gradient tracking is enabled. Often when you're working with a model and you don't need to train it (or perform backward passes for any other reason) it's useful to disable gradient tracking. In PyTorch, this is done with `t.set_grad_enabled(False)`.


```python
def log_forward(x: Tensor) -> Tensor:
    '''Performs np.log on a Tensor object.'''
    pass


log = log_forward
tests.test_log(Tensor, log_forward)
tests.test_log_no_grad(Tensor, log_forward)
a = Tensor([1], requires_grad=True)
grad_tracking_enabled = False
b = log_forward(a)
grad_tracking_enabled = True
assert not b.requires_grad, "should not require grad if grad tracking globally disabled"
assert b.recipe is None, "should not create recipe if grad tracking globally disabled"
```

<details>
<summary>Help - I need more hints on how to implement this function.</summary>

You need to define a tensor `out` by feeding it the underlying data (log of `x.array`) and the `requires_grad` flag.

Then, if `requires_grad` is true, you should also create a recipe object and store it in `out`. You can look at the diagrams above to see what the recipe should look like (it will be even simpler than the ones pictured, because there's only one parent, one arg, and no kwargs).
</details>

<details>
<summary>Solution</summary>


```python
class BackwardFuncLookup:
    def __init__(self) -> None:
        # SOLUTION
        self.back_funcs: defaultdict[Callable, dict[int, Callable]] = defaultdict(dict)

    def add_back_func(self, forward_fn: Callable, arg_position: int, back_fn: Callable) -> None:
        # SOLUTION
        self.back_funcs[forward_fn][arg_position] = back_fn

    def get_back_func(self, forward_fn: Callable, arg_position: int) -> Callable:
        # SOLUTION
        return self.back_funcs[forward_fn][arg_position]

def log_forward(x: Tensor) -> Tensor:
    '''Performs np.log on a Tensor object.'''
    # SOLUTION
    
    # Get the function argument as a numpy array
    # Calculate the output (which is also a numpy array)
    array = np.log(x.array)

    # Find whether the tensor requires grad
    requires_grad = grad_tracking_enabled and (x.requires_grad or (x.recipe is not None))

    # Create the output tensor from the underlying data and the requires_grad flag
    out = Tensor(array, requires_grad)
    
    # If requires_grad, then create a recipe
    if requires_grad:
        out.recipe = Recipe(func=np.log, args=(x.array,), kwargs={}, parents={0: x})
    else:
        out.recipe = None
        
    return out
```
</details>


Now let's do the same for multiply, to see how to handle functions with multiple arguments. 


### Exercise - implement `multiply_forward`

```yaml
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 15-20 minutes on this exercise.
```

There are a few differences between this and log:

- The actual function to be called is different
- We need more than one argument in `args` and `parents`, when defining `Recipe`
- `requires_grad` should be true if `grad_tracking_enabled=True`, and ANY of the input tensors require grad
- One of the inputs may be an int, so you'll need to deal with this case before calculating `out`

If you're confused, you can scroll up to the diagram at the top of the page (which tells you how to construct the recipe for functions like multiply or add when they are both arrays, or when one is an array and the other is a scalar).


```python
def multiply_forward(a: Union[Tensor, int], b: Union[Tensor, int]) -> Tensor:
    '''Performs np.multiply on a Tensor object.'''
    assert isinstance(a, Tensor) or isinstance(b, Tensor)

    pass


multiply = multiply_forward
tests.test_multiply(Tensor, multiply_forward)
tests.test_multiply_no_grad(Tensor, multiply_forward)
tests.test_multiply_float(Tensor, multiply_forward)
a = Tensor([2], requires_grad=True)
b = Tensor([3], requires_grad=True)
grad_tracking_enabled = False
b = multiply_forward(a, b)
grad_tracking_enabled = True
assert not b.requires_grad, "should not require grad if grad tracking globally disabled"
assert b.recipe is None, "should not create recipe if grad tracking globally disabled"
```

<details>
<summary>Help - I get "AttributeError: 'int' object has no attribute 'array'".</summary>

Remember that your multiply function should also accept integers. You need to separately deal with the cases where `a` and `b` are integers or Tensors.
</details>

<details>
<summary>Help - I get "AssertionError: assert len(c.recipe.parents) == 1 and c.recipe.parents[0] is a" in the "test_multiply_float" test.</summary>

This is probably because you've stored the inputs to `multiply` as integers when one of the is an integer. Remember, `parents` should just be a list of the **Tensors** that were inputs to `multiply`, so you shouldn't add ints.
</details>

<details>
<summary>Solution</summary>


```python
def multiply_forward(a: Union[Tensor, int], b: Union[Tensor, int]) -> Tensor:
    '''Performs np.multiply on a Tensor object.'''
    assert isinstance(a, Tensor) or isinstance(b, Tensor)

    # SOLUTION
    
    # Get all function arguments as non-tensors (i.e. either ints or arrays)
    arg_a = a.array if isinstance(a, Tensor) else a
    arg_b = b.array if isinstance(b, Tensor) else b

    # Calculate the output (which is a numpy array)
    out_arr = arg_a * arg_b
    assert isinstance(out_arr, np.ndarray)
    
    # Find whether the tensor requires grad (need to check if ANY of the inputs do)
    requires_grad = grad_tracking_enabled and any([
        (isinstance(x, Tensor) and (x.requires_grad or x.recipe is not None)) for x in (a, b)
    ])
    
    # Create the output tensor from the underlying data and the requires_grad flag
    out = Tensor(out_arr, requires_grad)
    
    # If requires_grad, then create a recipe
    if requires_grad:
        parents = {idx: arr for idx, arr in enumerate([a, b]) if isinstance(arr, Tensor)}
        out.recipe = Recipe(np.multiply, (arg_a, arg_b), {}, parents)
        
    return out
```
</details>


## Forward Pass - Generic Version

All our forward functions are going to look extremely similar to `log_forward` and `multiply_forward`. 
Implement the higher order function `wrap_forward_fn` that takes a `Arr -> Arr` function and returns a `Tensor -> Tensor` function. In other words, `wrap_forward_fn(np.multiply)` should evaluate to a callable that does the same thing as your `multiply_forward` (and same for `np.log`). 


### Exercise - implement `wrap_forward_fn`

```yaml
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 20-25 minutes on this exercise.

This exercise is conceptually important, but might be a bit difficult if it isn't clear what the question is asking for.
```


```python
def wrap_forward_fn(numpy_func: Callable, is_differentiable=True) -> Callable:
    '''
    numpy_func: Callable
        takes any number of positional arguments, some of which may be NumPy arrays, and 
        any number of keyword arguments which we aren't allowing to be NumPy arrays at 
        present. It returns a single NumPy array.
    
    is_differentiable: 
        if True, numpy_func is differentiable with respect to some input argument, so we 
        may need to track information in a Recipe. If False, we definitely don't need to
        track information.

    Return: Callable
        It has the same signature as numpy_func, except wherever there was a NumPy array, 
        this has a Tensor instead.
    '''

    def tensor_func(*args: Any, **kwargs: Any) -> Tensor:
        pass
        
    return tensor_func


def _sum(x: Arr, dim=None, keepdim=False) -> Arr:
    # need to be careful with sum, because kwargs have different names in torch and numpy
    return np.sum(x, axis=dim, keepdims=keepdim)


log = wrap_forward_fn(np.log)
multiply = wrap_forward_fn(np.multiply)
eq = wrap_forward_fn(np.equal, is_differentiable=False)
sum = wrap_forward_fn(_sum)

tests.test_log(Tensor, log)
tests.test_log_no_grad(Tensor, log)
tests.test_multiply(Tensor, multiply)
tests.test_multiply_no_grad(Tensor, multiply)
tests.test_multiply_float(Tensor, multiply)
tests.test_sum(Tensor)
```

<details>
<summary>Help - I'm not sure where to start.</summary>

Start with the code from `multiply_forward`. The way this function was structured (i.e. the five comments in the solution) should also give you a good template for how to structure your `tensor_func` function.
</details>

<details>
<summary>Help - I'm getting <code>NameError: name 'getitem' is not defined</code>.</summary>

This is probably because you're calling `numpy_func` on the args themselves. Recall that `args` will be a list of `Tensor` objects, and that you should call `numpy_func` on the underlying arrays.
</details>

<details>
<summary>Help - I'm getting an AssertionError on <code>assert c.requires_grad == True</code> (or something similar).</summary>

This is probably because you're not defining `requires_grad` correctly. Remember that the output of a forward function should have `requires_grad = True` if and only if all of the following hold:

* Grad tracking is enabled
* The function is differentiable
* **Any** of the inputs are tensors with `requires_grad = True`
</details>

<details>
<summary>Help - my function passes all tests up to <code>test_sum</code>, but then fails here.</summary>

`test_sum`, unlike the previous tests, wraps a function that uses keyword arguments. So if you're failing here, it's probably because you didn't use `kwargs` correctly.

`kwargs` should be used in two ways: once when actually calling the `numpy_func`, and once when defining the `Recipe` object for the output tensor.
</details>

<details>
<summary>Solution</summary>

```python
def wrap_forward_fn(numpy_func: Callable, is_differentiable=True) -> Callable:
    '''
    numpy_func: Callable
        takes any number of positional arguments, some of which may be NumPy arrays, and 
        any number of keyword arguments which we aren't allowing to be NumPy arrays at 
        present. It returns a single NumPy array.
    
    is_differentiable: 
        if True, numpy_func is differentiable with respect to some input argument, so we 
        may need to track information in a Recipe. If False, we definitely don't need to
        track information.

    Return: Callable
        It has the same signature as numpy_func, except wherever there was a NumPy array, 
        this has a Tensor instead.
    '''

    def tensor_func(*args: Any, **kwargs: Any) -> Tensor:
        # SOLUTION
        
        # Get all function arguments as non-tensors (i.e. either ints or arrays)
        arg_arrays = tuple([(a.array if isinstance(a, Tensor) else a) for a in args])
        
        # Calculate the output (which is a numpy array)
        out_arr = numpy_func(*arg_arrays, **kwargs)
        
        # Find whether the tensor requires grad (need to check if ANY of the inputs do)
        requires_grad = grad_tracking_enabled and is_differentiable and any([
            (isinstance(a, Tensor) and (a.requires_grad or a.recipe is not None)) for a in args
        ])

        # Create the output tensor from the underlying data and the requires_grad flag
        out = Tensor(out_arr, requires_grad)
        
        # If requires_grad, then create a recipe
        if requires_grad:
            parents = {idx: a for idx, a in enumerate(args) if isinstance(a, Tensor)}
            out.recipe = Recipe(numpy_func, arg_arrays, kwargs, parents)
        
        return out

    return tensor_func
```
</details>


Note - none of these functions involve keyword args, so the tests won't detect if you're handling kwargs incorrectly (or even failing to use them at all). If your code fails in later exercises, you might want to come back here and check that you're using the kwargs correctly. Alternatively, once you pass the tests, you can compare your code to the solutions and see how they handle kwargs.


## Backpropagation

Now all the pieces are in place to implement backpropagation. We need to:
- Loop over the nodes from right to left. At each node:
    - Call the backward function to transform the grad wrt output to the grad wrt input.
    - If the node is a leaf, write the grad to the grad field.
    - Otherwise, accumulate the grad into temporary storage.


### Topological Sort

As part of backprop, we need to sort the nodes of our graph so we can traverse the graph in the appropriate order.


### Exercise - implement `topological_sort`

```yaml
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª
Importance: ðŸ”µâšªâšªâšªâšª

You should spend up to 20-25 minutes on this exercise.

Note, it's completely fine to skip this problem if you're not very interested in it. This is more of a fun LeetCode-style puzzle, and writing a solution for this isn't crucial for the overall experience of these exercises.
```


Write a general function `topological_sort` that return a list of node's children in topological order (beginning with the furthest descendants, ending with the starting node) using [depth-first search](https://en.wikipedia.org/wiki/Topological_sorting). 

We've given you a `Node` class, with a `children` attribute, and a `get_children` function. You shouldn't change any of these, and your `topological_sort` function should use `get_children` to access a node's children rather than calling `node.children` directly. In subsequent exercises, we'll replace the `Node` class with the `Tensor` class (and using a different `get_children` function), so this will ensure your code still works for this new case.

If you're stuck, try looking at the pseudocode from some of [these examples](https://en.wikipedia.org/wiki/Topological_sorting#Kahn's_algorithm). 


```python
class Node:
    def __init__(self, *children):
        self.children = list(children)


def get_children(node: Node) -> List[Node]:
    return node.children


def topological_sort(node: Node, get_children: Callable) -> List[Node]:
    '''
    Return a list of node's descendants in reverse topological order from future to past (i.e. `node` should be last).
    
    Should raise an error if the graph with `node` as root is not in fact acyclic.
    '''
    pass



tests.test_topological_sort_linked_list(topological_sort)
tests.test_topological_sort_branching(topological_sort)
tests.test_topological_sort_rejoining(topological_sort)
tests.test_topological_sort_cyclic(topological_sort)
```

<details>
<summary>Help - my function is hanging without returning any values.</summary>

This is probably because it's going around in cycles when fed a cyclic graph. You should add a way of raising an error if your function detects that the graph isn't cyclic. One way to do this is to create a set `temp`, which stores the nodes you've visited on a particular excursion into the graph, then you can raise an error if you come across an already visited node.
</details>

<details>
<summary>Help - I'm completely stuck on how to implement this, and would like the template for some code.</summary>

Here is the template for a depth-first search implementation:

```python
def topological_sort(node: Node, get_children: Callable) -> list[Node]:
    
    result: List[Node] = [] # stores the list of nodes to be returned (in reverse topological order)
    perm: set[Node] = set() # same as `result`, but as a set (faster to check for membership)
    temp: set[Node] = set() # keeps track of previously visited nodes (to detect cyclicity)

    def visit(cur: Node):
        '''
        Recursive function which visits all the children of the current node, and appends them all
        to `result` in the order they were found.
        '''
        pass

    visit(node)
    return result
```
</details>

<details>
<summary>Solution</summary>


```python
class Node:
    def __init__(self, *children):
        self.children = list(children)


def get_children(node: Node) -> List[Node]:
    return node.children


def topological_sort(node: Node, get_children: Callable) -> List[Node]:
    '''
    Return a list of node's descendants in reverse topological order from future to past (i.e. `node` should be last).
    
    Should raise an error if the graph with `node` as root is not in fact acyclic.
    '''
    # SOLUTION
    
    result: List[Node] = [] # stores the list of nodes to be returned (in reverse topological order)
    perm: set[Node] = set() # same as `result`, but as a set (faster to check for membership)
    temp: set[Node] = set() # keeps track of previously visited nodes (to detect cyclicity)

    def visit(cur: Node):
        '''
        Recursive function which visits all the children of the current node, and appends them all
        to `result` in the order they were found.
        '''
        if cur in perm:
            return
        if cur in temp:
            raise ValueError("Not a DAG!")
        temp.add(cur)

        for next in get_children(cur):
            visit(next)

        result.append(cur)
        perm.add(cur)
        temp.remove(cur)

    visit(node)
    return result
```
</details>


Now, you should write the function `sorted_computational_graph`. This should be a short function (the main part of it is calling `topological_sort`), but there are a few things to keep in mind:

* You'll need a different `get_children` function for when you call `topological_sort`. This should actually return the **parents** of the tensor in question (sorry for the confusing terminology!).
* You should return the tensors in the order needed for backprop, in other words the `tensor` argument should be the first one in your list.


<img src="https://github.com/callummcdougall/Fundamentals/blob/main/images/abcdefg.png?raw=true" width=500>


```python
def sorted_computational_graph(tensor: Tensor) -> List[Tensor]:
    '''
    For a given tensor, return a list of Tensors that make up the nodes of the given Tensor's computational graph, 
    in reverse topological order (i.e. `tensor` should be first).
    '''
    pass   


a = Tensor([1], requires_grad=True)
b = Tensor([2], requires_grad=True)
c = Tensor([3], requires_grad=True)
d = a * b
e = c.log()
f = d * e
g = f.log()
name_lookup = {a: "a", b: "b", c: "c", d: "d", e: "e", f: "f", g: "g"}

print([name_lookup[t] for t in sorted_computational_graph(g)])
```

```python
a = Tensor([1], requires_grad=True)
b = a * 2
c = a * 1
d = b * c
name_lookup = {a: "a", b: "b", c: "c", d: "d"}

print([name_lookup[t] for t in sorted_computational_graph(d)])
```

<details>
<summary>Solution</summary>


```python
def sorted_computational_graph(tensor: Tensor) -> List[Tensor]:
    '''
    For a given tensor, return a list of Tensors that make up the nodes of the given Tensor's computational graph, 
    in reverse topological order (i.e. `tensor` should be first).
    '''
    # SOLUTION
    def get_parents(tensor: Tensor) -> List[Tensor]:
        if tensor.recipe is None:
            return []
        return list(tensor.recipe.parents.values())

    return topological_sort(tensor, get_parents)[::-1]
```
</details>


Compare your output with the computational graph. You should never be printing `x` before `y` if there is an edge `x --> ... --> y` (this should result in approximately reverse alphabetical order).


### The `backward` method

Now we're really ready for backprop!

Recall that in the implementation of the class `Tensor`, we had:

```python
class Tensor:

    def backward(self, end_grad: Union[Arr, "Tensor", None] = None) -> None:
        if isinstance(end_grad, Arr):
            end_grad = Tensor(end_grad)
        return backprop(self, end_grad)
```

In other words, for a tensor `out`, calling `out.backward()` is equivalent to `backprop(out)`.


### End grad

You might be wondering what role the `end_grad` argument in `backward` plays. We usually just call `out.backward()` when we're working with loss functions; why do we need another argument?

The reason is that we've only ever called `tensor.backward()` on scalars (i.e. tensors with a single element). If `tensor` is multi-dimensional, then we can get a scalar from it by taking a weighted sum of all of the elements. The elements of `end_grad` are precisely the coefficients of our weighted sum. In other words, calling `tensor.backward(end_grad)` implicitly does the following:

* Defines the value `L = (tensor * end_grad).sum()`
* Backpropagates from `L` to all the other tensors in the graph before `tensor`

So if `end_grad` is specified, it will be used as the `grad_out` argument in our first backward function. If `end_grad` is not specified, we assume `L = tensor.sum()`, i.e. `end_grad` is a tensor of all ones with the same shape as `tensor`.


### Leaf nodes

The `Tensor` object has an `is_leaf` property. Recall from the code earlier, we had:

```python
class Tensor:

    @property
    def is_leaf(self):
        '''Same as https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html'''
        if self.requires_grad and self.recipe and self.recipe.parents:
            return False
        return True
```

In other words, leaf node tensors are any with either `requires_grad=False`, or which were *not* created by some operation on other tensors. 

In backprop, only the leaf nodes with `requires_grad=True` accumulate gradients. You can think of leaf nodes as being end points of the computational graph, i.e. nodes from which you can't move further backwards topologically. 

For example, suppose we have a neural network with a single linear layer called `layer`, and it produces `output` when we pass in `input`. Then:

* `output` is not a leaf node, because it is the result of an operation on `layer.weight` and `input` (i.e. `recipe.parents` is not None)
* `input` is a leaf node because it has `requires_grad=False` (this is the default for tensors) and it wasn't created from anything (i.e. `recipe` is None). So gradients will stop propagating when they get to `input`, but it won't store any gradients.
* `layer.weight` is a leaf node because it wasn't created from anything (i.e. `recipe` is None). So gradients will stop propagating when they get to `layer.weight`, and it will store gradients (since `requires_grad=True`).

```python
layer = torch.nn.Linear(3, 4)
input = torch.ones(3)
output = layer(input)

print(layer.weight.is_leaf)       # -> True
print(layer.weight.requires_grad) # -> True

print(output.is_leaf)             # -> False

print(input.is_leaf)              # -> True
print(input.requires_grad)        # -> False
```

In the computational graph in the next section, the only leaves are `a`, `b` and `c`.


### Exercise - implement `backprop`

```yaml
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´ðŸ”´
Importance: ðŸ”µðŸ”µðŸ”µðŸ”µðŸ”µ

You should spend up to 30-40 minutes on this exercise.

This exercise is the most conceptually important today. You should be willing to spend around 30 minutes on it. We've provided several dropdowns to help you.
```


Now, we get to the actual backprop function! Some code is provided below, which you should complete.

If you want a challenge, you can try and implement it straight away, with out any help. However, because this is quite a challenging exercise, you can also use the dropdowns below. The first one gives you a sketch of the backpropagation algorithm, the second gives you a diagram which provides a bit more detail, and the third gives you the annotations for the function (so you just have to fill in the code). You are recommended to start by trying to implement it without help, but use the dropdowns (in order) if this is too difficult.

We've also provided a few dropdowns to address specific technical errors that can arise from implementing this function. If you're having trouble, you can use these to help you debug.

Either way, you're recommended to take some time with this function, as it's definitely the single most conceptually important exercise in the "Build Your Own Backpropagation Framework" section.


```python
def backprop(end_node: Tensor, end_grad: Optional[Tensor] = None) -> None:
    '''Accumulates gradients in the grad field of each leaf node.

    tensor.backward() is equivalent to backprop(tensor).

    end_node: 
        The rightmost node in the computation graph. 
        If it contains more than one element, end_grad must be provided.
    end_grad: 
        A tensor of the same shape as end_node. 
        Set to 1 if not specified and end_node has only one element.
    '''
    pass


tests.test_backprop(Tensor)
tests.test_backprop_branching(Tensor)
tests.test_backprop_requires_grad_false(Tensor)
tests.test_backprop_float_arg(Tensor)
tests.test_backprop_shared_parent(Tensor)
```

<details>
<summary>Dropdown #1 - sketch of algorithm</summary>

You should iterate through the computational graph, in the order returned by your function (i.e. from right to left). For each tensor, you need to do two things:

* If necessary, store the gradient in the `grad` field of the tensor. (This means you'll have to store the gradients in an external object, before setting them as attributes of the tensors.)
* For each of the tensor's parents, store the gradients of those tensors for this particular path through the graph (this will require calling your backward functions, which you should get from the `BACK_FUNCS` object).
</details>

<details>
<summary>Dropdown #2 - diagram of algorithm</summary>

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/backprop-2.png" width=800>
</details>

<details>
<summary>Dropdown #3 - annotations</summary>

Fill in the code beneath each annotation line that doesn't already have a line of code beneath it.

Most annotations only require one line of code below them.

```python
def backprop(end_node: Tensor, end_grad: Optional[Tensor] = None) -> None:

    # Get value of end_grad_arr

    # Create dictionary 'grads' to store gradients

    # Iterate through the computational graph, using your sorting function
    for node in sorted_computational_graph(end_node):
        
        # Get the outgradient from the grads dict
        
        # If this node is a leaf & requires_grad is true, then store the gradient
                
        # For all parents in the node:
            
        # If node has a recipe, then we iterate through parents (which is a dict of {arg_posn: tensor})
        for argnum, parent in node.recipe.parents.items():
            
            # Get the backward function corresponding to the function that created this node
            
            # Use this backward function to calculate the gradient

            # Add the gradient to this node in the dictionary `grads`
```
</details>


Specific technical issues:

<details>
<summary>Help - I get AttributeError: 'NoneType' object has no attribute 'func'</summary>

This error is probably because you're trying to access `recipe.func` from the wrong node. Possibly, you're calling your backward functions using the parents nodes' `recipe.func`, rather than the node's `recipe.func`.
        
To explain further, suppose your computational graph is simply:

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/Screenshot%202023-02-17%20174308.png" width=320>

When you reach `b` in your backprop iteration, you should calculate the gradient wrt `a` (the only parent of `b`) and store it in your `grads` dictionary, as `grads[a]`. In order to do this, you need the backward function for `func1`, which is stored in the node `b` (recall that the recipe of a tensor can be thought of as a set of instructions for how that tensor was created).
</details>

<details>
<summary>Help - I get AttributeError: 'numpy.ndarray' object has no attribute 'array'</summary>

This might be because you've set `node.grad` to be an array, rather than a tensor. You should store gradients as tensors (think of PyTorch, where `tensor.grad` will have type `torch.Tensor`).
        
It's fine to store numpy arrays in the `grads` dictionary, but when it comes time to set a tensor's grad attribute, you should use a tensor.
</details>

<details>
<summary>Help - I get 'RuntimeError: bool value of Tensor with more than one value is ambiguous'.</summary>

This error is probably because your computational graph function checks whether a tensor is in a list. The way these classes are compared for equality is a bit funky, and using sets rather than lists should make this error go away (i.e. checking whether a tensor is in a set should be fine).
</details>


And finally, the solution:

<details>
<summary>Solution</summary>

A note on the solution below - you might be wondering why we need to use the `grads` dict at all. Couldn't we just store gradients in nodes' `.grad` attribute, then set `node.grad = None` if it's *not* a leaf node?

The reason we don't do this is that, as a general rule, we never want to have non-None values for non-leaf tensors. We only ever store the gradients of non-leaves in the `grads` dictionary, to avoid having to store the gradients in the leaves themselves. This is a bit annoying, but it follows the behaviour of PyTorch.
```python
def backprop(end_node: Tensor, end_grad: Optional[Tensor] = None) -> None:
    '''Accumulates gradients in the grad field of each leaf node.

    tensor.backward() is equivalent to backprop(tensor).

    end_node: 
        The rightmost node in the computation graph. 
        If it contains more than one element, end_grad must be provided.
    end_grad: 
        A tensor of the same shape as end_node. 
        Set to 1 if not specified and end_node has only one element.
    '''
    # SOLUTION
    
    # Get value of end_grad_arr
    end_grad_arr = np.ones_like(end_node.array) if end_grad is None else end_grad.array
    
    # Create dict to store gradients
    grads: Dict[Tensor, Arr] = {end_node: end_grad_arr}

    # Iterate through the computational graph, using your sorting function
    for node in sorted_computational_graph(end_node):
        
        # Get the outgradient from the grads dict
        outgrad = grads.pop(node)
        # We only store the gradients if this node is a leaf & requires_grad is true
        if node.is_leaf and node.requires_grad:
            # Add the gradient to this node's grad (need to deal with special case grad=None)
            if node.grad is None:
                node.grad = Tensor(outgrad)
            else:
                node.grad.array += outgrad
                
        # If node has no parents, then the backtracking through the computational
        # graph ends here
        if node.recipe is None or node.recipe.parents is None:
            continue
            
        # If node has a recipe, then we iterate through parents (which is a dict of {arg_posn: tensor})
        for argnum, parent in node.recipe.parents.items():
            
            # Get the backward function corresponding to the function that created this node
            back_fn = BACK_FUNCS.get_back_func(node.recipe.func, argnum)
            
            # Use this backward function to calculate the gradient
            in_grad = back_fn(outgrad, node.array, *node.recipe.args, **node.recipe.kwargs)
            
            # Add the gradient to this node in the dictionary `grads`
            # Note that we only set node.grad (from the grads dict) in the code block above
            if parent not in grads:
                grads[parent] = in_grad
            else:
                grads[parent] += in_grad
```
</details>




""", unsafe_allow_html=True)
