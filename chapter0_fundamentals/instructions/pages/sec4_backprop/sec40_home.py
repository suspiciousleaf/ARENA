import streamlit as st


def section():

    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#introduction'>Introduction</a></li>
    <li class='margtop'><a class='contents-el' href='#content-learning-objectives'>Content & Learning Objectives</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#introduction'>Introduction</a></li>
        <li><a class='contents-el' href='#autograd'>Autograd</a></li>
        <li><a class='contents-el' href='#more-forward-backward-functions'>More forward & backward functions</a></li>
        <li><a class='contents-el' href='#putting-everything-together'>Putting everything together</a></li>
        <li><a class='contents-el' href='#bonus'>Bonus</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#setup'>Setup</a></li>
</ul>""", unsafe_allow_html=True)

    st.markdown(
r"""

# [0.4] - Build Your Own Backpropagation Framework

### Colab: [exercises](https://colab.research.google.com/drive/1el3ba9T6ORczG7prKzKe1iKZnR9xspYz) | [solutions](https://colab.research.google.com/drive/19fhocNzLbCmYDOsMKn9h8Ps7yTwnKNVN)

Please send any problems / bugs on the `#errata` channel in the [Slack group](https://join.slack.com/t/arena-uk/shared_invite/zt-28h0xs49u-ZN9ZDbGXl~oCorjbBsSQag), and ask any questions on the dedicated channels for this chapter of material.

If you want to change to dark mode, you can do this by clicking the three horizontal lines in the top-right, then navigating to Settings â†’ Theme.

Links to other chapters: [**(1) Transformers & Mech Interp**](https://arena3-chapter1-transformer-interp.streamlit.app/), [**(2) RL**](https://arena3-chapter2-rl.streamlit.app/).

<img src="https://raw.githubusercontent.com/callummcdougall/Fundamentals/main/images/backprop.png" width="350">

## Introduction

Today you're going to build your very own system that can run the backpropagation algorithm in essentially the same way as PyTorch does. By the end of the day, you'll be able to train a multi-layer perceptron neural network, using your own backprop system!

The main differences between the full PyTorch and our version are:

* We will focus on CPU only, as all the ideas are the same on GPU.
* We will use NumPy arrays internally instead of ATen, the C++ array type used by PyTorch. Backpropagation works independently of the array type.
* A real `torch.Tensor` has about 700 fields and methods. We will only implement a subset that are particularly instructional and/or necessary to train the MLP.

Note - for today, I'd lean a lot more towards being willing to read the solutions, and even move on from some of them if you don't fully understand them. The low-level messy implementation details for today are much less important than the high-level conceptual takeaways.

Also, if you don't have enough time to finish all sections (which is understandable, because there's a *lot* of content today!), I'd focus on sections **Introduction** and **Autograd**, since conceptually these are the most important. Once you've done both of these, you should have a strong working understanding of the mechanics of backpropagation. If you've finished these sections but you're still short on time, others I'd recommend taking a closer look at are the backwards functions for matrix multiplication (at the end of section 3) and the `NoGrad` context manager (near the end of section 4).

## Content & Learning Objectives

### Introduction to backprop

This takes you through what a **computational graph** is, and the basics of how gradients can be backpropagated through such a graph. You'll also implement the backwards versions of some basic functions: if we have tensors `output = func(input)`, then the backward function of `func` can calculate the grad of `input` as a function of the grad of `output`.

> ##### Learning Objectives
> 
> * Understand what a computational graph is, and how it can be used to calculate gradients.
> * Start to implement backwards versions of some basic functions.

### Autograd

This section goes into more detail on the backpropagation methodology. In order to find the `grad` of each tensor in a computational graph, we first have to perform a **topological sort** of the tensors in the graph, so that each time we try to calculate `tensor.grad`, we've already computed all the other gradients which are used in this calculation. We end this section by writing a `backprop` function, which works just like the `tensor.backward()` method you're already used to in PyTorch.

> ##### Learning Objectives
>
> * Perform a topological sort of a computational graph (and understand why this is important).
> * Implement a the `backprop` function, to calculate and store gradients for all tensors in a computational graph.

### More forward & backward functions

Now that we've established the basics, this section allows you to add more forward and backward functions, extending the set of functions we can use in our computational graph.

> ##### Learning Objectives
>
> * Implement more forward and backward functions, including for 
>   * Indexing
>   * Non-differentiable functions
>   * Matrix multiplication

### Putting everything together

In this section, we build your own equivalents of `torch.nn` features like `nn.Parameter`, `nn.Module`, and `nn.Linear`. We can then use these to build our own neural network to classify MINST data.

This completes the chain which starts at basic numpy arrays, and ends with us being able to build essentially any neural network architecture we want!

> ##### Learning Objectives
>
> * Complete the process of building up a neural network from scratch and training it via gradient descent.

### Bonus

A few bonus exercises are suggested, for pushing your understanding of backpropagation further.


## Setup

```python
import os
os.environ["KMP_DUPLICATE_LIB_OK"] ="TRUE"
import sys
import re
import time
import torch as t
import numpy as np
from pathlib import Path
from collections import defaultdict
from dataclasses import dataclass
from typing import Any, Callable, Iterator, Iterable, Optional, Union, Dict, List, Tuple
from torch.utils.data import DataLoader
from tqdm import tqdm

Arr = np.ndarray
grad_tracking_enabled = True

# Make sure exercises are in the path
chapter = r"chapter0_fundamentals"
exercises_dir = Path(f"{os.getcwd().split(chapter)[0]}/{chapter}/exercises").resolve()
section_dir = exercises_dir / "part4_backprop"
if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))

import part4_backprop.tests as tests
from part4_backprop.utils import visualize, get_mnist
from plotly_utils import line

MAIN = __name__ == "__main__"
```



""", unsafe_allow_html=True)

