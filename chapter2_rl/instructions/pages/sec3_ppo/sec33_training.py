import streamlit as st

def section():

    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#writing-your-training-loop'>Writing your training loop</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#data-logging'>Data Logging</a></li>
        <li><a class='contents-el' href='#video-logging'>Video Logging</a></li>
        <li><a class='contents-el' href='#exercise-complete-the-ppotrainer-class'><b>Exercise</b> - complete the <code>PPOTrainer</code> class</a></li>
        <li><a class='contents-el' href='#catastrophic-forgetting'>Catastrophic forgetting</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#reward-shaping'>Reward shaping</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-reward-shaping'><b>Exercise</b> - implement reward shaping</a></li>
    </ul></li>
</ul>""", unsafe_allow_html=True)

    st.markdown(
r"""
# Training Loop

> ### Learning objectives
>
> * Build a full training loop for the PPO algorithm
> * Train our agent, and visualise its performance with Weights & Biases media logger
> * Use reward shaping to improve your agent's training (and make it do tricks!)

## Writing your training loop

Finally, we can package this all together into our full training loop. Under this implementation, you have two main methods to fill in - `rollout_phase` and `learning_phase`. These will do the following:

* `rollout_phase`
    * Step the agent through the environment for `args.num_steps` total steps, collecting data into the replay memory.
    * If using `wandb`, log any relevant variables.
    * Return the most recent episode length (or `None` if no episodes terminated in any environment in this rollout phase) - this is for the progress bar, so you can see if your agent is improving.
        * Hint - the `infos` dicts returned by your agent's `play_step` method will contain key-value pair `("episode", {"l": episode_length, "r": episode_reward})`. Episode length is especially useful to log.
* `learning_phase`
    * Sample from the replay memory using `agent.get_minibatches()` (which returns a list of minibatches).
        * Note - calling this function also empties the memory, so you don't need to worry about doing this manually.
    * Iterate over these minibatches, and for each minibatch:
        * Calculate the total objective function.
        * Backpropagate the loss.
        * Update the agent's parameters using the optimizer.
        * **Clip the gradients** in accordance with [detail #11](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Global%20Gradient%20Clipping%20)
            * You can use `nn.utils.clip_grad_norm(parameters, max_norm)` for this - see [documentation page](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html).
        * Step the scheduler.
    * If using `wandb`, log any relevant variables.

We've also given you the template for `compute_ppo_objective`. You can fill this method in, then call it within `learning_phase`.

Lastly, we've written the `train` function for you (it's structured very similarly to the DQN `train` function from yesterday). The rough structure is:

```python
# initialise PPOArgs and PPOTrainer objects
for phase in args.total_phases:
    trainer.rollout_phase()
    trainer.learning_phase()
    # log any relevant variables
```

### Data Logging

You should mostly only focus on logging once you've got a mininal version of the code working. Once you do, you can try logging variables in the way described by [detail #12](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Debug%20variables). This will involve adding code to the `rollout_phase` and `learning_phase` methods.

Remember, you can log variables using `wandb.log({"variable_name": variable_value}, step=step)`.

### Video Logging

To control the frequency of video logging, you can edit the `make_env` function, which is found in the `part3_ppo.utils` file. In particular, the following code:

```python
env = gym.wrappers.RecordVideo(
    env, 
    f"videos/{run_name}", 
    episode_trigger = lambda x : x % video_log_freq == 0
)
```

creates a wrapper which logs video every `video_log_freq` episodes. You can change this argument, or you can use the argument `step_trigger` rather than `episode_trigger` to make the logging more uniform (because an episode-based trigger will log more frequently at first, because the episodes are shorter).

For now you can leave this code as it is, because the default values should work fine.

### Exercise - complete the `PPOTrainer` class

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´ðŸ”´
Importance: ðŸ”µðŸ”µðŸ”µðŸ”µðŸ”µ

You should spend up to 30-60 minutes on this exercise (including logging).
```

If you get stuck at any point during this implementation, you can look at the solutions or send a message in the Slack channel for help.

```python
class PPOTrainer:

    def __init__(self, args: PPOArgs):
        set_global_seeds(args.seed)
        self.args = args
        self.run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
        self.envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed + i, i, args.capture_video, self.run_name, args.mode) for i in range(args.num_envs)])
        self.agent = PPOAgent(self.args, self.envs).to(device)
        self.optimizer, self.scheduler = make_optimizer(self.agent, self.args.total_training_steps, self.args.learning_rate, 0.0)


    def rollout_phase(self) -> Optional[int]:
        '''
        This function populates the memory with a new set of experiences, using `self.agent.play_step`
        to step through the environment. It also returns the episode length of the most recently terminated
        episode (used in the progress bar readout).
        '''
        pass


    def learning_phase(self) -> None:
        '''
        This function does the following:

            - Generates minibatches from memory
            - Calculates the objective function, and takes an optimization step based on it
            - Clips the gradients (see detail #11)
            - Steps the learning rate scheduler
        '''
        pass


    def compute_ppo_objective(self, minibatch: ReplayMinibatch) -> Float[Tensor, ""]:
        '''
        Handles learning phase for a single minibatch. Returns objective function to be maximized.
        '''
        pass


    def train(self) -> None:
    
        if args.use_wandb: wandb.init(
            project=self.args.wandb_project_name,
            entity=self.args.wandb_entity,
            name=self.run_name,
            monitor_gym=self.args.capture_video
        )

        progress_bar = tqdm(range(self.args.total_phases))

        for epoch in progress_bar:

            last_episode_len = self.rollout_phase()
            if last_episode_len is not None:
                progress_bar.set_description(f"Epoch {epoch:02}, Episode length: {last_episode_len}")

            self.learning_phase()

        self.envs.close()
        if self.args.use_wandb:
            wandb.finish()
```

<details>
<summary>Solution (full)</summary>

```python
def rollout_phase(self) -> Optional[int]:
    '''
    This function populates the memory with a new set of experiences, using `self.agent.play_step`
    to step through the environment. It also returns the episode length of the most recently terminated
    episode (used in the progress bar readout).
    '''
    # SOLUTION
    last_episode_len = None
    for step in range(self.args.num_steps):
        infos = self.agent.play_step()
        for info in infos:
            if "episode" in info.keys():
                last_episode_len = info["episode"]["l"]
                last_episode_return = info["episode"]["r"]
                if self.args.use_wandb: wandb.log({
                    "episode_length": last_episode_len,
                    "episode_return": last_episode_return,
                }, step=self.agent.step)
    return last_episode_len


def learning_phase(self) -> None:
    '''
    This function does the following:

        - Generates minibatches from memory
        - Calculates the objective function, and takes an optimization step based on it
        - Clips the gradients (see detail #11)
        - Steps the learning rate scheduler
    '''
    # SOLUTION
    minibatches = self.agent.get_minibatches()
    for minibatch in minibatches:
        objective_fn = self.compute_ppo_objective(minibatch)
        objective_fn.backward()
        nn.utils.clip_grad_norm_(self.agent.parameters(), self.args.max_grad_norm)
        self.optimizer.step()
        self.optimizer.zero_grad()
        self.scheduler.step()


def compute_ppo_objective(self, minibatch: ReplayMinibatch) -> Float[Tensor, ""]:
    '''
    Handles learning phase for a single minibatch. Returns objective function to be maximized.
    '''
    # SOLUTION
    logits = self.agent.actor(minibatch.observations)
    probs = Categorical(logits=logits)
    values = self.agent.critic(minibatch.observations).squeeze()

    clipped_surrogate_objective = calc_clipped_surrogate_objective(probs, minibatch.actions, minibatch.advantages, minibatch.logprobs, self.args.clip_coef)
    value_loss = calc_value_function_loss(values, minibatch.returns, self.args.vf_coef)
    entropy_bonus = calc_entropy_bonus(probs, self.args.ent_coef)

    total_objective_function = clipped_surrogate_objective - value_loss + entropy_bonus

    with t.inference_mode():
        newlogprob = probs.log_prob(minibatch.actions)
        logratio = newlogprob - minibatch.logprobs
        ratio = logratio.exp()
        approx_kl = (ratio - 1 - logratio).mean().item()
        clipfracs = [((ratio - 1.0).abs() > self.args.clip_coef).float().mean().item()]
    if self.args.use_wandb: wandb.log(dict(
        total_steps = self.agent.step,
        values = values.mean().item(),
        learning_rate = self.scheduler.optimizer.param_groups[0]["lr"],
        value_loss = value_loss.item(),
        clipped_surrogate_objective = clipped_surrogate_objective.item(),
        entropy = entropy_bonus.item(),
        approx_kl = approx_kl,
        clipfrac = np.mean(clipfracs)
    ), step=self.agent.step)

    return total_objective_function
```

</details>

<details>
<summary>Solution (simple, no logging)</summary>

```python
def rollout_phase(self) -> Optional[int]:
    '''
    This function populates the memory with a new set of experiences, using `self.agent.play_step`
    to step through the environment. It also returns the episode length of the most recently terminated
    episode (used in the progress bar readout).
    '''
    # SOLUTION
    last_episode_len = None
    for step in range(self.args.num_steps):
        infos = self.agent.play_step()
        for info in infos:
            if "episode" in info.keys():
                last_episode_len = info["episode"]["l"]
    return last_episode_len


def learning_phase(self) -> None:
    '''
    This function does the following:

        - Generates minibatches from memory
        - Calculates the objective function, and takes an optimization step based on it
        - Clips the gradients (see detail #11)
        - Steps the learning rate scheduler
    '''
    # SOLUTION
    minibatches = self.agent.get_minibatches()
    for minibatch in minibatches:
        objective_fn = self.compute_ppo_objective(minibatch)
        objective_fn.backward()
        nn.utils.clip_grad_norm_(self.agent.parameters(), self.args.max_grad_norm)
        self.optimizer.step()
        self.optimizer.zero_grad()
        self.scheduler.step()


def compute_ppo_objective(self, minibatch: ReplayMinibatch) -> Float[Tensor, ""]:
    '''
    Handles learning phase for a single minibatch. Returns objective function to be maximized.
    '''
    # SOLUTION
    logits = self.agent.actor(minibatch.observations)
    probs = Categorical(logits=logits)
    values = self.agent.critic(minibatch.observations).squeeze()

    clipped_surrogate_objective = calc_clipped_surrogate_objective(probs, minibatch.actions, minibatch.advantages, minibatch.logprobs, self.args.clip_coef)
    value_loss = calc_value_function_loss(values, minibatch.returns, self.args.vf_coef)
    entropy_bonus = calc_entropy_bonus(probs, self.args.ent_coef)

    total_objective_function = clipped_surrogate_objective - value_loss + entropy_bonus

    return total_objective_function
```

</details>

Here's some code to run your model on the probe environments (and assert that they're all working fine).

A brief recap of the probe environments, along with recommendations of where to go to debug if one of them fails (note that these won't be true 100% of the time, but should hopefully give you some useful direction):

* **Probe 1 tests basic learning ability**. If this fails, it means the agent has failed to learn to associate a constant observation with a constant reward. You should check your loss functions and optimizers in this case.
* **Probe 2 tests the agent's ability to differentiate between 2 different observations (and learn their respective values)**. If this fails, it means the agent has issues with handling multiple possible observations.
* **Probe 3 tests the agent's ability to handle time & reward delay**. If this fails, it means the agent has problems with multi-step scenarios of discounting future rewards. You should look at how your agent step function works.
* **Probe 4 tests the agent's ability to learn from actions leading to different rewards**. If this fails, it means the agent has failed to change its policy for different rewards, and you should look closer at how your agent is updating its policy based on the rewards it receives & the loss function.
* **Probe 5 tests the agent's ability to map observations to actions**. If this fails, you should look at the code which handles multiple timesteps, as well as the code that handles the agent's map from observations to actions.

```python
def test_probe(probe_idx: int):
    '''
    Tests a probe environment by training a network on it & verifying that the value functions are
    in the expected range.
    '''
    # Train our network
    args = PPOArgs(
        env_id=f"Probe{probe_idx}-v0",
        exp_name=f"test-probe-{probe_idx}",
        total_timesteps=[5000, 5000, 10000, 20000, 20000][probe_idx-1],
        learning_rate=0.001,
        capture_video=False,
        use_wandb=False,
    )
    trainer = PPOTrainer(args)
    trainer.train()
    agent = trainer.agent

    # Get the correct set of observations, and corresponding values we expect
    obs_for_probes = [[[0.0]], [[-1.0], [+1.0]], [[0.0], [1.0]], [[0.0]], [[0.0], [1.0]]]
    expected_value_for_probes = [[[1.0]], [[-1.0], [+1.0]], [[args.gamma], [1.0]], [[1.0]], [[1.0], [1.0]]]
    expected_probs_for_probes = [None, None, None, [[0.0, 1.0]], [[1.0, 0.0], [0.0, 1.0]]]
    tolerances = [1e-3, 1e-3, 1e-3, 2e-3, 2e-3]
    obs = t.tensor(obs_for_probes[probe_idx-1]).to(device)

    # Calculate the actual value & probs, and verify them
    with t.inference_mode():
        value = agent.critic(obs)
        probs = agent.actor(obs).softmax(-1)
    expected_value = t.tensor(expected_value_for_probes[probe_idx-1]).to(device)
    t.testing.assert_close(value, expected_value, atol=tolerances[probe_idx-1], rtol=0)
    expected_probs = expected_probs_for_probes[probe_idx-1]
    if expected_probs is not None:
        t.testing.assert_close(probs, t.tensor(expected_probs).to(device), atol=tolerances[probe_idx-1], rtol=0)
    print("Probe tests passed!\n")


for probe_idx in range(1, 6):
    test_probe(probe_idx)
```

Once you've passed the tests for all 5 probe environments, you should test your model on Cartpole.

See an example wandb run you should be getting [here](https://api.wandb.ai/links/callum-mcdougall/fdmhh8gq).

```python
args = PPOArgs(use_wandb=True)
trainer = PPOTrainer(args)
trainer.train()
```

<details>
<summary>Question - if you've done this correctly (and logged everything), clipped surrogate objective will be close to zero. Should you infer from this that it's not important in the overall algorithm (compared to the components of the objective function which are larger)?</summary>

No, this doesn't mean that it's not important.

Clipped surrogate objective is a moving target. At each rollout phase, we generate new experiences, and the expected value of the clipped surrogate objective will be zero (because the expected value of advantages is zero). But this doesn't mean that differentiating clipped surrogate objective wrt the policy doesn't have a large gradient!

As we make update steps in the learning phase, the policy values $\pi(a_t \mid s_t)$ will increase for actions which have positive advantages, and decrease for actions which have negative advantages, so the clipped surrogate objective will no longer be zero in expectation. But (thanks to the fact that we're clipping changes larger than $\epsilon$) it will still be very small.

</details>

### Catastrophic forgetting

Note - you might see performance very high initially and then drop off rapidly (before recovering again).

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/cf2.png" width="600">

(This plot shows episodic return, which in this case is identical to episodic length.)

This is a well-known RL phenomena called **catastrophic forgetting**. It happens when the memory only contains good experiences, and the agent forgets how to recover from bad experiences. One way to fix this is to change your buffer to keep 10 of experiences from previous epochs, and 90% of experiences from the current epoch. Can you implement this?

(Note - reward shaping can also help fix this problem - see next section.)

## Reward Shaping

Recall the [docs](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) and [source code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) for the `CartPole` environment.

The current rewards for `CartPole` encourage the agent to keep the episode running for as long as possible, which it then needs to associate with balancing the pole.

Here, we inherit from `CartPoleEnv` so that we can modify the dynamics of the environment.

Try to modify the reward to make the task as easy to learn as possible. Compare this against your performance on the original environment, and see if the agent learns faster with your shaped reward. If you can bound the reward on each timestep between 0 and 1, this will make comparing the results to `CartPole-v1` easier.


<details>
<summary>Help - I'm not sure what I'm meant to return in this function.</summary>

The tuple `(obs, reward, done, info)` is returned from the CartPole environment. Here, `rew` is always 1 unless the episode has terminated.

You should change this, so that `reward` incentivises good behaviour, even if the pole hasn't fallen yet. You can use the information returned in `obs` to construct a new reward function.

</details>

<details>
<summary>Help - I'm confused about how to choose a reward function. (Try and think about this for a while before looking at this dropdown.)</summary>

Right now, the agent always gets a reward of 1 for each timestep it is active. You should try and change this so that it gets a reward between 0 and 1, which is closer to 1 when the agent is performing well / behaving stably, and equals 0 when the agent is doing very poorly.

The variables we have available to us are cart position, cart velocity, pole angle, and pole angular velocity, which I'll denote as $x$, $v$, $\theta$ and $\omega$.

Here are a few suggestions which you can try out:
* $r = 1 - (\theta / \theta_{\text{max}})^2$. This will have the effect of keeping the angle close to zero.
* $r = 1 - (x / x_{\text{max}})^2$. This will have the effect of pushing it back towards the centre of the screen (i.e. it won't tip and fall to the side of the screen).

You could also try using e.g. $|\theta / \theta_{\text{max}}|$ rather than $(\theta / \theta_{\text{max}})^2$. This would still mean reward is in the range (0, 1), but it would result in a larger penalty for very small deviations from the vertical position.

You can also try a linear combination of two or more of these rewards!
</details>


<details>
<summary>Help - my agent's episodic return is smaller than it was in the original CartPole environment.</summary>

This is to be expected, because your reward function is no longer always 1 when the agent is upright. Both your time-discounted reward estimates and your actual realised rewards will be less than they were in the cartpole environment.

For a fairer test, measure the length of your episodes - hopefully your agent learns how to stay upright for the entire 500 timestep interval as fast as or faster than it did previously.
</details>

Note - if you want to use the maximum possible values of `x` and `theta` in your reward function (to keep it bounded between 0 and 1) then you can. These values can be found at the [documentation page](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) (note - the actual values you'll want are given in the bullet points below the table, not in the table itself!). You can also use `self.x_threshold` and `self.theta_threshold_radians` to get these values directly (you can look at the source code for `CartPoleEnv` to see how these are calculated).

### Exercise - implement reward shaping

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 15-30 minutes on this exercise.
```

See [this link](https://api.wandb.ai/links/callum-mcdougall/p7e739rp) for what an ideal wandb run here should look like (using the reward function in the solutions).

```python
from gym.envs.classic_control.cartpole import CartPoleEnv

class EasyCart(CartPoleEnv):
    def step(self, action):
        (obs, reward, done, info) = super().step(action)
        x, v, theta, omega = obs
        pass

        
gym.envs.registration.register(id="EasyCart-v0", entry_point=EasyCart, max_episode_steps=500)
args = PPOArgs(env_id="EasyCart-v0", use_wandb=True)
trainer = PPOTrainer(args)
trainer.train()
```

<details>
<summary>Solution (one possible implementation)</summary>

I tried out a few different simple reward functions here. One of the best ones I found used a mix of absolute value penalties for both the angle and the horizontal position (this outperformed using absolute value penalty for just one of these two). My guess as to why this is the case - penalising by horizontal position helps the agent improve its long-term strategy, and penalising by angle helps the agent improve its short-term strategy, so both combined work better than either on their own.

```python
class EasyCart(CartPoleEnv):
    def step(self, action):
        (obs, rew, done, info) = super().step(action)
        x, v, theta, omega = obs
        
        # First reward: angle should be close to zero
        reward_1 = 1 - abs(theta / 0.2095)
        # Second reward: position should be close to the center
        reward_2 = 1 - abs(x / 2.4)

        # Combine both rewards (keep it in the [0, 1] range)
        reward_new = (reward_1 + reward_2) / 2

        return (obs, reward_new, done, info)
```

The result:

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/best-episode-length.png" width="600">

To illustrate the point about different forms of reward optimizing different kinds of behaviour - below are links to three videos generated during the WandB training, one of just position penalisation, one of just angle penalisation, and one of both. Can you guess which is which?

* [First video](https://wandb.ai//callum-mcdougall/PPOCart/reports/videos-23-07-07-13-48-08---Vmlldzo0ODI1NDcw?accessToken=uajtb4w1gaqkbrf2utonbg2b93lfdlw9eaet4qd9n6zuegkb3mif7l3sbuke8l4j)
* [Second video](https://wandb.ai//callum-mcdougall/PPOCart/reports/videos-23-07-07-13-47-22---Vmlldzo0ODI1NDY2?accessToken=qoss34zyuaso1b5s40nehamsk7nj93ijopmscesde6mjote0i194e7l99sg2k6dg)
* [Third video](https://wandb.ai//callum-mcdougall/PPOCart/reports/videos-23-07-07-13-45-15---Vmlldzo0ODI1NDQ4?accessToken=n1btft5zfqx0aqk8wkuh13xtp5mn19q5ga0mpjmvjnn2nq8q62xz4hsomd0vnots)

<details>
<summary>Answer</summary>

* First video = angle penalisation
* Second video = both (from the same video as the loss curve above)
* Third video = position penalisation

</details>

</details>

Now, change the environment such that the reward incentivises the agent to spin very fast. You may change the termination conditions of the environment (i.e. return a different value for `done`) if you think this will help.

See [this link](https://api.wandb.ai/links/callum-mcdougall/86y2vtsk) for what an ideal wandb run here should look like (using the reward function in the solutions).

```python
class SpinCart(CartPoleEnv):

    def step(self, action):
        obs, reward, done, info = super().step(action)
        x, v, theta, omega = obs
        pass


gym.envs.registration.register(id="SpinCart-v0", entry_point=SpinCart, max_episode_steps=500)
args = PPOArgs(env_id="SpinCart-v0", use_wandb=True)
trainer = PPOTrainer(args)
trainer.train()
```

<details>
<summary>Solution (one possible implementation)</summary>

```python
class SpinCart(CartPoleEnv):

    def step(self, action):
        obs, reward, done, info = super().step(action)
        x, v, theta, omega = obs
        # Allow for 360-degree rotation (but keep the cart on-screen)
        done = (abs(x) > self.x_threshold)
        # Reward function incentivises fast spinning while staying still & near centre
        rotation_speed_reward = min(1, 0.1*abs(omega))
        stability_penalty = max(1, abs(x/2.5) + abs(v/10))
        reward_new = rotation_speed_reward - 0.5 * stability_penalty
        return (obs, reward_new, done, info)
```

</details>

Another thing you can try is "dancing". It's up to you to define what qualifies as "dancing" - work out a sensible definition, and the reward function to incentive it.

""", unsafe_allow_html=True)

