import streamlit as st

def section():

    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#____'>____</a></li>
    <li class='margtop'><a class='contents-el' href='#____'>____</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#____'>____</a></li>
        <li><a class='contents-el' href='#____'>____</a></li>
    </ul></li>
</ul>""", unsafe_allow_html=True)

    st.markdown(
r"""
# Mujoco

> ### Learning objectives
>
> * Understand how PPO can be used to train agents in continuous action spaces
> * Install and interact with the MuJoCo physics engine
> * Train an agent to solve the Hopper environment

## Installation & Rendering

Run the following to install the necessary packages.

```python
!sudo apt-get install -y \
    libgl1-mesa-dev \
    libgl1-mesa-glx \
    libglew-dev \
    libosmesa6-dev \
    software-properties-common

!sudo apt-get install -y patchelf

%pip install free-mujoco-py
%pip install mujoco
```

To test that this works, run the following. The first time you run this, it might take about 1-2 minutes, and throw up several warnings and messages. But the cell should still run without raising an exception, and all subsequent times you run it, it should be a lot faster (with no error messages).

```python
envs = gym.make("Hopper-v3")
```

You can see what the environment looks like with the following code (which saves a simple animation that you should be able to see in your local storage - it might not play inline).

```python
image_list = []
env = gym.make('Hopper-v3')
obs = env.reset()
for _ in tqdm(range(150)):
    action = env.action_space.sample()
    obs, reward, done, info = env.step(action)
    image = env.render(mode="rgb_array")
    image_list.append(image)

imgs = np.stack(image_list)
fig, ax = plt.subplots()
image = ax.imshow(imgs[0])
def update(frame):
    image.set_array(imgs[frame])
    return image,
animation = FuncAnimation(fig, update, frames=imgs.shape[0], blit=True, interval=30)
animation.save(str(section_dir / 'animation.mp4'), dpi=80, writer='ffmpeg');
```

## Action space

Previously, we've dealt with discrete action spaces (e.g. going right or left in Cartpole).

```python
env = gym.make("CartPole-v0")
print(env.action_space)
print(env.action_space.sample())
print(env.action_space.shape)
```

But here, we have a continuous action space:

```python
env = gym.make("Hopper-v3")
print(env.action_space)
print(env.action_space.sample())
print(env.action_space.shape)
```

In other words we have 3 different actions, each of which can be a float between -1 and 1.

Question - after reading the [documentation page](https://www.gymlibrary.dev/environments/mujoco/hopper/), can you see what these three actions mean?

<details>
<summary>Answer</summary>

They represent the **torque** applied between the three different links of the hopper. There is:

* The **thigh rotor** (i.e. connecting the upper and middle parts of the leg),
* The **leg rotor** (i.e. connecting the middle and lower parts of the leg),
* The **foot rotor** (i.e. connecting the lower part of the leg to the foot).

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/hopper-torque.png" width="400">

</details>

How do we deal with a continuous action space, when it comes to choosing actions? Rather than our actor network's output being a vector of `logits` which we turn into a probability distribution via `Categorical(logits=logits)`, we instead have our actor output two vectors `mu` and `log_sigma`, which we turn into a normal distribution which is then sampled from.

## Implementational details of MuJoCo

### Clipping, Scaling & Normalisation ([details #5-9](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Handling%20of%20action%20clipping%20to%20valid%20range%20and%20storage))

Just like for Atari, there are a few messy implementational details which will be taken care of with gym wrappers. For example, if we generate our actions by sampling from a normal distribution, then there's some non-zero chance that our action will be outside of the allowed action space. We deal with this by clipping the actions to be within the allowed range (in this case between -1 and 1).

See the function `prepare_mujoco_env` within `part3_ppo/utils` (and read details 5-9 on the PPO page) for more information.

### Actor and Critic networks ([details #1-4](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Continuous%20actions%20via%20normal%20distributions))

Our actor and critic networks are quite similar to the ones we used for cartpole. They won't have shared architecture.

<details>
<summary>Question - can you see why it's less useful to have shared architecture in this case, relative to the case of Atari?</summary>

The point of the shared architecture in Atari was that it allowed our critic and actor to perform **feature extraction**, i.e. the early part of the network (which was fed the raw pixel input) generated a high-level representation of the state, which was then fed into the actor and critic heads. But for CartPole and for MuJoCo, we have a very small observation space (4 discrete values in the case of CartPole, 11 for the Hopper in MuJoCo), so there's no feature extraction necessary.

</details>

The only difference will be in the actor network. There will be an `actor_mu` and `actor_log_sigma` network. The `actor_mu` will have exactly the same architecture as the CartPole actor network, and it will output a vector used as the mean of our normal distribution. The `actor_log_sigma` network will just be a bias, since the standard deviation is **state-independent** ([detail #2](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=State%2Dindependent%20log%20standard%20deviation)).

Because of this extra complexity, we'll create a class for our actor and critic networks.

### Exercise - implement `Actor` and `Critic`

```c
Difficulty: ðŸ”´ðŸ”´âšªâšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 10-15 minutes on this exercise.
```

As discussed, the architecture of `actor_mu` is identical to your cartpole actor network, and the critic is identical. The only difference is the addition of `actor_log_sigma` (which is given to you in the `__init__`).

Your `Actor` class's forward function should return a tuple of `(mu, sigma, dist)`, where `mu` and `sigma` are the parameters of the normal distribution, and `dist` was created from these values using `torch.distributions.Normal`.

<details>
<summary>Why do we use <code>log_sigma</code> rather than just outputting <code>sigma</code> ?</summary>

We have our network output `log_sigma` rather than `sigma` because the standard deviation is always positive. If we learn the log standard deviation rather than the standard deviation, then we can treat it just like a regular learned weight.
</details>

Tip - when creating your distribution, you can use the `broadcast_to` tensor method, so that your standard deviation and mean are the same shape.

We've given you the function `get_actor_and_critic_mujoco` (which is called when you call `get_actor_and_critic` with `mode="mujoco"`). All you need to do is fill in the `Actor` and `Critic` classes.

```python
class Critic(nn.Module):
    def __init__(self, num_obs):
        super().__init__()
        # YOUR CODE HERE - define critic
        pass

    def forward(self, obs) -> Tensor:
        # YOUR CODE HERE - fwd pass, return value
        pass


class Actor(nn.Module):
    actor_mu: nn.Sequential
    actor_log_sigma: nn.Parameter

    def __init__(self, num_obs, num_actions):
        super().__init__()
        # YOUR CODE HERE - define actor_mu and actor_log_sigma
        pass

    def forward(self, obs) -> Tuple[Tensor, Tensor, t.distributions.Normal]:
        # YOUR CODE HERE - fwd pass, return (mu, sigma, dist)
        pass


def get_actor_and_critic_mujoco(num_obs: int, num_actions: int):
    '''
    Returns (actor, critic) in the "classic-control" case, according to description above.
    '''
    return Actor(num_obs, num_actions), Critic(num_obs)


tests.test_get_actor_and_critic(get_actor_and_critic, mode="mujoco")
```

<details>
<summary>Solution</summary>

```python
class Critic(nn.Module):
    def __init__(self, num_obs):
        super().__init__()
        # SOLUTION
        self.critic = nn.Sequential(
            layer_init(nn.Linear(num_obs, 64)),
            nn.Tanh(),
            layer_init(nn.Linear(64, 64)),
            nn.Tanh(),
            layer_init(nn.Linear(64, 1), std=1.0)
        )

    def forward(self, obs) -> Tensor:
        # SOLUTION
        value = self.critic(obs)
        return value


class Actor(nn.Module):
    actor_mu: nn.Sequential
    actor_log_sigma: nn.Parameter

    def __init__(self, num_obs, num_actions):
        super().__init__()
        # SOLUTION
        self.actor_mu = nn.Sequential(
            layer_init(nn.Linear(num_obs, 64)),
            nn.Tanh(),
            layer_init(nn.Linear(64, 64)),
            nn.Tanh(),
            layer_init(nn.Linear(64, num_actions), std=0.01),
        )
        self.actor_log_sigma = nn.Parameter(t.zeros(1, num_actions))

    def forward(self, obs) -> Tuple[Tensor, Tensor, t.distributions.Normal]:
        # SOLUTION
        mu = self.actor_mu(obs)
        sigma = t.exp(self.actor_log_sigma).broadcast_to(mu.shape)
        dist = t.distributions.Normal(mu, sigma)
        return mu, sigma, dist
```

</details>

### Exercise - additional rewrites

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª
Importance: ðŸ”µðŸ”µâšªâšªâšª

You should spend up to 10-25 minutes on this exercise.
```

There are a few more rewrites you'll need for continuous action spaces, which is why we recommend that you create a new solutions file for this part (like we've done with `solutions.py` and `solutions_cts.py`).

You'll need to make the following changes:

#### Logprobs and entropy

Rather than `probs = Categorical(logits=logits)` as your distribution (which you sample from & pass into your loss functions), you'll just use `dist` as your distribution. Methods like `.logprobs(action)` and `.entropy()` will work on `dist` just like they did on `probs`.

Note that these two methods will return objects of shape `(batch_size, action_shape)` (e.g. for Hopper the last dimension will be 3). We treat the action components as independent ([detail #4](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Independent%20action%20components)), meaning **we take a product of the probabilities, so we sum the logprobs / entropies**. For example:

$$
\begin{aligned}
\operatorname{prob}\left(a_t\right)&=\operatorname{prob}\left(a_t^1\right) \cdot \operatorname{prob}\left(a_t^2\right) \\
\log\left(a_t\right)&=\log\left(a_t^1\right) + \log\left(a_t^2\right)
\end{aligned}
$$

So you'll need to sum logprobs and entropy over the last dimension. The logprobs value that you add to the replay memory should be summed over (because you don't need the individual logprobs, you only need the logprob of the action as a whole).

#### Logging

You should log `mu` and `sigma` during the learning phase.

```python
YOUR CODE HERE - make the necessary changes to your previous classes & functions
```

The dropdown below contains all the code you'll need to rewrite, removed of all comments except for those saying `# CHANGED ...`, to indicate that the following line of code has been changed from what it was in the previous chapters.

Note - make sure you remember to re-run the `train` function, otherwise you could be stuck with the older version of `train` (corresponding to your non-updated functions), even though you don't actually need a different `train` function.

<details>
<summary>Solution (all the code you'll need to rewrite)</summary>

```python
class PPOAgent(nn.Module):
    critic: Critic
    actor: Actor

    def __init__(self, args: PPOArgs, envs: gym.vector.SyncVectorEnv):
        super().__init__()
        self.args = args
        self.envs = envs

        self.step = 0

        self.actor, self.critic = get_actor_and_critic(envs, mode=args.mode)

        self.next_obs = t.tensor(envs.reset()).to(device, dtype=t.float)
        self.next_done = t.zeros(envs.num_envs).to(device, dtype=t.float)

        self.memory = ReplayMemory(args, envs)


    def play_step(self) -> List[dict]:
        '''
        Carries out a single interaction step between the agent and the environment, and adds results to the replay buffer.

        Returns the list of info dicts returned from `self.envs.step`.
        '''
        obs = self.next_obs
        dones = self.next_done

        with t.inference_mode():
            # CHANGED (return (mu, sigma, dist), and use dist to sample actions)
            mu, sigma, dist = self.actor.forward(obs)
        actions = dist.sample()

        next_obs, rewards, next_dones, infos = self.envs.step(actions.cpu().numpy())

        # CHANGED (sum over action space to get logprobs)
        logprobs = dist.log_prob(actions).sum(-1)
        with t.inference_mode():
            values = self.critic.forward(obs).flatten()
        self.memory.add(obs, actions, logprobs, values, rewards, dones)

        self.next_obs = t.from_numpy(next_obs).to(device, dtype=t.float)
        self.next_done = t.from_numpy(next_dones).to(device, dtype=t.float)
        self.step += self.envs.num_envs

        return infos


    def get_minibatches(self) -> None:
        '''
        Gets minibatches from the replay buffer.
        '''
        with t.inference_mode():
            next_value = self.critic(self.next_obs).flatten()
        return self.memory.get_minibatches(next_value, self.next_done)


def calc_clipped_surrogate_objective(
    dist: t.distributions.Normal,
    mb_action: Int[Tensor, "minibatch_size *action_shape"],
    mb_advantages: Float[Tensor, "minibatch_size"],
    mb_logprobs: Float[Tensor, "minibatch_size"],
    clip_coef: float,
    eps: float = 1e-8,
) -> Float[Tensor, ""]:
    '''Return the clipped surrogate objective, suitable for maximisation with gradient ascent.

    dist:
        a distribution containing the actor's unnormalized logits of shape (minibatch_size, num_actions)
    mb_action:
        what actions actions were taken in the sampled minibatch
    mb_advantages:
        advantages calculated from the sampled minibatch
    mb_logprobs:
        logprobs of the actions taken in the sampled minibatch (according to the old policy)
    clip_coef:
        amount of clipping, denoted by epsilon in Eq 7.
    eps:
        used to add to std dev of mb_advantages when normalizing (to avoid dividing by zero)
    '''
    # CHANGED (different assert statement)
    assert (mb_action.shape[0],) == mb_advantages.shape == mb_logprobs.shape
    # CHANGED (sum over first dim)
    logits_diff = dist.log_prob(mb_action).sum(-1) - mb_logprobs

    r_theta = t.exp(logits_diff)

    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + eps)

    non_clipped = r_theta * mb_advantages
    clipped = t.clip(r_theta, 1-clip_coef, 1+clip_coef) * mb_advantages

    return t.minimum(non_clipped, clipped).mean()


def calc_entropy_bonus(dist: t.distributions.Normal, ent_coef: float):
    '''Return the entropy bonus term, suitable for gradient ascent.

    dist:
        the probability distribution for the current policy
    ent_coef:
        the coefficient for the entropy loss, which weights its contribution to the overall objective function. Denoted by c_2 in the paper.
    '''
    # CHANGED (sum over first dim before taking mean)
    return ent_coef * dist.entropy().sum(-1).mean()


class PPOTrainer:

    def __init__(self, args: PPOArgs):
        set_global_seeds(args.seed)
        self.args = args
        self.run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
        self.envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed + i, i, args.capture_video, self.run_name, args.mode) for i in range(args.num_envs)])
        self.agent = PPOAgent(self.args, self.envs).to(device)
        self.optimizer, self.scheduler = make_optimizer(self.agent, self.args.total_training_steps, self.args.learning_rate, 0.0)
        if args.use_wandb: wandb.init(
            project=args.wandb_project_name,
            entity=args.wandb_entity,
            name=self.run_name,
            monitor_gym=args.capture_video
        )


    def rollout_phase(self):
        '''Should populate the replay buffer with new experiences.'''
        last_episode_len = None
        for step in range(self.args.num_steps):
            infos = self.agent.play_step()
            for info in infos:
                if "episode" in info.keys():
                    last_episode_len = info["episode"]["l"]
                    last_episode_return = info["episode"]["r"]
                    if self.args.use_wandb: wandb.log({
                        "episode_length": last_episode_len,
                        "episode_return": last_episode_return,
                    }, step=self.agent.step)
        return last_episode_len


    def learning_phase(self) -> None:
        '''Should get minibatches and iterate through them (performing an optimizer step at each one).'''
        minibatches = self.agent.get_minibatches()
        for minibatch in minibatches:
            objective_fn = self.compute_ppo_objective(minibatch)
            objective_fn.backward()
            nn.utils.clip_grad_norm_(self.agent.parameters(), self.args.max_grad_norm)
            self.optimizer.step()
            self.optimizer.zero_grad()
            self.scheduler.step()


    def compute_ppo_objective(self, minibatch: ReplayMinibatch) -> Float[Tensor, ""]:
        '''Handles learning phase for a single minibatch. Returns objective function to be maximized.'''
        # CHANGED (returned tuple (mu, sigma, dist), and use dist for loss functions)
        mu, sigma, dist = self.agent.actor.forward(minibatch.observations)
        values = self.agent.critic.forward(minibatch.observations).squeeze()

        clipped_surrogate_objective = calc_clipped_surrogate_objective(dist, minibatch.actions, minibatch.advantages, minibatch.logprobs, self.args.clip_coef)
        value_loss = calc_value_function_loss(values, minibatch.returns, self.args.vf_coef)
        entropy_bonus = calc_entropy_bonus(dist, self.args.ent_coef)

        total_objective_function = clipped_surrogate_objective - value_loss + entropy_bonus

        with t.inference_mode():
            newlogprob = dist.log_prob(minibatch.actions).sum(-1)
            logratio = newlogprob - minibatch.logprobs
            ratio = logratio.exp()
            approx_kl = (ratio - 1 - logratio).mean().item()
            clipfracs = [((ratio - 1.0).abs() > self.args.clip_coef).float().mean().item()]
        if self.args.use_wandb: wandb.log(dict(
            total_steps = self.agent.step,
            values = values.mean().item(),
            learning_rate = self.scheduler.optimizer.param_groups[0]["lr"],
            value_loss = value_loss.item(),
            clipped_surrogate_objective = clipped_surrogate_objective.item(),
            entropy = entropy_bonus.item(),
            approx_kl = approx_kl,
            clipfrac = np.mean(clipfracs),
            # CHANGED (log mu and sigma)
            mu = mu.mean().item(),
            sigma = sigma.mean().item(),
        ), step=self.agent.step)

        return total_objective_function
```

</details>

## Training MuJoCo

Now, you should be ready to run your training loop! We recommend using the following parameters, to match the original implmentation which the [37 Implementational Details](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details) post is based on (but you can experiment with different values if you like).

[Here](https://api.wandb.ai/links/callum-mcdougall/zk2jnf8u) is the wandb run for these parameters.

```python
args = PPOArgs(
    env_id = "Hopper-v3",
    wandb_project_name = "PPOMuJoCo",
    use_wandb = True,
    mode = "mujoco",
    learning_rate = 3e-4,
    ent_coef = 0.0,
    num_minibatches = 32,
    num_steps = 2048,
    num_envs = 1,
)
agent = train(args)
```

Although we've used `Hopper-v3` in these examples, you might also want to try `InvertedPendulum-v2`. It's a much easier environment to solve, and it's a good way to check that your implementation is working (after all if it worked for CartPole then it should work here - in fact your inverted pendulum agent should converge to a perfect solution almost instantly, no reward shaping required). You can check out the other MuJoCo environments [here](https://www.gymlibrary.dev/environments/mujoco/).

See the `solutions_cts.py` file for a full implementation. All the comments have been removed from the code, except for comments which go next to parts of the code that have been changed from the original `solutions.py` (these comments will say `# CHANGED`, followed by a summary of the change).


""", unsafe_allow_html=True)

