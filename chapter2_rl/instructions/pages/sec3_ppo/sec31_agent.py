import streamlit as st


# graph TD
#     subgraph "Critic"
#         Linear1B["Linear(num_obs, 64)"] --> TanhB["Tanh"] --> Linear2B["Linear(64, 64)"] --> Tanh2B["Tanh"] --> Linear3B["Linear(64, 1)<br>std=1.0"] --> OutB["Out"]
#     end

#     subgraph "Actor"
#         Linear1["Linear(num_obs, 64)"] --> Tanh --> Linear2["Linear(64, 64)"] --> Tanh2["Tanh"] --> Linear3["Linear(64, num_actions)<br>std=0.01"] --> Out
#     end
    
    


def section():

    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#ppo-arguments'>PPO Arguments</a></li>
    <li class='margtop'><a class='contents-el' href='#actor-critic-implementation-detail-2'>Actor-Critic Implementation (detail #2)</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-get-actor-and-critic'><b>Exercise</b> - implement <code>get_actor_and_critic</code></a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#generalized-advantage-estimation-detail-5'>Generalized Advantage Estimation (detail #5)</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-compute-advantages'><b>Exercise</b> - implement <code>compute_advantages</code></a></li>
        <li><a class='contents-el' href='#a-note-on-truncation'>A note on truncation</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#replay-memory'>Replay Memory</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-minibatch-indices'>Exercise - implement <code>minibatch_indices</code></a></li>
        <li><a class='contents-el' href='#replaymemory-class'><code>ReplayMemory<code> class</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#ppoagent'>PPOAgent</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-ppoagent'><b>Exercise</b> - implement <code>PPOAgent</code></a></li>
    </ul></li>
</ul>""", unsafe_allow_html=True)

    st.markdown(
r"""
> ### Learning objectives
>
> * Understand the difference between the actor & critic networks, and what their roles are
> * Learn about & implement generalised advantage estimation (GAE)
> * Build a replay memory to store & sample experiences
> * Design an agent class to step through the environment & record experiences

In this section, we'll do the following:

* Define a dataclass to hold our PPO arguments
* Write functions to create our actor and critic networks (which will eventually be stored in our `PPOAgent` instance)
* Write a function to do **generalized advantage estimation** (this will be necessary when computing our objective function during the learning phase)
* Fill in our `ReplayMemory` class (for storing and sampling experiences)
* Fill in our `PPOAgent` class (a wrapper around our networks and our replay memory, which will turn them into an agent)

As a reminder, we'll be continually referring back to [The 37 Implementation Details of Proximal Policy Optimization](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#solving-pong-in-5-minutes-with-ppo--envpool) as we go through these exercises. Most of our sections wil refer to one or more of these details.

## PPO Arguments

Just like for DQN, we've provided you with a dataclass containing arguments for your `train_ppo` function. We've also given you a function from `utils` to display all these arguments (including which ones you've changed). Lots of these are the same as for the DQN dataclass.

Don't worry if these don't all make sense right now, they will by the end.

```python
@dataclass
class PPOArgs:
    # Basic / global
    seed: int = 1
    cuda: bool = t.cuda.is_available()
    env_id: str = "CartPole-v1"
    mode: Literal["classic-control", "atari", "mujoco"] = "classic-control"

    # Wandb / logging
    use_wandb: bool = False
    capture_video: bool = True
    exp_name: str = "PPO_Implementation"
    log_dir: str = "logs"
    wandb_project_name: str = "PPOCart"
    wandb_entity: str = None

    # Duration of different phases
    total_timesteps: int = 500000
    num_envs: int = 4
    num_steps: int = 128
    num_minibatches: int = 4
    batches_per_learning_phase: int = 4

    # Optimization hyperparameters
    learning_rate: float = 2.5e-4
    max_grad_norm: float = 0.5

    # Computing advantage function
    gamma: float = 0.99
    gae_lambda: float = 0.95

    # Computing other loss functions
    clip_coef: float = 0.2
    ent_coef: float = 0.01
    vf_coef: float = 0.25

    def __post_init__(self):
        self.batch_size = self.num_steps * self.num_envs
        assert self.batch_size % self.num_minibatches == 0, "batch_size must be divisible by num_minibatches"
        self.minibatch_size = self.batch_size // self.num_minibatches
        self.total_phases = self.total_timesteps // self.batch_size
        self.total_training_steps = self.total_phases * self.batches_per_learning_phase * self.num_minibatches


args = PPOArgs(num_minibatches=2)
utils.arg_help(args)
```

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/table-ppo.png" width="800">

## Actor-Critic Implementation ([detail #2](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Orthogonal%20Initialization%20of%20Weights%20and%20Constant%20Initialization%20of%20biases))

Implement the `Agent` class according to the diagram, inspecting `envs` to determine the observation shape and number of actions. We are doing separate Actor and Critic networks because [detail #13](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Shared%20and%20separate%20MLP%20networks%20for%20policy%20and%20value%20functions) notes that is performs better than a single shared network in simple environments.

Note that today `envs` will actually have multiple instances of the environment inside, unlike yesterday's DQN which had only one instance inside. From the **37 implementation details** post:

> In this architecture, PPO first initializes a vectorized environment `envs` that runs $N$ (usually independent) environments either sequentially or in parallel by leveraging multi-processes. `envs` presents a synchronous interface that always outputs a batch of $N$ observations from $N$ environments, and it takes a batch of $N$ actions to step the $N$ environments. When calling `next_obs = envs.reset()`, next_obs gets a batch of $N$ initial observations (pronounced "next observation"). PPO also initializes an environment `done` flag variable next_done (pronounced "next done") to an $N$-length array of zeros, where its i-th element `next_done[i]` has values of 0 or 1 which corresponds to the $i$-th sub-environment being *not done* and *done*, respectively.

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/screenshot-2.png" width="800">

### Exercise - implement `get_actor_and_critic`

```c
Difficulty: ðŸ”´ðŸ”´âšªâšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 10-20 minutes on this exercise.
```

Use `layer_init` to initialize each `Linear`, overriding the standard deviation argument `std` according to the diagram.

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/ppo_mermaid.svg" width="500">

Note, we've given you a "high level function" `get_actor_and_critic` which calls one of three possible functions, depending on the `mode` argument. You'll implement the other two modes later. This is one way to keep our code modular.

We've also defined the `num_obs` and `num_actions`, which are all you should need for creating the actor and critic networks.

```python
def layer_init(layer: nn.Linear, std=np.sqrt(2), bias_const=0.0):
    t.nn.init.orthogonal_(layer.weight, std)
    t.nn.init.constant_(layer.bias, bias_const)
    return layer


def get_actor_and_critic(
    envs: gym.vector.SyncVectorEnv,
    mode: Literal["classic-control", "atari", "mujoco"] = "classic-control",
) -> Tuple[nn.Module, nn.Module]:
    '''
    Returns (actor, critic), the networks used for PPO, in one of 3 different modes.
    '''
    assert mode in ["classic-control", "atari", "mujoco"]

    obs_shape = envs.single_observation_space.shape
    num_obs = np.array(obs_shape).prod()
    num_actions = (
        envs.single_action_space.n
        if isinstance(envs.single_action_space, gym.spaces.Discrete)
        else np.array(envs.single_action_space.shape).prod()
    )

    if mode == "classic-control":
        actor, critic = get_actor_and_critic_classic(num_obs, num_actions)
    if mode == "atari":
        actor, critic = get_actor_and_critic_atari(obs_shape, num_actions)
    if mode == "mujoco":
        actor, critic = get_actor_and_critic_mujoco(num_obs, num_actions)

    return actor.to(device), critic.to(device)

    
def get_actor_and_critic_classic(num_obs: int, num_actions: int):
    '''
    Returns (actor, critic) in the "classic-control" case, according to diagram above.
    '''
    pass


tests.test_get_actor_and_critic(get_actor_and_critic, mode="classic-control")
```

<details>
<summary>Solution</summary>

```python
def get_actor_and_critic_classic(num_obs: int, num_actions: int):
    '''
    Returns (actor, critic) in the "classic-control" case, according to diagram above.
    '''
    critic = nn.Sequential(
        layer_init(nn.Linear(num_obs, 64)),
        nn.Tanh(),
        layer_init(nn.Linear(64, 64)),
        nn.Tanh(),
        layer_init(nn.Linear(64, 1), std=1.0)
    )

    actor = nn.Sequential(
        layer_init(nn.Linear(num_obs, 64)),
        nn.Tanh(),
        layer_init(nn.Linear(64, 64)),
        nn.Tanh(),
        layer_init(nn.Linear(64, num_actions), std=0.01)
    )
    return actor, critic
```

</details>

<details>
<summary>Question - what do you think is the benefit of using a small standard deviation for the last actor layer?</summary>

The purpose is to center the initial `agent.actor` logits around zero, in other words an approximately uniform distribution over all actions independent of the state. If you didn't do this, then your agent might get locked into a nearly-deterministic policy early on and find it difficult to train away from it.

[Studies suggest](https://openreview.net/pdf?id=nIAxjsniDzg) this is one of the more important initialisation details, and performance is often harmed without it.

</details>

## Generalized Advantage Estimation ([detail #5](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Generalized%20Advantage%20Estimation))

The advantage function $A_\pi(s,a)$ indicates how much better choosing action $a$ would be in state $s$ as compared to the value obtained by letting $\pi$ choose the action (or if $\pi$ is stochastic, compared to the on expectation value by letting $\pi$ decide).
$$
A_\pi(s,a) = Q_\pi(s,a) - V_\pi(s)
$$

There are various ways to compute advantages - follow [detail #5](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Generalized%20Advantage%20Estimation) closely for today.

Given a batch of experiences, we want to compute each `advantage[t][env]`. This is equation $(11)$ of the [PPO paper](https://arxiv.org/pdf/1707.06347.pdf).

*Note - $(11)$ has a typo; the exponent be $T-(t+1)$ not $T-t+1$.*

### Exercise - implement `compute_advantages`

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 20-30 minutes on this exercise. Use the hints if you're stuck; it can be quite a messy function to implement.
```

Below, you should fill in `compute_advantages`. We recommend using a reversed for loop over $t$ to get it working, and not worrying about trying to completely vectorize it.

If you're confused about the formula for advantages (especially in how it pertains to environment terminations), then use the dropdowns below. The first image gives you a TL;DR summary of everything you need to complete this function. The second image gives a bit more of a thorough explanation in case you're curious, but you should find the first image sufficient.

A few tips for this function:

* If you've imported torch with `import torch as t`, be careful about using `t` as a variable during your iteration! Recommended alternatives are `T`, `t_`, `s`, or `timestep`.
* Be careful with the use of `values` vs `next_value` (and same with `dones`). You should be able to see from Image #1 why we need the next value and next done.
    * Tip - you might want to create an object `next_values` by concatenating `values[1:]` and `next_value`, and same for dones. You'll find this helpful in the calculation!

<br>

<details>
<summary>Image #1</summary>

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/advantages-tldr-1-light.png" width="700">

</details>

<details>
<summary>Image #2</summary>

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/advantages-tldr-2-light.png" width="700">

</details>

```python
@t.inference_mode()
def compute_advantages(
    next_value: t.Tensor,
    next_done: t.Tensor,
    rewards: t.Tensor,
    values: t.Tensor,
    dones: t.Tensor,
    gamma: float,
    gae_lambda: float,
) -> t.Tensor:
    '''Compute advantages using Generalized Advantage Estimation.
    next_value: shape (env,)
    next_done: shape (env,)
    rewards: shape (buffer_size, env)
    values: shape (buffer_size, env)
    dones: shape (buffer_size, env)
    Return: shape (buffer_size, env)
    '''
    pass


tests.test_compute_advantages(compute_advantages)
```

<details>
<summary>Solution</summary>

```python
@t.inference_mode()
def compute_advantages(
    next_value: t.Tensor,
    next_done: t.Tensor,
    rewards: t.Tensor,
    values: t.Tensor,
    dones: t.Tensor,
    gamma: float,
    gae_lambda: float,
) -> t.Tensor:
    '''Compute advantages using Generalized Advantage Estimation.
    next_value: shape (env,)
    next_done: shape (env,)
    rewards: shape (buffer_size, env)
    values: shape (buffer_size, env)
    dones: shape (buffer_size, env)
    Return: shape (buffer_size, env)
    '''
    # SOLUTION
    T = values.shape[0]
    next_values = t.concat([values[1:], next_value.unsqueeze(0)])
    next_dones = t.concat([dones[1:], next_done.unsqueeze(0)])
    deltas = rewards + gamma * next_values * (1.0 - next_dones) - values
    advantages = t.zeros_like(deltas)
    advantages[-1] = deltas[-1]
    for s in reversed(range(1, T)):
        advantages[s-1] = deltas[s-1] + gamma * gae_lambda * (1.0 - dones[s]) * advantages[s]
    return advantages
```

</details>

## Replay Memory

Our replay memory has some similarities to the replay buffer from yesterday, as well as some important differences.

### Sampling method

Yesterday, we continually updated our buffer and sliced off old data, and each time we called `sample` we'd take a randomly ordered subset of that data (with replacement).

With PPO, we alternate between rollout and learning phases. In rollout, we fill our replay memory entirely. In learning, we call `get_minibatches` to return the entire contents of the replay memory, but randomly shuffled and sorted into minibatches. In this way, we update on every experience, not just random samples. In fact, we'll update on each experience more than once, since we'll repeat the process of (generate minibatches, update on all of them) `batches_per_learning_phase` times during each learning phase.

### New variables

We store some of the same variables as before - $(s_t, a_t, d_t)$, but with the addition of 3 new variables: the **logprobs** $\pi(a_t\mid s_t)$, the **advantages** $A_t$ and the **returns**. Explaining these two variables and why we need them:

- `logprobs` are the logit outputs of our `actor.agent` network, corresponding to the actions $a_t$ which our agent actually chose.
    * These are necessary for calculating the clipped surrogate objective (see equation $(7)$ on page page 3 in the [PPO Algorithms paper](https://arxiv.org/pdf/1707.06347.pdf)), which as we'll see later makes sure the agent isn't rewarded for changing its policy an excessive amount.
- `advantages` are the terms $\hat{A}_t$, computed using our function `compute_advantages` from earlier.
    - Again, these are used in the calculation of the clipped surrogate objective.
- `returns` are given by the formula `returns = advantages + values` - see [detail #9](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Value%20Function%20Loss%20Clipping).
    - They are used to train the value network, in a way which is equivalent to minimizing the TD residual loss used in DQN.

Don't worry if you don't understand all of this now, we'll get to all these variables later.

### Exercise - implement `minibatch_indices`

```c
Difficulty: ðŸ”´ðŸ”´âšªâšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 10-15 minutes on this exercise.
```

We'll start by implementing the `minibatch_indices` function, as described in [detail #6](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Mini%2Dbatch%20Updates). This takes a batch size (total number of elements in memory, i.e. $N \times M$ in detail #6) and minibatch size, and returns a randomly permuted set of indices (which we'll use to index into the memory object). The diagram below should help explain this:

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/ppo-buffer-sampling-3.png" width="1200">

To make this clearer, we've given you the test code inline (so you can see exactly what your function is required to do).

```python
def minibatch_indexes(rng: Generator, batch_size: int, minibatch_size: int) -> List[np.ndarray]:
    '''
    Return a list of length num_minibatches = (batch_size // minibatch_size), where each element is an
    array of indexes into the batch. Each index should appear exactly once.

    To relate this to the diagram above: if we flatten the non-shuffled experiences into:

        [1,1,1,1,2,2,2,2,3,3,3,3]

    then the output of this function could be the following list of arrays:

        [array([0,5,4,3]), array([11,6,7,8]), array([1,2,9,10])]

    which would give us the minibatches seen in the first row of the diagram above:

        [array([1,2,2,1]), array([3,2,2,3]), array([1,1,3,3])]
    '''
    assert batch_size % minibatch_size == 0
    pass


rng = np.random.default_rng(0)
batch_size = 6
minibatch_size = 2
indexes = minibatch_indexes(rng, batch_size, minibatch_size)

assert np.array(indexes).shape == (batch_size // minibatch_size, minibatch_size)
assert sorted(np.unique(indexes)) == [0, 1, 2, 3, 4, 5]
print("All tests in `test_minibatch_indexes` passed!")
```

<details>
<summary>Solution</summary>

```python
def minibatch_indexes(rng: Generator, batch_size: int, minibatch_size: int) -> List[np.ndarray]:
    '''
    Return a list of length num_minibatches = (batch_size // minibatch_size), where each element is an
    array of indexes into the batch. Each index should appear exactly once.

    To relate this to the diagram above: if we flatten the non-shuffled experiences into:

        [1,1,1,1,2,2,2,2,3,3,3,3]

    then the output of this function could be the following list of arrays:

        [array([0,5,4,3]), array([11,6,7,8]), array([1,2,9,10])]

    which would give us the minibatches seen in the first row of the diagram above:

        [array([1,2,2,1]), array([3,2,2,3]), array([1,1,3,3])]
    '''
    assert batch_size % minibatch_size == 0
    # SOLUTION
    indices = rng.permutation(batch_size)
    indices = einops.rearrange(indices, "(mb_num mb_size) -> mb_num mb_size", mb_size=minibatch_size)
    return list(indices)
```

</details>

### `ReplayMemory` class

Next, we've given you the `ReplayMemory` class (since a lot of the exercise of filling it in is similar to what you'll have done for DQN yesterday). You should look through this class and make sure you understand everything. In particular, make sure you understand the `get_minibatches` method which does the following:

- Computes `advantages` & `returns` from the data stored in memory
- Uses these to assemble a list of all the data required for the `ReplayMinibatch` class (as tensors not numpy arrays)
- Returns a list of `batches_per_learning_phase * n_minibatches` minibatches, where:
    - `batches_per_learning_phase` is the number of times we loop through every experience in memory (i.e. the vertical height in the diagram above)
    - `n_minibatches = batch_size // minibatch_size` is the number of minibatches we form from each copy of the replay memory

<details>
<summary>Question - can you see why <code>advantages</code> can't be computed from the replay memory samples?</summary>

The samples are not in chronological order, they're shuffled. The formula for computing advantages required the data to be in chronological order.

</details>

Also take note of how the `add` function works. Unlike for DQN yesterday, we need to keep the stored memories and timesteps as different dimensions - the reason for this is the same as the one in the dropdown above.

We've also given you a helper function which converts tensors to numpy arrays, which is applied to each input to the `add` function. Although this might make some of your later exercises less frustrating, it's also a risk to add things like this because they can sometimes mask errors! These kind of design decisions are important to consider.

```python
def to_numpy(arr: Union[np.ndarray, Tensor]):
    '''
    Converts a (possibly cuda and non-detached) tensor to numpy array.
    '''
    if isinstance(arr, Tensor):
        arr = arr.detach().cpu().numpy()
    return arr


@dataclass
class ReplayMinibatch:
    '''
    Samples from the replay memory, converted to PyTorch for use in neural network training.

    Data is equivalent to (s_t, a_t, logpi(a_t|s_t), A_t, A_t + V(s_t), d_t)
    '''    
    observations: Tensor # shape [minibatch_size, *observation_shape]
    actions: Tensor # shape [minibatch_size,, *action_shape]
    logprobs: Tensor # shape [minibatch_size,]
    advantages: Tensor # shape [minibatch_size,]
    returns: Tensor # shape [minibatch_size,]
    dones: Tensor # shape [minibatch_size,]


class ReplayMemory:
    '''
    Contains buffer; has a method to sample from it to return a ReplayMinibatch object.
    '''
    rng: Generator
    observations: np.ndarray # shape [buffer_size, num_envs, *observation_shape]
    actions: np.ndarray # shape [buffer_size, num_envs, *action_shape]
    logprobs: np.ndarray # shape [buffer_size, num_envs]
    values: np.ndarray # shape [buffer_size, num_envs]
    rewards: np.ndarray # shape [buffer_size, num_envs]
    dones: np.ndarray # shape [buffer_size, num_envs]

    def __init__(self, args: PPOArgs, envs: gym.vector.SyncVectorEnv):
        self.args = args
        self.rng = np.random.default_rng(args.seed)
        self.num_envs = envs.num_envs
        self.obs_shape = envs.single_observation_space.shape
        self.action_shape = envs.single_action_space.shape
        self.reset_memory()


    def reset_memory(self):
        '''
        Resets all stored experiences, ready for new ones to be added to memory.
        '''
        self.observations = np.empty((0, self.num_envs, *self.obs_shape), dtype=np.float32)
        self.actions = np.empty((0, self.num_envs, *self.action_shape), dtype=np.int32)
        self.logprobs = np.empty((0, self.num_envs), dtype=np.float32)
        self.values = np.empty((0, self.num_envs), dtype=np.float32)
        self.rewards = np.empty((0, self.num_envs), dtype=np.float32)
        self.dones = np.empty((0, self.num_envs), dtype=bool)


    def add(self, obs, actions, logprobs, values, rewards, dones) -> None:
        '''
        Each argument can be a PyTorch tensor or NumPy array.

        obs: shape (num_environments, *observation_shape)
            Observation before the action
        actions: shape (num_environments,)
            Action chosen by the agent
        logprobs: shape (num_environments,)
            Log probability of the action that was taken (according to old policy)
        values: shape (num_environments,)
            Values, estimated by the critic (according to old policy)
        rewards: shape (num_environments,)
            Reward after the action
        dones: shape (num_environments,)
            If True, the episode ended and was reset automatically
        '''
        assert obs.shape == (self.num_envs, *self.obs_shape)
        assert actions.shape == (self.num_envs, *self.action_shape)
        assert logprobs.shape == (self.num_envs,)
        assert values.shape == (self.num_envs,)
        assert dones.shape == (self.num_envs,)
        assert rewards.shape == (self.num_envs,)

        self.observations = np.concatenate((self.observations, to_numpy(obs[None, :])))
        self.actions = np.concatenate((self.actions, to_numpy(actions[None, :])))
        self.logprobs = np.concatenate((self.logprobs, to_numpy(logprobs[None, :])))
        self.values = np.concatenate((self.values, to_numpy(values[None, :])))
        self.rewards = np.concatenate((self.rewards, to_numpy(rewards[None, :])))
        self.dones = np.concatenate((self.dones, to_numpy(dones[None, :])))


    def get_minibatches(self, next_value: t.Tensor, next_done: t.Tensor) -> List[ReplayMinibatch]:
        minibatches = []

        # Stack all experiences, and move them to our device
        obs, actions, logprobs, values, rewards, dones = [t.from_numpy(exp).to(device) for exp in [
            self.observations, self.actions, self.logprobs, self.values, self.rewards, self.dones
        ]]

        # Compute advantages and returns (then get the list of tensors, in the right order to add to our ReplayMinibatch)
        advantages = compute_advantages(next_value, next_done, rewards, values, dones.float(), self.args.gamma, self.args.gae_lambda)
        returns = advantages + values
        replay_memory_data = [obs, actions, logprobs, advantages, returns, dones]

        # Generate `batches_per_learning_phase` sets of minibatches (each set of minibatches is a shuffled permutation of
        # all the experiences stored in memory)
        for _ in range(self.args.batches_per_learning_phase):

            indices_for_each_minibatch = minibatch_indexes(self.rng, self.args.batch_size, self.args.minibatch_size)

            for indices_for_minibatch in indices_for_each_minibatch:
                minibatches.append(ReplayMinibatch(*[
                    arg.flatten(0, 1)[indices_for_minibatch] for arg in replay_memory_data
                ]))
    
        # Reset memory, since we only run this once per learning phase
        self.reset_memory()

        return minibatches
```

Like before, here's some code to generate and plot observations. The dotted lines indicate a terminated episode. We don't need to worry about terminal observations here, because we're not actually logging `next_obs` (unlike DQN, this won't be part of our loss function).

Note that we're actually using four environments inside our `envs` object, rather than just one like last time. The 3 solid (non-dotted) lines in the first plot below indicate the transition between different environments in `envs` (which we've stitched together into one long episode in the first plot below).

```python
args = PPOArgs()
envs = gym.vector.SyncVectorEnv([make_env("CartPole-v1", i, i, False, "test") for i in range(4)])
next_value = t.zeros(envs.num_envs).to(device)
next_done = t.zeros(envs.num_envs).to(device)
memory = ReplayMemory(args, envs)
obs = envs.reset()

for i in range(args.num_steps):
    actions = envs.action_space.sample()
    (next_obs, rewards, dones, infos) = envs.step(actions)
    # just dummy values for now, we won't be using them
    logprobs = values = t.zeros(envs.num_envs)
    # add everything to buffer (the casting from arrays to tensors is handled for us)
    memory.add(obs, actions, logprobs, values, rewards, dones)
    obs = next_obs

obs = memory.observations # shape [num_steps, num_envs, obs_shape=4]
dones = memory.dones # shape [num_steps, num_envs]

plot_cartpole_obs_and_dones(obs, dones, title="CartPole experiences (dotted lines = termination, solid lines = environment separators)")
```

The next code shows **a single minibatch**, sampled from this replay memory.

```python
minibatches = memory.get_minibatches(next_value, next_done)

obs = minibatches[0].observations.cpu() # shape [minibatch_size, obs_shape=4]
dones = minibatches[0].dones.cpu() # shape [minibatch_size,]

plot_cartpole_obs_and_dones(obs, dones, title="CartPole experiences for single minibatch (dotted lines = termination)")
```

## PPO Agent

As the final task in this section, you should fill in the agent's `play_step` method. This is conceptually similar to what you did during DQN, but with a few key differences.

In DQN, we did the following:

* used the Q-Network and an epsilon greedy policy to select an action based on current observation,
* stepped the environment with this action,
* stored the transition in the replay memory (using the `add` method of the replay buffer)

In PPO, you'll do the following:

* use the actor network to return a distribution over actions based on current observation & sample from this distribution to select an action,
* step the environment with this action,
* calculate `logprobs` and `values` (which we'll need during our learning step),
* store the transition in the replay memory (using the `add` method of the replay memory)

### Exercise - implement `PPOAgent`

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª
Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª

You should spend up to 20-35 minutes on this exercise.
```

A few gotchas:

* Make sure the things you're adding to memory are the right shape (otherwise the `add` method will throw an error). This includes paying attention to the batch dimension when you put things through the actor and critic networks.
    * The memory expects everything to have shape `(num_envs,)` except for observations which have shape `(num_envs, 4)`.
* Don't forget to use inference mode when running your actor and critic networks, since you're only generating data $\theta_\text{old}$ (i.e. you don't want to update the weights of the network based on these values).
* Don't forget to increment the step count `self.step` by the number of environments (you're stepping once for each env!) in each call to `play_step`.
* At the end of `play_step`, you should update `self.next_obs` and `self.next_done` (because this is where our agent will start next time `play_step` is called).
    * For more on why we need these values, see [this section](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=role%20to%20help%20transition%20between%20phases) of detail #1 in the "37 implementational details" post.

<details>
<summary>Tip - how to sample from distributions (to get <code>actions</code>)</summary>

You might remember using `torch.distributions.categorical.Categorical` when we were sampling from transformers in the previous chapter. We can use this again!

You can define a `Categorical` object by passing in `logits` (the output of the actor network), and then you can:

* Sample from it using the `sample` method,
* Calculate the logprobs of a given action using the `log_prob` method (with the actions you took as input argument to this method).
</details>

For this exercise and others to follow, there's a trade-off in the test functions between being strict and being lax. Too lax and the tests will let failures pass; too strict and they might fail for odd reasons even if your code is mostly correct. If you find youself continually failing tests (or passing tests despite not matching the solutions exactly, and running into confusing errors later!) then you should ask a TA for help.

Two final notes on this implementation:

* We've chosen `PPOAgent` to subclass `nn.Module`. This is so so that we can call `agent.parameters()` to get the parameters of the actor and critic networks, and feed these params into our optimizer.
* We've given you a `get_minibatches` method, which is useful because the earlier `get_minibatches` function required the next observation & value, and these are either stored or easily computable by our agent. You should be using this method rather than the original `get_minibatches` function.

Decisions like these aren't super important in the grand scheme of things, although they're important to keep track of, because they can often be the difference between a buggy and working RL implementation.

```python
class PPOAgent(nn.Module):
    critic: nn.Sequential
    actor: nn.Sequential

    def __init__(self, args: PPOArgs, envs: gym.vector.SyncVectorEnv):
        super().__init__()
        self.args = args
        self.envs = envs

        # Keep track of global number of steps taken by agent
        self.step = 0

        # Get actor and critic networks
        self.actor, self.critic = get_actor_and_critic(envs, mode=args.mode)

        # Define our first (obs, done), so we can start adding experiences to our replay memory
        self.next_obs = t.tensor(envs.reset()).to(device, dtype=t.float)
        self.next_done = t.zeros(envs.num_envs).to(device, dtype=t.float)

        # Create our replay memory
        self.memory = ReplayMemory(args, envs)


    def play_step(self) -> List[dict]:
        '''
        Carries out a single interaction step between the agent and the environment, and adds results to the replay memory.

        Returns the list of info dicts returned from `self.envs.step`.
        '''
        # Get newest observations (this is where we start from)
        obs = self.next_obs
        dones = self.next_done

        pass


    def get_minibatches(self) -> None:
        '''
        Gets minibatches from the replay memory.
        '''
        with t.inference_mode():
            next_value = self.critic(self.next_obs).flatten()
        return self.memory.get_minibatches(next_value, self.next_done)


tests.test_ppo_agent(PPOAgent)
```

<details>
<summary>Solution</summary>

```python
class PPOAgent(nn.Module):
    critic: nn.Sequential
    actor: nn.Sequential

    def __init__(self, args: PPOArgs, envs: gym.vector.SyncVectorEnv):
        super().__init__()
        self.args = args
        self.envs = envs

        # Keep track of global number of steps taken by agent
        self.step = 0

        # Get actor and critic networks
        self.actor, self.critic = get_actor_and_critic(envs, mode=args.mode)

        # Define our first (obs, done), so we can start adding experiences to our replay memory
        self.next_obs = t.tensor(envs.reset()).to(device, dtype=t.float)
        self.next_done = t.zeros(envs.num_envs).to(device, dtype=t.float)

        # Create our replay memory
        self.memory = ReplayMemory(args, envs)


    def play_step(self) -> List[dict]:
        '''
        Carries out a single interaction step between the agent and the environment, and adds results to the replay memory.

        Returns the list of info dicts returned from `self.envs.step`.
        '''
        # Get newest observations (this is where we start from)
        obs = self.next_obs
        dones = self.next_done

        # SOLUTION
        # Compute logits based on newest observation, and use it to get an action distribution we sample from
        with t.inference_mode():
            logits = self.actor(obs)
        probs = Categorical(logits=logits)
        actions = probs.sample()

        # Step environment based on the sampled action
        next_obs, rewards, next_dones, infos = self.envs.step(actions.cpu().numpy())

        # Calculate logprobs and values, and add this all to replay memory
        logprobs = probs.log_prob(actions)
        with t.inference_mode():
            values = self.critic(obs).flatten()
        self.memory.add(obs, actions, logprobs, values, rewards, dones)

        # Set next observation, and increment global step counter
        self.next_obs = t.from_numpy(next_obs).to(device, dtype=t.float)
        self.next_done = t.from_numpy(next_dones).to(device, dtype=t.float)
        self.step += self.envs.num_envs

        # Return infos dict, for logging
        return infos


    def get_minibatches(self) -> None:
        '''
        Gets minibatches from the replay memory.
        '''
        with t.inference_mode():
            next_value = self.critic(self.next_obs).flatten()
        return self.memory.get_minibatches(next_value, self.next_done)
```

</details>

""", unsafe_allow_html=True)

