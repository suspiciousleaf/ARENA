import streamlit as st

def section():

    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#setup'>Setup</a></li>
    <li class='margtop'><a class='contents-el' href='#introduction'>Introduction</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#playing-breakout'>Playing breakout</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#implementational-details-of-atari'>Implementational details of Atari</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#wrappers-details-1-7-and-9'>Wrappers (details #1-7, and #9)</a></li>
        <li><a class='contents-el' href='#shared-cnn-for-actor-critic-detail-8'>Shared CNN for actor & critic (detail #8)</a></li>
        <li><a class='contents-el' href='#exercise-rewrite-get-actor-and-critic'><b>Exercise</b> - rewrite <code>get_actor_and_critic</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#training-atari'>Training Atari</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#a-note-on-debugging-crashed-kernels'>A note on debugging crashed kernels</a></li>
    </ul></li>
</ul>""", unsafe_allow_html=True)

    st.markdown(
r"""
# Atari

> ### Learning objectives
>
> * Understand how PPO can be used in visual domains, with appropriate architectures (CNNs)
> * Understand the idea of policy and value heads
> * Train an agent to solve the Breakout environment

## Setup

First, run the following code. If you're working in colab, you may have to restart the kernel and run all cells again (except for the ones containing `pip install`s).

```python
%pip install autorom[accept-rom-license]
%pip install ale-py
```

## Introduction

In this section, you'll extend your PPO implementation to play Atari games.

The `gym` library supports a variety of different Atari games - you can find them [here](https://www.gymlibrary.dev/environments/atari/) (if you get a message when you click on this link asking whether you want to switch to gymnasium, ignore this and proceed to the gym site). You can try whichever ones you want, but we recommend you stick with the easier environments like Pong, Breakout, and Space Invaders.

The environments in this game are very different. Rather than having observations of shape `(4,)` (representing a vector of `(x, v, theta, omega)`), the raw observations are now images of shape `(210, 160, 3)`, representing pixels in the game screen. This leads to a variety of additional challenges relative to the Cartpole environment, for example:

* We need a much larger network, because finding the optimal strategy isn't as simple as solving a basic differential equation
* Reward shaping is much more difficult, because our observations are low-level and don't contain easily-accessible information about the high-level abstractions in the game (finding these abstractions in the first place is part of the model's challenge!)

The action space is also different for each environment. For example, in Breakout, the environment has 4 actions - run the code below to see this (if you get an error, try restarting the kernel and running everything again, minus the library installs).

```python
env = gym.make("ALE/Breakout-v5")
print(env.action_space)
```

These 4 actions are "do nothing", "fire the ball", "move right", and "move left" respectively. You can see more details on the game-specific [documentation page](https://www.gymlibrary.dev/environments/atari/breakout/).

On this documentation page, you can also see information like the reward for this environment. In this case, the reward comes from breaking bricks in the wall (more reward from breaking bricks higher up). This is a more challenging reward function than the one for CartPole, where a very simple strategy (move in the direction you're tipping) leads directly to a higher reward by marginally prolonging episode length.

### Playing Breakout

Just like for Cartpole and MountainCar, we're given you a Python file to play Atari games yourself. The file is called `play_breakout.py`, and running it (i.e. `python play_breakout.py`) will open up a window for you to play the game. Take note of the key instructions, which will be printed in your terminal.

You should also be able to try out other games, by changing the relevant parts of the `play_breakout.py` file to match those games' [documentation pages](https://www.gymlibrary.dev/environments/atari/complete_list/).

## Implementational details of Atari

The [37 Implementational Details of PPO](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Atari%2Dspecific%20implementation%20details) post describes how to get PPO working for games like Atari. In the sections below, we'll go through these steps.

### Wrappers (details [#1-7](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=The%20Use%20of%20NoopResetEnv), and [#9](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Scaling%20the%20Images%20to%20Range%20%5B0%2C%201%5D))

All the extra details except for one are just wrappers on the environment, which implement specific behaviours. For example:

* **Frame Skipping** - we repeat the agent's action for a number of frames (by default 4), and sum the reward over these frames. This saves time when the model's forward pass is computationally cheaper than an environment step.
* **Image Transformations** - we resize the image from `(210, 160)` to `(L, L)` for some smaller value `L` (in this case we'll use 84), and convert it to grayscale.

These wrappers have been implemented for you - to see details, look at the file `part3_ppo/atari_wrappers.py` file (and the `make_env` function in `part3_ppo/utils.py`). We won't spend too much time on these, because they aren't very conceptually interesting.

### Shared CNN for actor & critic ([detail #8](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Shared%20Nature%2DCNN%20network))

This is the most interesting one conceptually. If we have a new observation space then it naturally follows that we need a new architecture, and if we're working with images then using a convolutional neural network is reasonable. But another particularly interesting feature here is that we use a **shared architecture** for the actor and critic networks. The idea behind this is that the early layers of our model extract features from the environment (i.e. they find the high-level abstractions contained in the image), and then the actor and critic components perform **feature extraction** to turn these features into actions / value estimates respectively. This is commonly referred to as having a **policy head** and a **value head**. We'll see this idea come up later, when we perform RL on transformers.

### Exercise - rewrite `get_actor_and_critic`

```c
Difficulty: ðŸ”´ðŸ”´âšªâšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 10-15 minutes on this exercise.
```

The function `get_actor_and_critic` had a boolean argument `atari`, which we ignored previously, but which we'll now return to. When this argument is `False` then the function should behave exactly as it did before (i.e. the Cartpole version), but when `True` then it should return a shared CNN architecture for the actor and critic. The architecture should be as follows:

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/ppo_mermaid_2.svg" width="350">

Note - when calculating the number of input features for the linear layer, you can assume that the value `L` is 4 modulo 8, i.e. we can write `L = 8m + 4` for some integer `m`. This will make the convolutions easier to track. You shouldn't hardcode the number of input features assuming an input shape of `(4, 84, 84)`; this is bad practice!

We leave the exercise of finding the number of input features to the linear layer as a challenge for you. If you're stuck, you can find a hint in the section below (this isn't a particularly conceptually important detail).

<details>
<summary>Help - I don't know what the number of inputs for the linear layer should be.</summary>

The linear layer is fed 64 input features. By symmetry of convolutions and of original input, the shape of the linear layer's input (flattened) must have input features `64 * L_new * L_new`. Our only challenge is to find `L_new` in terms of `L`.

There's never any padding, so for a conv with parameters `(size, stride)`, the dimensions change as `L -> 1 + (L - size) // stride` (see the [documentation page](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)). So we have:

$$
\begin{aligned}
8m + 4  \quad &\rightarrow  \quad 1 + \frac{(8m + 4) - 8}{4} \quad = \quad 2m \\
 \\
2m      \quad &\rightarrow  \quad 1 + \frac{2m - 4}{2}       \quad = \quad m - 1 \\
 \\
m - 1   \quad &\rightarrow  \quad 1 + \frac{(m - 1) - 3}{1}  \quad = \quad m - 3
\end{aligned}
$$

For instance, if `L = 84` then `m = 10` and `L_new = m-3 = 7`. So the linear layer is fed 64 features of shape `(64, 7, 7)`

</details>

Now, you can fill in the `get_actor_and_critic_atari` function below, which is called when we call `get_actor_and_critic` with `mode == "atari"`.

Note that we take the observation shape as argument, not the number of observations. It should be `(4, L, L)` as indicated by the diagram. The shape `(4, L, L)` is a reflection of the fact that we're using 4 frames of history per input (which helps the model calculate things like velocity), and each of these frames is a monochrome resized square image.

```python
def get_actor_and_critic_atari(obs_shape: Tuple[int], num_actions: int):
    '''
    Returns (actor, critic) in the "atari" case, according to diagram above.
    '''
    pass


tests.test_get_actor_and_critic(get_actor_and_critic, mode="atari")
```

<details>
<summary>Solution</summary>

```python
def get_actor_and_critic_atari(obs_shape: Tuple[int], num_actions: int):
    '''
    Returns (actor, critic) in the "atari" case, according to diagram above.
    '''
    assert obs_shape[-1] % 8 == 4
    # Your new code should go here

    L_after_convolutions = (obs_shape[-1] // 8) - 3
    in_features = 64 * L_after_convolutions * L_after_convolutions

    hidden = nn.Sequential(
        layer_init(nn.Conv2d(4, 32, 8, stride=4, padding=0)),
        nn.ReLU(),
        layer_init(nn.Conv2d(32, 64, 4, stride=2, padding=0)),
        nn.ReLU(),
        layer_init(nn.Conv2d(64, 64, 3, stride=1, padding=0)),
        nn.ReLU(),
        nn.Flatten(),
        layer_init(nn.Linear(in_features, 512)),
        nn.ReLU(),
    )

    actor = nn.Sequential(
        hidden,
        layer_init(nn.Linear(512, num_actions), std=0.01)
    )

    critic = nn.Sequential(
        hidden,
        layer_init(nn.Linear(512, 1), std=1)
    )

    return actor, critic
```

</details>

## Training Atari

Now, you should be able to run an Atari training loop!

We recommend you use the following parameters, for fidelity:

```python
args = PPOArgs(
    env_id = "ALE/Breakout-v5",
    wandb_project_name = "PPOAtari",
    use_wandb = True,
    mode = "atari",
    clip_coef = 0.1,
    num_envs = 8,
)
agent = train(args)
```

Note that this will probably take a lot longer to train than your previous experiments, because the architecture is much larger, and finding an initial strategy is much harder than it was for CartPole.

[Here](https://api.wandb.ai/links/callum-mcdougall/n3g6ncd9) is the wandb run for the breakout environment, using the parameters above (all the code is in the solutions colab `solutions.py`). With this (and MuJoCo later), you might want to perform a few different runs (with different `args.seed` values).

### A note on debugging crashed kernels

Because the `gym` library is a bit fragile, sometimes you can get uninformative kernel errors like this:

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/kernel_error.png" width="600">

which annoyingly doesn't tell you much about the nature or location of the error. When this happens, it's often good practice to replace your code with lower-level code bit by bit, until the error message starts being informative.

For instance, you might start with `agent = train(args)`, and if this fails without an informative error message then you might try replacing it with the actual contents of the `train` function (which should involve the methods `trainer.rollout_phase()` and `trainer.learning_phase()`). If the problem is in `rollout_phase`, you can again replace this line with the actual contents of this method.

If you're working in `.py` files rather than `.ipynb`, a useful tip - as well as running `Shift + Enter` to run the cell your cursor is in, if you have text highlighted (and you've turned on `Send Selection To Interactive Window` in VSCode settings) then using `Shift + Enter` will run just the code you've highlighted. This could be a single variable name, a single line, or a single block of code.

""", unsafe_allow_html=True)

