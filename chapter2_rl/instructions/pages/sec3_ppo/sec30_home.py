import streamlit as st

def section():

    st.sidebar.markdown(
r"""
## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#introduction'>Introduction</a></li>
    <li class='margtop'><a class='contents-el' href='#content-learning-objectives'>Content & Learning Objectives</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#setting-up-our-agent'>Setting up our agent</a></li>
        <li><a class='contents-el' href='#learning-phase'>Learning phase</a></li>
        <li><a class='contents-el' href='#training-loop'>Training Loop</a></li>
        <li><a class='contents-el' href='#atari'>Atari</a></li>
        <li><a class='contents-el' href='#mujoco'>MuJoCo</a></li>
        <li><a class='contents-el' href='#bonus'>Bonus</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#ppo-vs-dqn'>PPO vs DQN</a></li>
    <li class='margtop'><a class='contents-el' href='#conceptual-overview-of-ppo'>Conceptual overview of PPO</a></li>
    <li class='margtop'><a class='contents-el' href='#implementational-overview-of-ppo'>Implementational overview of PPO</a></li>
    <li class='margtop'><a class='contents-el' href='#notes-on-todays-workflow'>Notes on today's workflow</a></li>
    <li class='margtop'><a class='contents-el' href='#readings'>Readings</a></li>
    <li class='margtop'><a class='contents-el' href='#setup'>Setup</a></li>
</ul>""", unsafe_allow_html=True)


    st.markdown(
r"""
# [2.3] PPO

### Colab: [**exercises**](https://colab.research.google.com/drive/1UgXZRsIDsGmv6FhqkEuBCRMfkRsBC6nb?usp=sharing) | [**solutions**](https://colab.research.google.com/drive/1aMzOHbw-CAy6g81Vue7SXs5Hqy8JcxJd?usp=sharing)

Please send any problems / bugs on the `#errata` channel in the [Slack group](https://join.slack.com/t/arena-uk/shared_invite/zt-28h0xs49u-ZN9ZDbGXl~oCorjbBsSQag), and ask any questions on the dedicated channels for this chapter of material.

You can toggle dark mode from the buttons on the top-right of this page.

Links to other chapters: [**(0) Fundamentals**](https://arena3-chapter0-fundamentals.streamlit.app/), [**(1) Transformer Interp**](https://arena-ch1-transformer-interp.streamlit.app/).

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/football.jpeg" width="350">

## Introduction

Proximal Policy Optimization (PPO) is a cutting-edge reinforcement learning algorithm that has gained significant attention in recent years. As an improvement over traditional policy optimization methods, PPO addresses key challenges such as sample efficiency, stability, and robustness in training deep neural networks for reinforcement learning tasks. With its ability to strike a balance between exploration and exploitation, PPO has demonstrated remarkable performance across a wide range of complex environments, including robotics, game playing, and autonomous control systems.

In this section, you'll build your own agent to perform PPO on the CartPole environment. By the end, you should be able to train your agent to near perfect performance in about 30 seconds. You'll also be able to try out other things like **reward shaping**, to make it easier for your agent to learn to balance, or to do fun tricks! There are also additional exercises which allow you to experiment with other tasks, including **Atari** and the 3D physics engine **MuJoCo**.

A lot of the setup as we go through these exercises will be similar to what we did yesterday for DQN, so you might find yourself moving quickly through certain sections.


## Content & Learning Objectives

#### 1️⃣ Setting up our agent

> ##### Learning objectives
>
> * Understand the difference between the actor & critic networks, and what their roles are
> * Learn about & implement generalised advantage estimation
> * Build a replay memory to store & sample experiences
> * Design an agent class to step through the environment & record experiences

#### 2️⃣ Learning Phase

> ##### Learning objectives
>
> * Implement the total objective function (sum of three separate terms)
> * Understand the importance of each of these terms for the overall algorithm
> * Write a function to return an optimizer and learning rate scheduler for your model

#### 3️⃣ Training Loop

> ##### Learning objectives
>
> * Build a full training loop for the PPO algorithm
> * Train our agent, and visualise its performance with Weights & Biases media logger
> * Use reward shaping to improve your agent's training (and make it do tricks!)

#### 4️⃣ Atari

> ##### Learning objectives
>
> * Understand how PPO can be used in visual domains, with appropriate architectures (CNNs)
> * Understand the idea of policy and value heads
> * Train an agent to solve the Breakout environment

#### 5️⃣ MuJoCo

> ##### Learning objectives
>
> * Understand how PPO can be used to train agents in continuous action spaces
> * Install and interact with the MuJoCo physics engine
> * Train an agent to solve the Hopper environment

#### 6️⃣ Bonus

We conclude with a set of optional bonus exercises, which you can try out before moving on to the RLHF sections.

## Introduction - PPO vs DQN

Today, we'll be working on PPO (Proximal Policy Optimization). It has some similarities to DQN, but is based on a fundamentally different approach:

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/ppo-vs-dqn-2.png" width="1000">

A quick note again on the distinction between **states** and **observations**. In reality these are two different things (denoted $s_t$ and $o_t$), because our agent might not be able to see all relevant information. However, the games we'll be working with for the rest of this section make no distinction between them. Thus, we will only refer to $s_t$ going forwards.

## Conceptual overview of PPO

Below is an algorithm showing the conceptual overview of PPO. It's split into 2 main phases: **learning** and **rollout**.

In **rollout**, we sample experiences using the current values of our actor and critic networks, and store them in memory. This is all done in inference mode. In **learning**, we use our current actor and critic networks (*not* in inference mode) plus these logged experiences to calculate an objective function and use it to update our network.

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/ppo-alg-conceptual-light.png" width="800">

## Implementational overview of PPO

There are 3 main classes you'll be using today:

* `ReplayMemory`
    * Stores experiences generated by agent
    * Has a `get_minibatches` method, which samples data from memory to actually be used in training
* `Agent`
    * Contains the actor and critic networks, the `play_step` function, and a replay memory
        * In other words, it contains both the thing doing the inference and the thing which interacts with environment & stores results
        * This is a design choice, other designs might keep these separate
    * Also has a `get_minibatches` method which calls the corresponding `ReplayMemory` method (and uses the agent's current state)
* `PPOTrainer`
    * This is the main class for training our model, it helps us keep methods like `rollout_phase` and `learning_phase` separate

## Notes on today's workflow

Your implementation might get huge benchmark scores by the end of the day, but don't worry if it struggles to learn the simplest of tasks. RL can be frustrating because the feedback you get is extremely noisy: the agent can fail even with correct code, and succeed with buggy code. Forming a systematic process for coping with the confusion and uncertainty is the point of today, more so than producing a working PPO implementation.

Some parts of your process could include:

- Forming hypotheses about why it isn't working, and thinking about what tests you could write, or where you could set a breakpoint to confirm the hypothesis.
- Implementing some of the even more basic Gym environments and testing your agent on those.
- Getting a sense for the meaning of various logged metrics, and what this implies about the training process
- Noticing confusion and sections that don't make sense, and investigating this instead of hand-waving over it.

## Readings

* [An introduction to Policy Gradient methods - Deep RL
](https://www.youtube.com/watch?v=5P7I-xPq8u8) (20 mins)
    * This is a useful video which motivates the core setup of PPO (and in particular the clipped objective function) without spending too much time with the precise derivations. We recommend watching this video before doing the exercises.
    * Note - you can ignore the short section on multi-GPU setup.
    * Also, near the end the video says that PPO outputs parameters $\mu$ and $\sigma$ from which actions are sampled, this is true for non-discrete action spaces (which we'll be using later on) but we'll start by implementing PPO on CartPole meaning our observation and action space is discrete just like yesterday.
* [The 37 Implementation Details of Proximal Policy Optimization](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#solving-pong-in-5-minutes-with-ppo--envpool)
    * **This is not required reading before the exercises**, but it will be a useful reference point as you go through the exercises.
    * The good news is that you won't need all 37 of these today, so no need to read to the end.
    * We will be tackling the 13 "core" details, not in the same order as presented here. Some of the sections below are labelled with the number they correspond to in this page (e.g. **Minibatch Update ([detail #6](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Mini%2Dbatch%20Updates))**).
* [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347.pdf)
    * **This is not required reading before the exercises**, but it will be a useful reference point for many of the key equations as you go through the exercises. In particular, you will find up to page 5 useful.

### Optional Reading

* [Spinning Up in Deep RL - Vanilla Policy Gradient](https://spinningup.openai.com/en/latest/algorithms/vpg.html#background)
    * PPO is a fancier version of vanilla policy gradient, so if you're struggling to understand PPO it may help to look at the simpler setting first.
* [Spinning Up in Deep RL - PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html)
    * You don't need to follow all the derivations here, although as a general idea by the end you should at least have a qualitative understanding of what all the symbols represent.
* [Andy Jones - Debugging RL, Without the Agonizing Pain](https://andyljones.com/posts/rl-debugging.html)
    * You've already read this previously but it will come in handy again.
    * You'll want to reuse your probe environments from yesterday, or you can import them from the solution if you didn't implement them all.
* [Tricks from Deep RL Bootcamp at UC Berkeley](https://github.com/williamFalcon/DeepRLHacks/blob/master/README.md) - more debugging tips that may be of use.
* [Lilian Weng Blog on PPO](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#ppo) - her writing on ML topics is consistently informative and informationally dense.

## Setup

```python
import os
import time
import sys
from dataclasses import dataclass
from tqdm import tqdm
import numpy as np
from numpy.random import Generator
import torch as t
from torch import Tensor
from torch.optim.optimizer import Optimizer
import gym
import gym.envs.registration
from gym.envs.classic_control.cartpole import CartPoleEnv
import torch.nn as nn
import torch.optim as optim
from torch.distributions.categorical import Categorical
import einops
from pathlib import Path
from typing import List, Tuple, Literal, Union, Optional
from jaxtyping import Float, Int
import wandb
from IPython.display import clear_output
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import warnings
warnings.filterwarnings('ignore')

# Make sure exercises are in the path
chapter = "chapter2_rl"
exercises_dir = Path(f"{os.getcwd().split(chapter)[0]}/{chapter}/exercises").resolve()
section_dir = exercises_dir / "part3_ppo"
if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))

from part2_q_learning_and_dqn.utils import set_global_seeds
from part2_q_learning_and_dqn.solutions import Probe1, Probe2, Probe3, Probe4, Probe5
from part3_ppo.utils import make_env
import part3_ppo.utils as utils
import part3_ppo.tests as tests
from plotly_utils import plot_cartpole_obs_and_dones

# Register our probes from last time
for idx, probe in enumerate([Probe1, Probe2, Probe3, Probe4, Probe5]):
    gym.envs.registration.register(id=f"Probe{idx+1}-v0", entry_point=probe)

Arr = np.ndarray

device = t.device("cuda" if t.cuda.is_available() else "cpu")

MAIN = __name__ == "__main__"
```

""", unsafe_allow_html=True)

