import streamlit as st

def section():

    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#readings'>Readings</a></li>
    <li class='margtop'><a class='contents-el' href='#introduction'>Introduction</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#conceptual-overview-of-dqn'>Conceptual overview of DQN</a></li>
        <li><a class='contents-el' href='#fast-feedback-loops'>Fast feedback loops</a></li>
        <li><a class='contents-el' href='#cartpole'>CartPole</a></li>
        <li><a class='contents-el' href='#outline-of-the-exercises'>Outline of the exercises</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#the-q-network'>The Q-Network</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-qnetwork'><b>Exercise</b> - implement <code>QNetwork</code></a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#replay-buffer'>Replay Buffer</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-replaybuffer'><b>Exercise</b> - implement <code>ReplayBuffer</code></a></li>
        <li><a class='contents-el' href='#environment-resets'>Environment resets</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#exploration'>Exploration</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-linear-scheduler'><b>Exercise</b> - implement linear scheduler</a></li>
        <li><a class='contents-el' href='#epsilon-greedy-policy'>Epsilon Greedy Policy</a></li>
        <li><a class='contents-el' href='#exercise-implement-epsilon-greedy-policy'><b>Exercise</b> - implement the epsilon greedy policy</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#main-dqn-algorithm'>Main DQN Algorithm</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#weights-biases'>Weights & Biases</a></li>
        <li><a class='contents-el' href='#dqn-dataclass'>DQN Dataclass</a></li>
        <li><a class='contents-el' href='#exercise-fill-in-the-agent-class'><b>Exercise</b> - fill in the Agent class</a></li>
        <li><a class='contents-el' href='#exercise-write-dqn-training-loop'><b>Exercise</b> - write DQN training loop</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#beyond-cartpole'>Beyond CartPole</a></li>
    <li class='margtop'><a class='contents-el' href='#bonus'>Bonus</a></li>
</ul>""", unsafe_allow_html=True)


    st.markdown(
r"""
# Deep Q-Learning

> ### Learning objectives
>
> - Understand the DQN algorithm
> - Learn more about RL debugging, and build probe environments to debug your agents
> - Create a replay buffer to store environment transitions
> - Implement DQN using PyTorch Lightning, on the CartPole environment

In this section, you'll implement Deep Q-Learning, often referred to as DQN for "Deep Q-Network". This was used in a landmark paper [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf).

At the time, the paper was very exicitng: The agent would play the game by only looking at the same screen pixel data that a human player would be looking at, rather than a description of where the enemies in the game world are. The idea that convolutional neural networks could look at Atari game pixels and "see" gameplay-relevant features like a Space Invader was new and noteworthy. In 2022, we take for granted that convnets work, so we're going to focus on the RL aspect solely, and not the vision component.

## Readings

* [Deep Q Networks Explained](https://www.lesswrong.com/posts/kyvCNgx9oAwJCuevo/deep-q-networks-explained) (25 minutes)
    * A high-level distillation as to how DQN works.
    * Read sections 1-4 (further sections optional).
* [Andy Jones - Debugging RL, Without the Agonizing Pain](https://andyljones.com/posts/rl-debugging.html) (10 minutes)
    * Useful tips for debugging your code when it's not working.
    * Read up to (not including) the Common Fixes section. Also read the Practical Advice section, up to and including "Use probe agents". The rest of the post is optional, and you're recommended to come back to it near the end if you're stuck.
    * The "probe environments" (a collection of simple environments of increasing complexity) section will be our first line of defense against bugs, you'll implement these in exercises below.

### Interesting Resources (not required reading)

- [An Outsider's Tour of Reinforcement Learning](http://www.argmin.net/2018/06/25/outsider-rl/) - comparison of RL techniques with the engineering discipline of control theory.
- [Towards Characterizing Divergence in Deep Q-Learning](https://arxiv.org/pdf/1903.08894.pdf) - analysis of what causes learning to diverge
- [Divergence in Deep Q-Learning: Tips and Tricks](https://amanhussain.com/post/divergence-deep-q-learning/) - includes some plots of average returns for comparison
- [Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures) - 2017 bootcamp with video and slides. Good if you like videos.
- [DQN debugging using OpenAI gym Cartpole](https://adgefficiency.com/dqn-debugging/) - random dude's adventures in trying to get it to work.
- [CleanRL DQN](https://github.com/vwxyzjn/cleanrl) - single file implementations of RL algorithms. Your starter code today is based on this; try not to spoiler yourself by looking at the solutions too early!
- [Deep Reinforcement Learning Doesn't Work Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html) - 2018 article describing difficulties preventing industrial adoption of RL.
- [Deep Reinforcement Learning Works - Now What?](https://tesslerc.github.io/posts/drl_works_now_what/) - 2020 response to the previous article highlighting recent progress.
- [Seed RL](https://github.com/google-research/seed_rl) - example of distributed RL using Docker and GCP.

## Introduction

### Conceptual overview of DQN

DQN is the natural extension of Q-Learning into the domain of deep learning. The main difference is that, instead of a table to store all the Q-values for each state-action pair, we train a neural network to learn this function for us. The usual implementation (which we'll use here) is for the Q-network to take the state as input, and output a vector of Q-values for each action, i.e. we're learning the function:

$$
s \to (Q(s, a_1), ..., Q(s, a_n))
$$

Below is an algorithm showing the conceptual overview of DQN. We cycle through the following process:

* Generate a batch of experiences using our current policy, by **epsilon-greedy sampling** (i.e. we mostly take the action with the highest Q-value, but occasionally take a random action to encourage exploration). Store these experiences in the **replay buffer**.
* Use these values to calculate a **TD (temporal difference) error**, and update our network.
    * To increase stability, we also have a **target network** we use for the "next step" part of the TD error. This is a lagged copy of the Q-network (i.e. we update our Q-network via gradient descent, and then every so often we copy the Q-network weights over to our target network).
* Repeat this until convergence.

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/dqn-alg-conceptual-light.png" width="600">

### Fast feedback loops

We want to have faster feedback loops, and learning from Atari pixels doesn't achieve that. It might take 15 minutes per training run to get an agent to do well on Breakout, and that's if your implementation is relatively optimized. Even waiting 5 minutes to learn Pong from pixels is going to limit your ability to iterate, compared to using environments that are as simple as possible.


### CartPole

The classic environment "CartPole-v1" is simple to understand, yet hard enough for a RL agent to be interesting, by the end of the day your agent will be able to do this and more! (Click to watch!)


[![CartPole](https://img.youtube.com/vi/46wjA6dqxOM/0.jpg)](https://www.youtube.com/watch?v=46wjA6dqxOM "CartPole")

If you like, run `python play_cartpole.py` (locally, not on the remote machine) to try having a go at the task yourself! Use Left/Right to move the cart, R to reset, and Q to quit. By default, the cart will alternate Left/Right actions (there's no no-op action) if you're not pressing a button.

The description of the task is [here](https://www.gymlibrary.dev/environments/classic_control/cart_pole/). Note that unlike the previous environments, the observation here is now continuous. You can see the source for CartPole [here](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py); don't worry about the implementation but do read the documentation to understand the format of the actions and observations.

The simple physics involved would be very easy for a model-based algorithm to fit, (this is a common assignment in control theory using [proportional-integral-derivative](https://en.wikipedia.org/wiki/PID_controller) (PID) controllers) but today we're doing it model-free: your agent has no idea that these observations represent positions or velocities, and it has no idea what the laws of physics are. The network has to learn in which direction to bump the cart in response to the current state of the world.

Each environment can have different versions registered to it. By consulting [the Gym source](https://github.com/openai/gym/blob/master/gym/envs/__init__.py) you can see that CartPole-v0 and CartPole-v1 are the same environment, except that v1 has longer episodes. Again, a minor change like this can affect what algorithms score well; an agent might consistently survive for 200 steps in an unstable fashion that means it would fall over if ran for 500 steps.

### Outline of the Exercises

The exercises are roughly split into 4 sections:

1. Implement the Q-network that maps a state to an estimated value for each action.
2. Implement a replay buffer to store experiences $e_t = (s_t, a_t, r_{t+1}, d_{t+1}, s_{t+1})$.
3. Implement the policy which chooses actions based on the Q-network, plus epsilon greedy randomness to encourage exploration.
4. Piece everything together into a training loop and train your agent.

## The Q-Network

The Q-Network takes in an observation and outputs a number for each available action predicting how good it is, mimicking he behaviour of our Q-value table from yesterday.
For best results, the architecture of the Q-network can be customized to each particular problem. For example, [the architecture of OpenAI Five](https://cdn.openai.com/research-covers/openai-five/network-architecture.pdf) used to play DOTA 2 is pretty complex and involves LSTMs.

For learning from pixels, a simple convolutional network and some fully connected layers does quite well. Where we have already processed features here, it's even easier: an MLP of this size should be plenty large for any environment today.

Implement the Q-network using a standard MLP, constructed of alternating Linear and ReLU layers.
The size of the input will match the dimensionality of the observation space, and the size of the output will match the number of actions to choose from (associating a reward to each.)
The dimensions of the hidden_sizes are provided.

Below is a diagram of what our particular Q-Network will look like for CartPole:

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/q_mermaid.svg" width="250">

<details>
<summary>Why do we not include a ReLU at the end?</summary>

If you end with a ReLU, then your network can only predict 0 or positive Q-values. This will cause problems as soon as you encounter an environment with negative rewards, or you try to do some scaling of the rewards.
</details>

<details>
<summary>CartPole-v1 gives +1 reward on every timestep. Why would the network not just learn the constant +1 function regardless of observation?</summary>

The network is learning Q-values (the sum of all future expected discounted rewards from this state/action pair), not rewards. Correspondingly, once the agent has learned a good policy, the Q-value associated with state action pair (pole is slightly left of vertical, move cart left) should be large, as we would expect a long episode (and correspondingly lots of reward) by taking actions to help to balance the pole. Pairs like (cart near right boundary, move cart right) cause the episode to terminate, and as such the network will learn low Q-values.

</details>




### Exercise - implement `QNetwork`

```c
Difficulty: 🔴🔴⚪⚪⚪
Importance: 🔵🔵🔵⚪⚪

You should spend up to 10-15 minutes on this exercise.
```


```python
class QNetwork(nn.Module):
    '''For consistency with your tests, please wrap your modules in a `nn.Sequential` called `layers`.'''
    layers: nn.Sequential

    def __init__(
        self,
        dim_observation: int,
        num_actions: int,
        hidden_sizes: List[int] = [120, 84]
    ):
        super().__init__()
        pass
        
    def forward(self, x: t.Tensor) -> t.Tensor:
        return self.layers(x)


net = QNetwork(dim_observation=4, num_actions=2)
n_params = sum((p.nelement() for p in net.parameters()))
assert isinstance(getattr(net, "layers", None), nn.Sequential)
print(net)
print(f"Total number of parameters: {n_params}")
print("You should manually verify network is Linear-ReLU-Linear-ReLU-Linear")
assert n_params == 10934
```

<details>
<summary>Solution</summary>

```python
class QNetwork(nn.Module):
    '''For consistency with your tests, please wrap your modules in a `nn.Sequential` called `layers`.'''
    layers: nn.Sequential

    def __init__(
        self,
        dim_observation: int,
        num_actions: int,
        hidden_sizes: List[int] = [120, 84]
    ):
        super().__init__()
        # SOLUTION
        in_features_list = [dim_observation] + hidden_sizes
        out_features_list = hidden_sizes + [num_actions]
        layers = []
        for i, (in_features, out_features) in enumerate(zip(in_features_list, out_features_list)):
            layers.append(nn.Linear(in_features, out_features))
            if i < len(in_features_list) - 1:
                layers.append(nn.ReLU())
        self.layers = nn.Sequential(*layers)

    def forward(self, x: t.Tensor) -> t.Tensor:
        return self.layers(x)
```

</details>

## Replay Buffer

The goal of DQN is to reduce the reinforcement learning problem to a supervised learning problem.
In supervised learning, training examples should be drawn **i.i.d**. from some distribution, and we hope to generalize to future examples from that distribution.

In RL, the distribution of experiences $e_t = (s_t, a_t, r_{t+1}, s_{t+1})$ to train from depend on the policy $\pi$ followed, which depends on the current state of the Q-value network, so DQN is always chasing a moving target. This is why the training loss curve isn't going to have a nice steady decrease like in supervised learning. We will extend experiences to $e_t = (o_t, a_t, r_{t+1}, s_{t+1}, d_{t+1})$. Here, $d_{t+1}$ is a boolean indicating that $s_{t+1}$ is a terminal observation, and that no further interaction happened beyond $s_{t+1}$ in the episode from which it was generated.

### Correlated States

Due to DQN using a neural network to learn the Q-values, the value of many state-action pairs are aggregated together (unlike tabular Q-learning which learns independently the value of each state-action pair). For example, consider a game of chess. The board will have some encoding as a vector, but visually similar board states might have wildly different consequences for the best move. Another problem is that states within an episode are highly correlated and not i.i.d. at all. A few bad moves from the start of the game might doom the rest of the game regardless how well the agent tries to recover, whereas a few bad moves near the end of the game might not matter if the agent has a very strong lead, or is so far behind the game is already lost. Training mostly on an episode where the agent opened the game poorly might disincentive good moves to recover, as these too will have poor Q-value estimates.

### Uniform Sampling

To recover from this problem and make the environment look "more i.i.d", a simple strategy that works decently well is to pick a buffer size, store experiences and uniformly sample out of that buffer. Intuitively, if we want the policy to play well in all sorts of states, the sampled batch should be a representative sample of all the diverse scenarios that can happen in the environment.

For complex environments, this implies a very large batch size (or doing something better than uniform sampling). [OpenAI Five](https://cdn.openai.com/dota-2.pdf) used batch sizes of over 2 million experiences for Dota 2.

The capacity of the replay buffer is yet another hyperparameter; if it's too small then it's just going to be full of recent and correlated examples. But if it's too large, we pay an increasing cost in memory usage and the information may be too old to be relevant.


### Exercise - implement `ReplayBuffer`

```c
Difficulty: 🔴🔴🔴⚪⚪
Importance: 🔵🔵🔵⚪⚪

You should spend up to 20-30 minutes on this exercise.
```

You have the classes `ReplayBuffer` and `ReplayBufferSamples` below. The former holds data from past experiences, and contains methods for sampling that data. When you sample that data, you return an object of the former type. The data in question are experiences $e_t = (s_t, a_t, r_{t+1}, d_{t+1}, s_{t+1})$, and they are held in a circular queue in the replay buffer object. If the buffer is already full, the oldest experience is overwritten.

You should also include objects `self.observations`, `self.actions`, etc in your `ReplayBuffer` class. This is just so that you can plot them against your shuffled replay buffer, and verify that the outputs look reasonable (see the next section).

To recap, your `add` method should:

* Add all the new experience data to the replay buffer (note that we've initialized tensors like `self.observations` to be empty with the correct size, so you don't need to deal with special cases like when the buffer is empty),
* If the replay buffer is longer than `self.buffer_size` then slice off the values at the end.

<details>
<summary>Divergence - whether you should add new data to the start or end of the buffer.</summary>

If the data you were appending to the buffer with each `add` step was data over multiple timesteps generated chronologically, then you should add the data to the end of the buffer and slice from the start of it. If you did it the other way around, then for a given chronological slice of data added to the buffer, it would be the latest experiences which were sliced off first, rather than the earliest ones.

Here we don't need to worry about that, because we only add one timestep of data at once. Doing it either way will still pass the tests.

However, you still do need to be careful - obviously appending data to the end of the buffer then slicing overflow off the end won't work!

</details>

And your `sample` method should:

* Sample indices with replacement from range `[0, current_batch_size)`,
* Use these to return a `ReplayBufferSamples` object containing the experiences sampled from the buffer.
    * Remember to convert them to tensors before creating the `ReplayBufferSamples` object, and move them to the right device, as indicated by the `device` argument.
    
Some more notes on the implementation below:

* The `ReplayBufferSamples` class has a `__post_init__` method which runs after a sample is created, which will test whether the objects actually are tensors. Mixing up numpy arrays and tensors is one of many very common RL mistakes!
* It's fine for the `add` method to store data along a single dimension (rather than having separate dimensions for `num_environments` and `num_steps`) because the number of environments is guaranteed to be 1 (see the `__init__` function). Tomorrow for PPO, we'll lift this restriction.

```python
@dataclass
class ReplayBufferSamples:
    '''
    Samples from the replay buffer, converted to PyTorch for use in neural network training.

    Data is equivalent to (s_t, a_t, r_{t+1}, d_{t+1}, s_{t+1}).
    '''
    observations: Tensor # shape [sample_size, *observation_shape]
    actions: Tensor # shape [sample_size, *action_shape]
    rewards: Tensor # shape [sample_size,]
    dones: Tensor # shape [sample_size,]
    next_observations: Tensor # shape [sample_size, observation_shape]

    def __post_init__(self):
        for exp in self.__dict__.values():
            assert isinstance(exp, Tensor), f"Error: expected type tensor, found {type(exp)}"


class ReplayBuffer:
    '''
    Contains buffer; has a method to sample from it to return a ReplayBufferSamples object.
    '''
    rng: Generator
    observations: np.ndarray # shape [buffer_size, *observation_shape]
    actions: np.ndarray # shape [buffer_size, *action_shape]
    rewards: np.ndarray # shape [buffer_size,]
    dones: np.ndarray # shape [buffer_size,]
    next_observations: np.ndarray # shape [buffer_size, *observation_shape]

    def __init__(self, num_environments: int, obs_shape: Tuple[int], action_shape: Tuple[int], buffer_size: int, seed: int):
        assert num_environments == 1, "This buffer only supports SyncVectorEnv with 1 environment inside."
        self.num_environments = num_environments
        self.obs_shape = obs_shape
        self.action_shape = action_shape
        self.buffer_size = buffer_size
        self.rng = np.random.default_rng(seed)

        self.observations = np.empty((0, *self.obs_shape), dtype=np.float32)
        self.actions = np.empty(0, dtype=np.int32)
        self.rewards = np.empty(0, dtype=np.float32)
        self.dones = np.empty(0, dtype=bool)
        self.next_observations = np.empty((0, *self.obs_shape), dtype=np.float32)


    def add(
        self, obs: np.ndarray, actions: np.ndarray, rewards: np.ndarray, dones: np.ndarray, next_obs: np.ndarray
    ) -> None:
        '''
        obs: shape (num_environments, *observation_shape)
            Observation before the action
        actions: shape (num_environments, *action_shape)
            Action chosen by the agent
        rewards: shape (num_environments,)
            Reward after the action
        dones: shape (num_environments,)
            If True, the episode ended and was reset automatically
        next_obs: shape (num_environments, *observation_shape)
            Observation after the action
            If done is True, this should be the terminal observation, NOT the first observation of the next episode.
        '''
        assert obs.shape == (self.num_environments, *self.obs_shape)
        assert actions.shape == (self.num_environments, *self.action_shape)
        assert rewards.shape == (self.num_environments,)
        assert dones.shape == (self.num_environments,)
        assert next_obs.shape == (self.num_environments, *self.obs_shape)

        pass

        
    def sample(self, sample_size: int, device: t.device) -> ReplayBufferSamples:
        '''
        Uniformly sample sample_size entries from the buffer and convert them to PyTorch tensors on device.
        Sampling is with replacement, and sample_size may be larger than the buffer size.
        '''
        pass


tests.test_replay_buffer_single(ReplayBuffer)
tests.test_replay_buffer_deterministic(ReplayBuffer)
tests.test_replay_buffer_wraparound(ReplayBuffer)
```

<details>
<summary>Solution</summary>

```python
@dataclass
class ReplayBufferSamples:
    '''
    Samples from the replay buffer, converted to PyTorch for use in neural network training.

    Data is equivalent to (s_t, a_t, r_{t+1}, d_{t+1}, s_{t+1}).
    '''
    observations: Tensor # shape [sample_size, *observation_shape]
    actions: Tensor # shape [sample_size, *action_shape]
    rewards: Tensor # shape [sample_size,]
    dones: Tensor # shape [sample_size,]
    next_observations: Tensor # shape [sample_size, observation_shape]

    def __post_init__(self):
        for exp in self.__dict__.values():
            assert isinstance(exp, Tensor), f"Error: expected type tensor, found {type(exp)}"


class ReplayBuffer:
    '''
    Contains buffer; has a method to sample from it to return a ReplayBufferSamples object.
    '''
    rng: Generator
    observations: np.ndarray # shape [buffer_size, *observation_shape]
    actions: np.ndarray # shape [buffer_size, *action_shape]
    rewards: np.ndarray # shape [buffer_size,]
    dones: np.ndarray # shape [buffer_size,]
    next_observations: np.ndarray # shape [buffer_size, *observation_shape]

    def __init__(self, num_environments: int, obs_shape: int, buffer_size: int, seed: int):
        assert num_environments == 1, "This buffer only supports SyncVectorEnv with 1 environment inside."
        self.num_environments = num_environments
        self.obs_shape = obs_shape
        self.action_shape = action_shape
        self.buffer_size = buffer_size
        self.rng = np.random.default_rng(seed)

        self.observations = np.empty((0, *self.obs_shape), dtype=np.float32)
        self.actions = np.empty(0, dtype=np.int32)
        self.rewards = np.empty(0, dtype=np.float32)
        self.dones = np.empty(0, dtype=bool)
        self.next_observations = np.empty((0, *self.obs_shape), dtype=np.float32)


    def add(
        self, obs: np.ndarray, actions: np.ndarray, rewards: np.ndarray, dones: np.ndarray, next_obs: np.ndarray
    ) -> None:
        '''
        obs: shape (num_environments, *observation_shape)
            Observation before the action
        actions: shape (num_environments, *action_shape)
            Action chosen by the agent
        rewards: shape (num_environments,)
            Reward after the action
        dones: shape (num_environments,)
            If True, the episode ended and was reset automatically
        next_obs: shape (num_environments, *observation_shape)
            Observation after the action
            If done is True, this should be the terminal observation, NOT the first observation of the next episode.
        '''
        assert obs.shape == (self.num_environments, *self.obs_shape)
        assert actions.shape == (self.num_environments, *self.action_shape)
        assert rewards.shape == (self.num_environments,)
        assert dones.shape == (self.num_environments,)
        assert next_obs.shape == (self.num_environments, *self.obs_shape)

        # SOLUTION
        # We update each one manually, but you could also use a for loop with setattr & getattr
        self.observations = np.concatenate((self.observations, obs))[-self.buffer_size:]
        self.actions = np.concatenate((self.actions, actions))[-self.buffer_size:]
        self.rewards = np.concatenate((self.rewards, rewards))[-self.buffer_size:]
        self.dones = np.concatenate((self.dones, dones))[-self.buffer_size:]
        self.next_observations = np.concatenate((self.next_observations, next_obs))[-self.buffer_size:]

    def sample(self, sample_size: int, device: t.device) -> ReplayBufferSamples:
        '''
        Uniformly sample sample_size entries from the buffer and convert them to PyTorch tensors on device.
        Sampling is with replacement, and sample_size may be larger than the buffer size.
        '''
        # SOLUTION
        current_buffer_size = self.observations.shape[0]
        indices = self.rng.integers(0, current_buffer_size, sample_size)
        buffer_experiences = [self.observations, self.actions, self.rewards, self.dones, self.next_observations]
        samples = [t.as_tensor(buffer_exp[indices], device=device) for buffer_exp in buffer_experiences]
        return ReplayBufferSamples(*samples)


tests.test_replay_buffer_single(ReplayBuffer)
tests.test_replay_buffer_deterministic(ReplayBuffer)
tests.test_replay_buffer_wraparound(ReplayBuffer)
```

</details>

When you've passed the tests, you can run the following code to visualize your cart's position and angle, and how these look in both the buffer and the buffer's random samples. Do the samples look correctly shuffled? Also, based on the [CartPole source code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py),do the termination points look like they make sense? (Note, the values in the tables on the source code page are the maximum allowed values, which are different to the values which cause the episode to terminate).

Look at the plots closely - you can see how our termination timesteps $d_{t+1} = 1$ (indicated by grey lines) align with observations $s_t$ which are just *within* the allowed bounds, in other words $s_{t+1}$ (which we don't see in the graphs below) is out of bounds.

```python
rb = ReplayBuffer(num_environments=1, obs_shape=(4,), action_shape=(), buffer_size=256, seed=0)
envs = gym.vector.SyncVectorEnv([make_env("CartPole-v1", 0, 0, False, "test")])
obs = envs.reset()
for i in range(256):
    # Choose a random next action, and take a step in the environment
    actions = envs.action_space.sample()
    (next_obs, rewards, dones, infos) = envs.step(actions)
    # Add observations to buffer, and set obs = next_obs ready for the next step
    rb.add(obs, actions, rewards, dones, next_obs)
    obs = next_obs

plot_cartpole_obs_and_dones(rb.observations, rb.dones, title="CartPole experiences s<sub>t</sub> (dotted lines = termination)")

sample = rb.sample(256, t.device("cpu"))
plot_cartpole_obs_and_dones(sample.observations, sample.dones, title="CartPole experiences s<sub>t</sub> (randomly sampled) (dotted lines = termination)")
```

## Environment Resets

There's a subtlety to the Gym API around what happens when the agent fails and the episode is terminated. Our environment is set up to automatically reset at the end of an episode, but when this happens the `next_obs` returned from `step` is actually the initial observation of the new episode.

To see this, run the cell above again, but replace `rb.observations` with `rb.next_observations`. You'll see that the values corresponding to termination $d_{t+1} = 1$ are actually the new values of the reset environment. We don't want this, instead we want these `next_obs` values to be the out-of-bounds values which caused the episode to terminate.

Run the code below to see how we handle this (make sure you understand this, because you'll need to implement this later on). The code also plots `buffer.next_observations`, and if you examine this plot you should see that the values $s_{t+1}$ corresponding to termination $d_{t+1}$ are out of bounds, which is what we want.

```python
rb = ReplayBuffer(num_environments=1, obs_shape=(4,), action_shape=(), buffer_size=256, seed=0)
envs = gym.vector.SyncVectorEnv([make_env("CartPole-v1", 0, 0, False, "test")])
obs = envs.reset()
for i in range(256):
    # Choose a random next action, and take a step in the environment
    actions = envs.action_space.sample()
    (next_obs, rewards, dones, infos) = envs.step(actions)

    # Get actual next_obs, by replacing next_obs with terminal observation at all envs which are terminated
    real_next_obs = next_obs.copy()
    for environment, done in enumerate(dones):
        if done:
            print(f'Environment {environment} terminated after {infos[0]["episode"]["l"]} steps')
            real_next_obs[environment] = infos[environment]["terminal_observation"]

    # Add the next_obs to the buffer (which has the terminated states), but set obs=new_obs (which has the restarted states)
    rb.add(obs, actions, rewards, dones, real_next_obs)
    obs = next_obs

plot_cartpole_obs_and_dones(rb.next_observations, rb.dones, title="CartPole experiences s<sub>t+1</sub> (dotted lines = termination)")
```

## Exploration

DQN makes no attempt to explore intelligently. The exploration strategy is the same as
for Q-Learning: agents take a random action with probability epsilon, but now we gradually
decrease epsilon. The Q-network is also randomly initialized (rather than initialized with zeros),
so its predictions of what is the best action to take are also pretty random to start.

Some games like [Montezuma's Revenge](https://paperswithcode.com/task/montezumas-revenge) have sparse rewards that require more advanced exploration methods to obtain. The player is required to collect specific keys to unlock specific doors, but unlike humans, DQN has no prior knowledge about what a key or a door is, and it turns out that bumbling around randomly has too low of a probability of correctly matching a key to its door. Even if the agent does manage to do this, the long separation between finding the key and going to the door makes it hard to learn that picking the key up was important.

As a result, DQN scored an embarrassing 0% of average human performance on this game.

### Reward Shaping

One solution to sparse rewards is to use human knowledge to define auxillary reward functions that are more dense and made the problem easier (in exchange for leaking in side knowledge and making
the algorithm more specific to the problem at hand). What could possibly go wrong?

The canonical example is for a game called [CoastRunners](https://openai.com/blog/faulty-reward-functions/), where the goal was given to maximize the
score (hoping that the agent would learn to race around the map). Instead, it found it could
gain more score by driving in a loop picking up power-ups just as they respawn, crashing and
setting the boat alight in the process.

### Reward Hacking

For Montezuma's Revenge, the reward was shaped by giving a small reward for
picking up the key.
One time this was tried, the reward was given slightly too early and the agent learned it could go close to the key without quite picking it up, obtain the auxillary reward, and then back up and repeat.

[![Montezuma Reward Hacking](https://img.youtube.com/vi/_sFp1ffKIc8/0.jpg)](https://www.youtube.com/watch?v=_sFp1ffKIc8 "Montezuma Reward Hacking")

A collected list of examples of Reward Hacking can be found [here](https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml).


### Advanced Exploration

It would be better if the agent didn't require these auxillary rewards to be hardcoded by humans,
but instead reply on other signals from the environment that a state might be worth exploring. One idea is that a state which is "surprising" or "novel" (according to the agent's current belief
of how the environment works) in some sense might be valuable. Designing an agent to be
innately curious presents a potential solution to exploration, as the agent will focus exploration
in areas it is unfamiliar with. In 2018, OpenAI released [Random Network Distillation](https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/) which made progress in formalizing this notion, by measuring the agent's ability to predict the output of a neural network
on visited states. States that are hard to predict are poorly explored, and thus highly rewarded.
In 2019, an excellent paper [First return, then explore](https://arxiv.org/pdf/2004.12919v6.pdf) found an even better approach. Such reward shaping can also be gamed, leading to the
noisy TV problem, where agents that seek novelty become entranced by a source of randomness in the
environment (like a analog TV out of tune displaying white noise), and ignore everything else
in the environment.

### Exercise - implement linear scheduler

```c
Difficulty: 🔴⚪⚪⚪⚪
Importance: 🔵🔵⚪⚪⚪

You should spend up to 5-10 minutes on this exercise.
```

For now, implement the basic linearly decreasing exploration schedule.

```python
def linear_schedule(
    current_step: int, start_e: float, end_e: float, exploration_fraction: float, total_timesteps: int
) -> float:
    '''Return the appropriate epsilon for the current step.

    Epsilon should be start_e at step 0 and decrease linearly to end_e at step (exploration_fraction * total_timesteps).
    In other words, we are in "explore mode" with start_e >= epsilon >= end_e for the first `exploration_fraction` fraction
    of total timesteps, and then stay at end_e for the rest of the episode.
    '''
    pass


epsilons = [
    linear_schedule(step, start_e=1.0, end_e=0.05, exploration_fraction=0.5, total_timesteps=500)
    for step in range(500)
]
line(epsilons, labels={"x": "steps", "y": "epsilon"}, title="Probability of random action", height=400, width=600)

tests.test_linear_schedule(linear_schedule)
```

<details>

<summary>Plot of the Intended Schedule</summary>

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/newplot.png" width="560">
</details>

<details>
<summary>Solution</summary>

```python
def linear_schedule(
    current_step: int, start_e: float, end_e: float, exploration_fraction: float, total_timesteps: int
) -> float:
    '''Return the appropriate epsilon for the current step.

    Epsilon should be start_e at step 0 and decrease linearly to end_e at step (exploration_fraction * total_timesteps).
    In other words, we are in "explore mode" with start_e >= epsilon >= end_e for the first `exploration_fraction` fraction
    of total timesteps, and then stay at end_e for the rest of the episode.
    '''
    # SOLUTION
    return start_e + (end_e - start_e) * min(current_step / (exploration_fraction * total_timesteps), 1)
```

</details>


## Epsilon Greedy Policy

In DQN, the policy is implicitly defined by the Q-network: we take the action with the maximum predicted reward. This gives a bias towards optimism. By estimating the maximum of a set of values $v_1, \ldots, v_n$ using the maximum of some noisy estimates $\hat{v}_1, \ldots, \hat{v}_n$ with $\hat{v}_i \approx v$, we get unlucky and get very large positive noise on some samples, which the maximum then chooses. Hence, the agent will choose actions that the Q-network is overly optimistic about.

See Sutton and Barto, Section 6.7 if you'd like a more detailed explanation, or the original [Double Q-Learning](https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf) paper which notes this maximisation bias, and introduces a method to correct for it using two separate Q-value estimators, each used to update the other.

### Exercise - implement epsilon greedy policy

```c
Difficulty: 🔴🔴⚪⚪⚪
Importance: 🔵🔵🔵⚪⚪

You should spend up to 10-20 minutes on this exercise.
```

We've given you the first line of code, to convert the numpy array `obs` into a tensor on the correct device.

Other tips:

- Don't forget to convert the result back to a `np.ndarray`.
- Use `rng.random()` to generate random numbers in the range $[0,1)$, and `rng.integers(0, n, size)` for an array of shape `size` random integers in the range $0, 1, \ldots, n-1$.
- Use `envs.single_action_space.n` to retrieve the number of possible actions.

```python
def epsilon_greedy_policy(
    envs: gym.vector.SyncVectorEnv, q_network: QNetwork, rng: Generator, obs: np.ndarray, epsilon: float
) -> np.ndarray:
    '''With probability epsilon, take a random action. Otherwise, take a greedy action according to the q_network.
    Inputs:
        envs : gym.vector.SyncVectorEnv, the family of environments to run against
        q_network : QNetwork, the network used to approximate the Q-value function
        obs : The current observation
        epsilon : exploration percentage
    Outputs:
        actions: (n_environments, *action_shape) the sampled action for each environment.
    '''
    # Convert `obs` into a tensor so we can feed it into our model
    device = next(q_network.parameters()).device
    obs = t.from_numpy(obs).to(device)

    pass


tests.test_epsilon_greedy_policy(epsilon_greedy_policy)
```

<details>
<summary>Solution</summary>

```python
def epsilon_greedy_policy(
    envs: gym.vector.SyncVectorEnv, q_network: QNetwork, rng: Generator, obs: np.ndarray, epsilon: float
) -> np.ndarray:
    '''With probability epsilon, take a random action. Otherwise, take a greedy action according to the q_network.
    Inputs:
        envs : gym.vector.SyncVectorEnv, the family of environments to run against
        q_network : QNetwork, the network used to approximate the Q-value function
        obs : The current observation
        epsilon : exploration percentage
    Outputs:
        actions: (n_environments, *action_shape) the sampled action for each environment.
    '''
    # Convert `obs` into a tensor so we can feed it into our model
    device = next(q_network.parameters()).device
    obs = t.from_numpy(obs).to(device)

    # SOLUTION
    num_actions = envs.single_action_space.n
    if rng.random() < epsilon:
        return rng.integers(0, num_actions, size = (envs.num_envs,))
    else:
        q_scores = q_network(obs)
        return q_scores.argmax(-1).detach().cpu().numpy()


tests.test_epsilon_greedy_policy(epsilon_greedy_policy)
```

</details>

## Probe Environments

Extremely simple probe environments are a great way to debug your algorithm. The first one is given below.

Let's try and break down how this environment works. We see that the function `step` always returns the same thing. The observation and reward are always the same, and `done` is always true (i.e. the episode always terminates after one action). We expect the agent to rapidly learn that the value of the constant observation `[0.0]` is `+1`. This is in some sense the simplest possible probe.

### A note on action spaces

The action space we're using here is `gym.spaces.Box`. This means we're dealing with real-valued quantities, i.e. continuous not discrete. The first two arguments of `Box` are `low` and `high`, and these define a box in $\mathbb{R}^n$. For instance, if these arrays are `(0, 0)` and `(1, 1)` respectively, this defines the box $0 \leq x, y \leq 1$ in 2D space.

```python
ObsType = np.ndarray
ActType = int


class Probe1(gym.Env):
    '''One action, observation of [0.0], one timestep long, +1 reward.

    We expect the agent to rapidly learn that the value of the constant [0.0] observation is +1.0. Note we're using a continuous observation space for consistency with CartPole.
    '''

    action_space: Discrete
    observation_space: Box

    def __init__(self):
        super().__init__()
        self.observation_space = Box(np.array([0]), np.array([0]))
        self.action_space = Discrete(1)
        self.seed()
        self.reset()

    def step(self, action: ActType) -> Tuple[ObsType, float, bool, dict]:
        return (np.array([0]), 1.0, True, {})

    def reset(
        self, seed: Optional[int] = None, return_info=False, options=None
    ) -> Union[ObsType, Tuple[ObsType, dict]]:
        super().reset(seed=seed)
        if return_info:
            return (np.array([0.0]), {})
        return np.array([0.0])


gym.envs.registration.register(id="Probe1-v0", entry_point=Probe1)
env = gym.make("Probe1-v0")
assert env.observation_space.shape == (1,)
assert env.action_space.shape == ()
```

### Exercise - read & understand other probe environments

```c
Difficulty: 🔴🔴⚪⚪⚪
Importance: 🔵🔵🔵🔵⚪

You should spend up to 10-20 minutes here.
```

For each of the probes below, read their implementation code, and understand how they correspond to their docstrings (and to the [descriptions](https://andyljones.com/posts/rl-debugging.html#:~:text=Use%20probe%20environments) given in Andy Jones' post).

It's very important to understand how these probes work, and why they're useful tools for debugging. When you're working on your own RL projects, you might have to write your own probes to suit your particular use cases.

```python
class Probe2(gym.Env):
    '''One action, observation of [-1.0] or [+1.0], one timestep long, reward equals observation.

    We expect the agent to rapidly learn the value of each observation is equal to the observation.
    '''

    action_space: Discrete
    observation_space: Box

    def __init__(self):
        super().__init__()
        self.observation_space = Box(np.array([-1.0]), np.array([+1.0]))
        self.action_space = Discrete(1)
        self.reset()
        self.reward = None

    def step(self, action: ActType) -> Tuple[ObsType, float, bool, dict]:
        assert self.reward is not None
        return np.array([self.observation]), self.reward, True, {}

    def reset(
        self, seed: Optional[int] = None, return_info=False, options=None
    ) -> Union[ObsType, Tuple[ObsType, dict]]:
        super().reset(seed=seed)
        self.reward = 1.0 if self.np_random.random() < 0.5 else -1.0
        self.observation = self.reward
        if return_info:
            return np.array([self.reward]), {}
        return np.array([self.reward])

gym.envs.registration.register(id="Probe2-v0", entry_point=Probe2)


class Probe3(gym.Env):
    '''One action, [0.0] then [1.0] observation, two timesteps, +1 reward at the end.

    We expect the agent to rapidly learn the discounted value of the initial observation.
    '''

    action_space: Discrete
    observation_space: Box

    def __init__(self):
        super().__init__()
        self.observation_space = Box(np.array([-0.0]), np.array([+1.0]))
        self.action_space = Discrete(1)
        self.reset()

    def step(self, action: ActType) -> Tuple[ObsType, float, bool, dict]:
        self.n += 1
        if self.n == 1:
            return np.array([1.0]), 0.0, False, {}
        elif self.n == 2:
            return np.array([0.0]), 1.0, True, {}
        raise ValueError(self.n)

    def reset(
        self, seed: Optional[int] = None, return_info=False, options=None
    ) -> Union[ObsType, Tuple[ObsType, dict]]:
        # SOLUTION
        super().reset(seed=seed)
        self.n = 0
        if return_info:
            return np.array([0.0]), {}
        return np.array([0.0])

gym.envs.registration.register(id="Probe3-v0", entry_point=Probe3)


class Probe4(gym.Env):
    '''Two actions, [0.0] observation, one timestep, reward is -1.0 or +1.0 dependent on the action.

    We expect the agent to learn to choose the +1.0 action.
    '''

    action_space: Discrete
    observation_space: Box

    def __init__(self):
        self.observation_space = Box(np.array([-0.0]), np.array([+0.0]))
        self.action_space = Discrete(2)
        self.reset()

    def step(self, action: ActType) -> Tuple[ObsType, float, bool, dict]:
        reward = -1.0 if action == 0 else 1.0
        return np.array([0.0]), reward, True, {}

    def reset(
        self, seed: Optional[int] = None, return_info=False, options=None
    ) -> Union[ObsType, Tuple[ObsType, dict]]:
        super().reset(seed=seed)
        if return_info:
            return np.array([0.0]), {}
        return np.array([0.0])

gym.envs.registration.register(id="Probe4-v0", entry_point=Probe4)


class Probe5(gym.Env):
    '''Two actions, random 0/1 observation, one timestep, reward is 1 if action equals observation otherwise -1.

    We expect the agent to learn to match its action to the observation.
    '''

    action_space: Discrete
    observation_space: Box

    def __init__(self):
        self.observation_space = Box(np.array([-1.0]), np.array([+1.0]))
        self.action_space = Discrete(2)
        self.reset()

    def step(self, action: ActType) -> Tuple[ObsType, float, bool, dict]:
        reward = 1.0 if action == self.obs else -1.0
        return np.array([self.obs]), reward, True, {}

    def reset(
        self, seed: Optional[int] = None, return_info=False, options=None
    ) -> Union[ObsType, Tuple[ObsType, dict]]:
        super().reset(seed=seed)
        self.obs = 1.0 if self.np_random.random() < 0.5 else 0.0
        if return_info:
            return np.array([self.obs], dtype=float), {}
        return np.array([self.obs], dtype=float)

gym.envs.registration.register(id="Probe5-v0", entry_point=Probe5)
```


A brief summary of these, along with recommendations of where to go to debug if one of them fails (note that these won't be true 100% of the time, but should hopefully give you some useful direction):

* **Probe 1 tests basic learning ability**. If this fails, it means the agent has failed to learn to associate a constant observation with a constant reward. You should check your loss functions and optimizers in this case.
* **Probe 2 tests the agent's ability to differentiate between 2 different observations (and learn their respective values)**. If this fails, it means the agent has issues with handling multiple possible observations.
* **Probe 3 tests the agent's ability to handle time & reward delay**. If this fails, it means the agent has problems with multi-step scenarios of discounting future rewards. You should look at how your agent step function works.
* **Probe 4 tests the agent's ability to learn from actions leading to different rewards**. If this fails, it means the agent has failed to change its policy for different rewards, and you should look closer at how your agent is updating its policy based on the rewards it receives & the loss function.
* **Probe 5 tests the agent's ability to map observations to actions**. If this fails, you should look at the code which handles multiple timesteps, as well as the code that handles the agent's map from observations to actions.

## Main DQN Algorithm

We now combine all the elements we have designed thus far into the final DQN algorithm. Here, we assume the environment returns three parameters $(s_{new}, r, d)$, a new state $s_{new}$, a reward $r$ and a boolean $d$ indicating whether interaction has terminated yet.

Our Q-value function $Q(s,a)$ is now a network $Q(s,a ; \theta)$ parameterised by weights $\theta$. The key idea, as in Q-learning, is to ensure the Q-value function satisfies the optimal Bellman equation
$$
Q(s,a ; \theta)
= \mathbb{E}_{s',r \sim p(\cdot \mid s,a)} \left[r + \gamma \max_{a'} Q(s', a' ;\theta) \right]
$$

$$
\delta_t = \mathbb{E} \left[ r_t + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t) \right]
$$
Letting $y_t = r_t + \gamma \max_a Q(s_{t+1}, a)$, we can use the expected squared TD-Error $\delta_t^2 = (y_t - Q(s_t, a_t))^2$ as the loss function to optimize against. Since we want the modle to learn from a variety of experiences (recall that supervised learning is assuming i.i.d) we approximate the expectation by sampling a batch $B = \{s^i, a^i, r^i, s^i_\text{new}\}$ of experiences from the replay buffer, and try to adjust $\theta$ to make the loss
$$
L(\theta) = \frac{1}{|B|} \sum_{i=1}^B \left( r^i +
\gamma \max_a Q(s^i_\text{new}, a ; \theta_\text{target}) - Q(s^i, a^i ; \theta) \right)^2
$$
smaller via gradient descent. Here, $\theta_\text{target}$ is a previous copy of the parameters $\theta$. Every so often, we then update the target $\theta_\text{target} \leftarrow \theta$ as the agent improves it's Q-values from experience.

The image below uses the notation $[[ S ]]$ - this means 1 if $S$ is True, and 0 if $S$ is False.

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/dqn_algo.png" width="550">

On line 13, we need to check if the environment is still running before adding the $\gamma \max_{a'}Q(s^i_\text{new}, a' ; \theta_\text{target})$ term, as terminal states don't have future rewards, so we zero the second term if $d^i = \text{True}$, indicating that the episode has terminated.

### Weights & Biases

In previous parts, we've just trained the agent, and then plotted the reward per episode after training. For small toy examples that train in a few seconds this is fine, but for longer runs we'd like to watch the run live and make sure the agent is doing something interesting (especially if we were planning to run the model overnight.)

Luckily, **Weights and Biases** has got us covered! When you run your experiments, you'll be able to view not only *live plots* of the loss and average reward per episode while the agent is training - you can also log and view animations, which visualise your agent's progress in real time! The code below will handle all logging.

### DQN Dataclass

Below is a dataclass for training your DQN. You can use the `arg_help` method to see a description of each argument (it will also highlight any arguments which have ben changed from their default values).

The exact breakdown of training is as follows:

* The agent takes `total_timesteps` steps in the environment during the training loop.
* The first `buffer_size` of these steps are used to fill the replay buffer (we don't update gradients until the buffer is full).
* After this point, we perform an optimizer step every `train_frequency` steps of our agent.

This is shown in the diagram below (not to scale!).

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/dqn_breakdown.png" width="500">


```python
@dataclass
class DQNArgs:
    # Basic / global
    seed: int = 1
    cuda: bool = t.cuda.is_available()
    env_id: str = "CartPole-v1"

    # Wandb / logging
    use_wandb: bool = False
    capture_video: bool = True
    exp_name: str = "DQN_implementation"
    log_dir: str = "logs"
    wandb_project_name: str = "CartPoleDQN"
    wandb_entity: Optional[str] = None

    # Duration of different phases
    buffer_size: int = 10_000
    train_frequency: int = 10
    total_timesteps: int = 500_000
    target_network_frequency: int = 500

    # Optimization hyperparameters
    batch_size: int = 128
    learning_rate: float = 0.00025
    start_e: float = 1.0
    end_e: float = 0.1

    # Misc. RL related
    gamma: float = 0.99
    exploration_fraction: float = 0.2
    
    def __post_init__(self):
        assert self.total_timesteps - self.buffer_size >= self.train_frequency
        self.total_training_steps = (self.total_timesteps - self.buffer_size) // self.train_frequency


args = DQNArgs(batch_size=256)
utils.arg_help(args)
```

### Exercise - fill in the agent class

```c
Difficulty: 🔴🔴🔴🔴⚪
Importance: 🔵🔵🔵🔵⚪

You should spend up to 25-45 minutes on this exercise.
```

You should now fill in the methods for the `DQNAgent` class below. This is a class which is designed to handle taking steps in the environment (with an epsilon greedy policy), and updating the buffer.

The `play_step` function should do the following:

* Get a new set of actions via the `self.get_actions` method (taking `self.next_obs` as our current observation)
* Step the environment, via `self.envs.step` (which returns a new set of experiences)
* Add the new experiences to the buffer (make sure you use the correct version of `next_obs`, i.e. the one which has out-of-bounds states if the environment has terminated)
* Set `self.next_obs` to the new observations, so the agent knows where it is for the next step (make sure you use the version of `next_obs` which has reset states if the environment has terminated)
* Increment the global step counter `self.step`
* Return the diagnostic information from the new experiences (i.e. the `infos` dicts which is the fourth argument returned from `self.envs.step`)

The `get_actions` function should do the following:

* Set `self.epsilon` according to the linear schedule function & the current global step counter
* Sample actions according to the epsilon-greedy policy (i.e. using your `epsilon_greedy_policy` function), and return them

```python
class DQNAgent:
    '''Base Agent class handling the interaction with the environment.'''

    def __init__(
        self,
        envs: gym.vector.SyncVectorEnv,
        args: DQNArgs,
        rb: ReplayBuffer,
        q_network: QNetwork,
        target_network: QNetwork,
        rng: np.random.Generator
    ):
        self.envs = envs
        self.args = args
        self.rb = rb
        self.next_obs = self.envs.reset() # Need a starting observation!
        self.step = 0
        self.epsilon = args.start_e
        self.q_network = q_network
        self.target_network = target_network
        self.rng = rng

    def play_step(self) -> List[dict]:
        '''
        Carries out a single interaction step between the agent and the environment, and adds results to the replay buffer.

        Returns `infos` (list of dictionaries containing info we will log).
        '''
        pass

    def get_actions(self, obs: np.ndarray) -> np.ndarray:
        '''
        Samples actions according to the epsilon-greedy policy using the linear schedule for epsilon.
        '''
        pass


tests.test_agent(DQNAgent)
```

<details>
<summary>Solution</summary>

```python
class DQNAgent:
    '''Base Agent class handling the interaction with the environment.'''

    def __init__(
        self,
        envs: gym.vector.SyncVectorEnv,
        args: DQNArgs,
        rb: ReplayBuffer,
        q_network: QNetwork,
        target_network: QNetwork,
        rng: np.random.Generator
    ):
        self.envs = envs
        self.args = args
        self.rb = rb
        self.next_obs = self.envs.reset() # Need a starting observation!
        self.step = 0
        self.epsilon = args.start_e
        self.q_network = q_network
        self.target_network = target_network
        self.rng = rng

    def play_step(self) -> List[dict]:
        '''
        Carries out a single interaction step between the agent and the environment, and adds results to the replay buffer.

        Returns `infos` (list of dictionaries containing info we will log).
        '''
        # SOLUTION
        obs = self.next_obs
        actions = self.get_actions(obs)
        next_obs, rewards, dones, infos = self.envs.step(actions)
        real_next_obs = next_obs.copy()
        for (environment, done) in enumerate(dones):
            if done:
                real_next_obs[environment] = infos[environment]["terminal_observation"]
        self.rb.add(obs, actions, rewards, dones, real_next_obs)

        self.next_obs = next_obs
        self.step += 1
        return infos

    def get_actions(self, obs: np.ndarray) -> np.ndarray:
        '''
        Samples actions according to the epsilon-greedy policy using the linear schedule for epsilon.
        '''
        # SOLUTION
        self.epsilon = linear_schedule(self.step, self.args.start_e, self.args.end_e, self.args.exploration_fraction, self.args.total_timesteps)
        actions = epsilon_greedy_policy(self.envs, self.q_network, self.rng, obs, self.epsilon)
        assert actions.shape == (len(self.envs.envs),)
        return actions
```

</details>

### Exercise - write DQN training loop

```c
Difficulty: 🔴🔴🔴🔴🔴
Importance: 🔵🔵🔵🔵🔵

You should spend up to 30-60 minutes on this exercise.
```

Now we'll create a new class `DQNTrainer`, which will handle the full training loop. We've filled in the `__init__` for you, which defines the following important attributes (among others):

* `q_network` and `target_network`, which you'll use to step and train the agent.
* `optimizer`, which is used during the training steps.
* `rb`, the replay buffer.
* `agent`, to handle stepping the agent in the environment.

You should fill in the remaining 2 methods:

- `add_to_replay_buffer`
    - This function makes the agent take `n` steps in the environment (and optionally logs any important variables)
    - Note - this is called at the start of training (at initialization) to fill the buffer, and once before each training step in the `train` function.
- `training_step`
    - This function samples experiences from the buffer using `self.rb.sample`, and then performs an update step on the agent
    - A single update step involves:
        - Getting the max of the target network for the next observations,
        - Getting the predicted Q-values from the Q-network,
        - Using these to calculate the TD loss,
        - Perform a gradient step on the TD loss,
        - If `self.agent.step % args.target_network_frequency == 0` then load the weights from the Q-network into the target network,
        - Log any important variables.

To recap, here's the formula for TD loss, with notation more in line with the notation we've been using (and ignoring the "taking mean over minibatch" part):

$$
\begin{aligned}
y &= \begin{cases}r_t & d_t=\text { True } \\ r_t+\gamma  \max _a Q\left(s_{t+1}, a \right) & d_t=\text { False }\end{cases} \\
TD &= y - Q(s_t, a_t) \\
loss &= TD^2
\end{aligned}
$$

Note that we can rewrite the formula for $y$ in the following way, which will be simpler for you to implement (you'll need to cast `done` to float for this):

$$
y =r_t +\gamma \, (1 - d_t) \, \max _a Q\left(s_{t+1}, a \right)
$$

A few tips for this exercise:

* Make sure you evaluate the target network in inference mode, since you don't want it to train.
* You can log variables using `wandb.log({"variable_name": variable_value}, step=steps)`.
* The `agent.play_step()` method returns a list of dictionaries containing data about the current run. If the agent terminated at that step, then the dictionary will contain `{"episode": {"l": episode_length, "r": episode_reward}}`.

<br>

Don't be discouraged if your code takes a while to work - it's normal for debugging RL to take longer than you would expect. Add asserts or your own tests, implement an appropriate probe environment, try anything in the Andy Jones post that sounds promising, and try to notice confusion. Reinforcement Learning is often so tricky as even if the algorithm has bugs, the agent might still learn something useful regardless (albeit maybe not as well), or even if everything is correct, the agent might just fail to learn anything useful (like how DQN failed to do anything on Montezuma's Revenge.)

Since the environment is already know to be one DQN can solve, and we've already provided hyperparameters that work for this environment, hopefully that's isolated a lot of the problems one would usually have with solving real world problems with RL.

```python
class DQNTrainer:

    def __init__(self, args: DQNArgs):
        self.args = args
        self.run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
        self.envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, self.run_name)])
        self.start_time = time.time()
        self.rng = np.random.default_rng(args.seed)

		# Get obs & action shapes (we assume we're dealing with a single discrete action)
        num_actions = self.envs.single_action_space.n
        action_shape = ()
        obs_shape = self.envs.single_observation_space.shape
        num_observations = np.array(obs_shape, dtype=int).prod()

        self.q_network = QNetwork(num_observations, num_actions).to(device)
        self.target_network = QNetwork(num_observations, num_actions).to(device)
        self.target_network.load_state_dict(self.q_network.state_dict())
        self.optimizer = t.optim.Adam(self.q_network.parameters(), lr=args.learning_rate)

        self.rb = ReplayBuffer(len(self.envs.envs), obs_shape, action_shape, args.buffer_size, args.seed)
        self.agent = DQNAgent(self.envs, self.args, self.rb, self.q_network, self.target_network, self.rng)


    def add_to_replay_buffer(self, n: int):
        '''Makes n steps, adding to the replay buffer (and logging any results).'''
        pass


    def training_step(self) -> None:
        '''Samples once from the replay buffer, and takes a single training step.'''
        pass


    def train(self) -> None:

        if self.args.use_wandb: wandb.init(
            project=self.args.wandb_project_name,
            entity=self.args.wandb_entity,
            name=self.run_name,
            monitor_gym=self.args.capture_video
        )
        
        print("Adding to buffer...")
        self.add_to_replay_buffer(self.args.buffer_size)

        progress_bar = tqdm(range(self.args.total_training_steps))
        last_logged_time = time.time()

        for step in progress_bar:

            last_episode_len = self.add_to_replay_buffer(self.args.train_frequency)

            if (last_episode_len is not None) and (time.time() - last_logged_time > 1):
                progress_bar.set_description(f"Step = {self.agent.step}, Episodic return = {last_episode_len}")
                last_logged_time = time.time()

            self.training_step()

        # Environments have to be closed before wandb.finish(), or else we get annoying errors 😠
        self.envs.close()
        if self.args.use_wandb:
            wandb.finish()
```

<details>
<summary>Solution</summary>

```python
class DQNTrainer:

    def __init__(self, args: DQNArgs):
        self.args = args
        self.run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
        self.envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, self.run_name)])
        self.start_time = time.time()
        self.rng = np.random.default_rng(args.seed)

		# Get obs & action shapes (we assume we're dealing with a single discrete action)
        num_actions = self.envs.single_action_space.n
        action_shape = ()
        obs_shape = self.envs.single_observation_space.shape
        num_observations = np.array(obs_shape, dtype=int).prod()

        self.q_network = QNetwork(num_observations, num_actions).to(device)
        self.target_network = QNetwork(num_observations, num_actions).to(device)
        self.target_network.load_state_dict(self.q_network.state_dict())
        self.optimizer = t.optim.Adam(self.q_network.parameters(), lr=args.learning_rate)

        self.rb = ReplayBuffer(len(self.envs.envs), obs_shape, action_shape, args.buffer_size, args.seed)
        self.agent = DQNAgent(self.envs, self.args, self.rb, self.q_network, self.target_network, self.rng)


    def add_to_replay_buffer(self, n: int):
        '''Makes n steps, adding to the replay buffer (and logging any results).'''
        # SOLUTION
        last_episode_len = None
        for step in range(n):
            infos = self.agent.play_step()
            for info in infos:
                if "episode" in info.keys():
                    last_episode_len = info["episode"]["l"]
                    if self.args.use_wandb:
                        wandb.log({"episode_len": last_episode_len}, step=self.agent.step)
        return last_episode_len


    def training_step(self) -> Float[Tensor, ""]:
        '''Samples once from the replay buffer, and takes a single training step.'''
        # SOLUTION
        data = self.rb.sample(self.args.batch_size, device)
        s_t, a_t, r_t_1, d_t_1, s_t_1 = data.observations, data.actions, data.rewards, data.dones, data.next_observations

        with t.inference_mode():
            target_max = self.target_network(s_t_1).max(-1).values
        predicted_q_vals = self.q_network(s_t)[range(self.args.batch_size), a_t.flatten()]

        td_error = r_t_1.flatten() + self.args.gamma * target_max * (1 - d_t_1.float().flatten()) - predicted_q_vals
        loss = td_error.pow(2).mean()
        loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()

        if self.agent.step % self.args.target_network_frequency == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())

        if self.args.use_wandb:
            wandb.log({
                "td_loss": loss,
                "q_values": predicted_q_vals.mean().item(),
                "SPS": int(self.agent.step / (time.time() - self.start_time))
            }, step=self.agent.step)


    def train(self) -> None:

        if self.args.use_wandb: wandb.init(
            project=self.args.wandb_project_name,
            entity=self.args.wandb_entity,
            name=self.run_name,
            monitor_gym=self.args.capture_video
        )

        print("Adding to buffer...")
        self.add_to_replay_buffer(self.args.buffer_size)

        progress_bar = tqdm(range(self.args.total_training_steps))
        last_logged_time = time.time()

        for step in progress_bar:

            last_episode_len = self.add_to_replay_buffer(self.args.train_frequency)

            if (last_episode_len is not None) and (time.time() - last_logged_time > 1):
                progress_bar.set_description(f"Step = {self.agent.step}, Episodic return = {last_episode_len}")
                last_logged_time = time.time()

            self.training_step()

        # Environments have to be closed before wandb.finish(), or else we get annoying errors 😠
        self.envs.close()
        if self.args.use_wandb:
            wandb.finish()
```

</details>

Here's some boilerplate code to run one of your probes (change the `probe_idx` variable to try out different probes):

```python
def test_probe(probe_idx: int):
    '''
    Tests a probe environment by training a network on it & verifying that the value functions are
    in the expected range.
    '''
    # Train our network
    args = DQNArgs(
        env_id=f"Probe{probe_idx}-v0",
        exp_name=f"test-probe-{probe_idx}",
        total_timesteps=3000 if probe_idx <= 2 else 4500,
        learning_rate=0.001,
        buffer_size=500,
        capture_video=False,
        use_wandb=False
    )
    trainer = DQNTrainer(args)
    trainer.train()
    q_network = train(args)

    # Get the correct set of observations, and corresponding values we expect
    obs_for_probes = [[[0.0]], [[-1.0], [+1.0]], [[0.0], [1.0]], [[0.0]], [[0.0], [1.0]]]
    expected_value_for_probes = [[[1.0]], [[-1.0], [+1.0]], [[args.gamma], [1.0]], [[-1.0, 1.0]], [[1.0, -1.0], [-1.0, 1.0]]]
    tolerances = [5e-4, 5e-4, 5e-4, 5e-4, 1e-3]
    obs = t.tensor(obs_for_probes[probe_idx-1]).to(device)

    # Calculate the actual value, and verify it
    value = q_network(obs)
    expected_value = t.tensor(expected_value_for_probes[probe_idx-1]).to(device)
    t.testing.assert_close(value, expected_value, atol=tolerances[probe_idx-1], rtol=0)
    print("\nProbe tests passed!\n\n")


for probe_idx in range(1, 6):
    test_probe(probe_idx)
```

Once you've passed the tests for all 5 probe environments, you should test your model on Cartpole.

You can see the run below [here](https://api.wandb.ai/links/callum-mcdougall/nyi65r5l). You can also click on the dropdown below to read a discussion of why the loss curve behaves the way it does.

<details>
<summary>Expected Behavior of the Loss</summary>

In supervised learning, we want our loss to always be decreasing and it's a bad sign if it's actually increasing, as that's the only metric we care about. In RL, it's the total reward per epsiode that should be (noisily) increasing over time.

Our agent's loss function just reflects how close together the Q-network's estimates are to the experiences currently sampled from the replay buffer, which might not adequately represent what the world actually looks like.

This means that once the agent starts to learn something and do better at the problem, it's expected for the loss to increase. The loss here is just the TD-error, the difference between how valuable the agent thinks the (state-action) is, v.s. the best  current bootstrapped estimate of the actual Q-value.

For example, the Q-network initially learned some state was bad, because an agent that reached them was just flapping around randomly and died shortly after. But now it's getting evidence that the same state is good, now that the agent that reached the state has a better idea what to do next. A higher loss is thus actually a good sign that something is happening (the agent hasn't stagnated), but it's not clear if it's learning anything useful without also checking how the total reward per episode has changed.

</details>

```python
args = DQNArgs(use_wandb=True)
trainer = DQNTrainer(args)
trainer.train()
```

## Beyond CartPole

If things go well and your agent masters CartPole, the next harder challenges are [Acrobot-v1](https://github.com/openai/gym/blob/master/gym/envs/classic_control/acrobot.py), and [MountainCar-v0](https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py). These also have discrete action spaces, which are the only type we're dealing with today. Feel free to Google for appropriate hyperparameters for these other problems - in a real RL problem you would have to do hyperparameter search using the techniques we learned on a previous day because bad hyperparameters in RL often completely fail to learn, even if the algorithm is perfectly correct.

There are many more exciting environments to play in, but generally they're going to require more compute and more optimization than we have time for today. If you finish the main material, some we recommend are:

- [Minimalistic Gridworld Environments](https://github.com/Farama-Foundation/gym-minigrid) - a fast gridworld environment for experiments with sparse rewards and natural language instruction.
- [microRTS](https://github.com/santiontanon/microrts) - a small real-time strategy game suitable for experimentation.
- [Megastep](https://andyljones.com/megastep/) - RL environment that runs fully on the GPU (fast!)
- [Procgen](https://github.com/openai/procgen) - A family of 16 procedurally generated gym environments to measure the ability for an agent to generalize. Optimized to run quickly on the CPU.

## Bonus

### Target Network

Why have the target network? Modify the DQN code above, but this time use the same network for both the target and the Q-value network, rather than updating the target every so often.

Compare the performance of this against using the target network.

### Shrink the Brain

Can DQN still learn to solve CartPole with a Q-network with fewer parameters? Could we get away with three-quarters or even half as many parameters? Try comparing the resulting training curves with a shrunken version of the Q-network. What about the same number of parameters, but with more/less layers, and less/more parameters per layer?

### Dueling DQN

Implement dueling DQN according to [the paper](https://arxiv.org/pdf/1511.06581.pdf) and compare its performance.


""", unsafe_allow_html=True)

