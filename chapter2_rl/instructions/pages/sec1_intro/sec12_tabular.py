import streamlit as st

def section():

    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#rl-known-environments'>RL, Known Environments</a></li>
    <li class='margtop'><a class='contents-el' href='#readings-optional'>Readings (optional)</a></li>
    <li class='margtop'><a class='contents-el' href='#what-is-reinforcement-learning'>What is Reinforcement Learning?</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#trajectories'>Trajectories</a></li>
        <li><a class='contents-el' href='#value-function'>Trajectories</a></li>
        <li><a class='contents-el' href='#bellman-equation'>Trajectories</a></li>
        <li><a class='contents-el' href='#optimal-policies'>Trajectories</a></li>
        <li><a class='contents-el' href='#theory-exercises-3-state-environment'>Theory Exercises: 3-state environment</a></li>
        <li><a class='contents-el' href='#exercise-compute-value-function'><b>Exercise</b> - compute value function (TODO - fix contents)</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#tabular-rl-known-environments'>Tabular RL, Known Environments</a></li>
    <li class='margtop'><a class='contents-el' href='#policy-evaluation'>Policy Evaluation</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-policy-eval-numerical'><b>Exercise</b> - implement <code>policy_eval_numerical</code></a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#exact-policy-evaluation'>Exact Policy Evaluation</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-policy-eval-exact'><b>Exercise</b> - implement <code>policy_eval_exact</code></a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#policy-improvement'>Policy Improvement</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-policy-improvement'><b>Exercise</b> - implement <code>policy_improvement</code></a></li>
        <li><a class='contents-el' href='#exercise-implement-find-optimal-policy'><b>Exercise</b> - implement <code>find_optimal_policy</code></a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#bonus'>Bonus</a></li>
</ul>""", unsafe_allow_html=True)

    st.markdown(
r"""

# Tabular RL & Policy Improvement

> ### Learning Objectives
>
> * Understand the tabular RL problem, for known environments.
> * Learn how to numerically evaluate a given policy (via an iterative update formula).
> * Understand the policy improvement theorem, and understand what circumstances can allow us to directly solve for the optimal policy.

## RL, Known Environments

We are presented with a environment, and the transition function that indicates how the environment will transition from state to state based on the action chosen. We will see how we can turn the problem of finding the best agent into an optimization problem, which we can then solve iteratively.

Here, we assume environments small enough where visiting all pairs of states and actions is tractable. These types of models don't learn any relationships between states that can be treated similarly, but keep track of an estimate
of how valuable each state is in a large lookup table.

## Readings (optional)

There are no compulsory readings before the material in this section, although the following sections of [Sutton and Barto](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf) are relevant and so you may want to refer to them if you get stuck / confused with anything:

- Chapter 3, Sections 3.1, 3.2, 3.3, 3.4, 3.5, 3.6
- Chapter 4, Sections 4.1, 4.2, 4.3, 4.4

## What is Reinforcement Learning?

In reinforcement learning, the agent interacts with an environment in a loop: The agent in state $s$ issues an **action** $a$ to the environment, and the environment replies with **state, reward** pairs $(s',r)$. We will assume the environment is **Markovian**, in that the next state $s'$ and reward $r$ depend solely on the current state $s$ and the action $a$ chosen by the agent (as opposed to environments which may depend on actions taken far in the past.)

Note - some of the sections below have a TLDR at the start. This indicates that not the entire section is crucially important to read and understand (e.g. it's a small pedantic note or a mathematical divergence), and you can safely read the TLDR and move onto the next section if you want.

### Trajectories

We define a trajectory in RL as a sequence of states and actions:

$$
s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, r_3, \ldots
$$

With this notation, it's the environment action that causes the timestep transition. In other words, given state $s_t$ the agent returns action $a_t$ (in the same time step), but given $(s_t, a_t)$ the environment generates $(s_{t+1}, r_{t+1})$.

> *Note - some authors use the convention that the agent's action causes the timestep transition, i.e. the trajectory looks like $s_0, a_0, r_0, s_1, a_1, r_1, \ldots$. We're following the convention in Sutton & Barto here, but it's important to be aware of this possible notation difference when reading other sources.*

The agent chooses actions using a policy $\pi$, which we can think of as either a deterministic function $a = \pi(s)$ from states to actions, or more generally a stochastic function from which actions are sampled, $a \sim \pi(\cdot | s)$.

Implicitly, we have assumed that the agent need only be Markovian as well (i.e. the action depends on the current state, not past states). Do you think this this a reasonable assumption? You should think about this before reading the dropdown.

<details>
<summary>Answer / discussion</summary>

In most environments, this may well be a reasonable assumption. For example, in a game of tic-tac-toe, knowledge of the current state of the board is sufficient to determine the optimal move, how the board got to that state is irrelevant.

There are more complex environments where a Markovian assumption wouldn't work. One classic example is a game of poker, since past player behaviour (betting patterns, bluffing, etc) might change what the optimal strategy is at a given point in time, even if the resulting game state is the same.

Of course, a non-Markovian game can be turned Markovian by redefining the state space to include relevant historical information, but this is splitting hairs.

</details>


The environment samples (state, reward) pairs from a probability distribution conditioned on the current state $s$ and the action $a$ the policy chose in that state, $(s', r) \sim p(\cdot | s, a)$. In the case where the environment is also deterministic, we write $(s', r) = p(s, a)$.

### Value function

The goal of the agent is to choose a policy that maximizes the **expected discounted return**, the sum of rewards it would expect to obtain by following it's currently chosen **policy** $\pi$. We call the expected discounted return from a state $s$ following policy $\pi$ the **state value function** $V_{\pi}(s)$, or simply **value function**, as
$$
V_{\pi}(s) = \mathbb{E}_{\pi} \left[ \sum_{i=t}^\infty \gamma^{i-t} r_{i+1} \Bigg| s_t = s \right]
$$
where the expectation is with respect to sampling actions from $\pi$, and (implicitly) sampling states and rewards from $p$.

> *Note - technically $V_\pi$ is also a function of the choice of environment $p$ and discount factor $\gamma$. We write $V_\pi$ rather than $V_{\pi, p, \gamma}$ simply because during training we're usually just optimizing for $\pi$.

Here are a few divergences into the exact nature of the reward function. You can just read the TLDRs and move on if you're not super interested in the deeper mathematics.

#### Divergence #1: why do we discount?

***TLDR: not discounting would lead to a bunch of problems when we try to sum an infinite sequence of rewards, since that sum could be unbounded (or more generally non-converging).***

We would like a way to signal to the agent that reward now is better than reward later.
If we didn't discount, the sum $\sum_{i=t}^\infty r_i$ may diverge.
This leads to strange behavior, as we can't meaningfully compare the returns for sequences of rewards that diverge. Trying to sum the sequence $1,1,1,\ldots$ or $2,2,2,2\ldots$ or even $2, -1, 2 , -1, \ldots$ is a problem since they all diverge to positive infinity. An agent with a policy that leads to infinite expected return might become lazy, as waiting around doing nothing for a thousand years, and then actually optimally leads to the same return (infinite) as playing optimal from the first time step.

Worse still, we can have reward sequences for which the sum may never approach any finite value, nor diverge to $\pm \infty$ (like summing $-1, 1, -1 ,1 , \ldots$).
We could otherwise patch this by requiring that the agent has a finite number of interactions with the environment (this is often true, and is called an **episodic** environment that we will see later) or restrict ourselves to environments for which the expected return for the optimal policy is finite, but these can often be undesirable constraints to place.

#### Divergence #2: why do we discount geometrically?

***TLDR: geometric discounting is "time consistent", since the nature of the problem looks the same from each timestep.***

In general, we could consider a more general discount function $\Gamma : \mathbb{N} \to [0,1)$ and define the discounted return as $\sum_{i=t}^\infty \Gamma(i) r_i$. The geometric discount $\Gamma(i) = \gamma^i$ is commonly used, but other discounts include the hyperbolic discount $\Gamma(i) = \frac{1}{1 + iD}$, where $D>0$ is a hyperparameter. (Humans are [often said](https://chris-said.io/2018/02/04/hyperbolic-discounting/) to act as if they use a hyperbolic discount.) Other than  being mathematically convenient to work with (as the sum of geometric discounts has an elegant closed form expression $\sum_{i=0}^\infty \gamma^i  = \frac{1}{1-\gamma}$), geometric is preferred as it is *time consistent*, that is, the discount from one time step to the next remains constant.
Rewards at timestep $t+1$ are always worth a factor of $\gamma$ less than rewards on timestep $t$

$$
\frac{\Gamma(t+1)}{\Gamma(t)} = \frac{\gamma^{t+1}}{\gamma^t} = \gamma
$$

whereas for the hyperbolic reward, the amount discounted from one step to the next
is a function of the timestep itself, and decays to 1 (no discount)

$$
\frac{\Gamma(t+1)}{\Gamma(t)} =  \frac{1+tD}{1+(t+1)D} \to 1
$$

so very little discounting is done once rewards are far away enough in the future.

Hence, using geometric discount lends a certain symmetry to the problem. If after $N$ actions the agent ends up in exactly the same state as it was in before, then there will be less total value remaining (because of the decay rate), but the optimal policy won't change (because the rewards the agent gets if it takes a series of actions will just be a scaled-down version of the rewards it would have gotten for those actions $N$ steps ago).

### Bellman equation

> Note we can write the value function in the following recursive manner:
>
> $$
> V_\pi(s) = \sum_a \pi(a | s) \sum_{s', r} p(s',r \mid s, a) \left( r + \gamma V_\pi(s') \right)
> $$
>
> (Optional) Try to prove this for yourself!
>
> <details>
> <summary>Hint</summary>
>
> Start by writing the expectation as a sum over all possible actions $a$ that can be taken from state $s$. Then, try to separate $r_{t+1}$ and the later reward terms inside the sum.
> </details>
>
> <details>
> <summary>Answer</summary>
>
> First, we write it out as a sum over possible next actions, using the **policy function** $\pi$:
> $$
> \begin{aligned}
> V_\pi(s) &=\mathbb{E}_\pi\left[\sum_{i=t}^{\infty} \gamma^{i-t} r_{i+1} \mid s_t=s\right] \\
> &=\sum_a \pi(a \mid s) \mathbb{E}_\pi\left[\sum_{i=t}^{\infty} \gamma^{i-t} r_{i+1} \mid s_t=s, a_t=a\right]
> \end{aligned}
> $$
> In other words, this is an average of the possible expected reward streams after a particular action, weighted by the probability that this action is chosen.
>
> We can then separate out the term for the rewward at step $t+1$:
> $$
> =\sum_a \pi(a \mid s) \mathbb{E}_\pi\left[r_{t+1}+\sum_{i=t+1}^{\infty} \gamma^{i-t} r_{i+1} \mid s_t=s, a_t=a\right]
> $$
> And then we expand the sum over all possible state-reward pairs $(s', r)$ which we might evolve to, following state-action pair $(s, a)$:
> $$
> \begin{aligned}
> &=\sum_{a, s^{\prime}, r} \pi(a \mid s) p\left(s^{\prime}, r \mid s, a\right)\left(r+\gamma \cdot \mathbb> {E}_\pi\left[\sum_{i=t+1}^{\infty} \gamma^{i-(t+1)} r_{i+1} \mid s_{t+1}=s^{\prime}\right]\right) \\
> \end{aligned}
> $$
> Finally, we can notice that the sum term looks a lot like our original expression for $V_{\pi}(s)$, just starting from $s'$ rather than $s$. So we can write this as:
> $$
> \begin{aligned}
> &=\sum_{a, s, r} \pi(a \mid s) p\left(s^{\prime}, r \mid s, a\right)\left(r+\gamma V_\pi\left(s^{\prime}\right)\right) \\
> \end{aligned}
> $$
> And finally, we rearrange the sum terms:
> $$
> \begin{aligned}
> &=\sum_a \pi(a \mid s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left(r+\gamma V_\pi\left(s^{\prime}\right)\right)
> \end{aligned}
> $$
> as required.
>
> How should we interpret this? This formula tells us that the total value from present time can be written as sum of next-timestep rewards and value terms discounted by a factor of $\gamma$. Recall earlier in our discussion of **geometric** vs **hyperbolic** discounting, we argued that geometric discounting has a symmetry through time, because of the constant discount factor. This is exactly what this formula shows, just on the scale of a single step.
>
> </details>

This recursive formulation of the value function is called the **Bellman equation**, and can be thought of as <i>"(value of following policy $\pi$ at current state) = (value of next reward, which is determined from following $\pi$) + (discounted value at next state if we continue following $\pi$)"</i>.

We can also define the **action-value function**, or **Q-value** of state $s$ and action $a$ following policy $\pi$:

$$
Q_\pi(s,a) = \mathbb{E}\left[ \sum_{i=t}^\infty \gamma^{i-t}r_{t+1} \Bigg| s_t=s, a_t=a   \right]
$$

which can be written recursively much like the value function can:

$$
Q_\pi(s,a) = \sum_{s',r} p(s',r \mid s,a) \left( r + \gamma \sum_{a'} \pi(a' \mid s') Q_\pi(s', a') \right)
$$

This equation can be thought of as <i>"(value of choosing particular action $a$ at current state, then following $\pi$) = (value of next reward, which is determined from action $a$) + (discounted value at next state if we continue following $\pi$)"</i>. In other words it's conceptually the same equation as before but rewritten in terms of $Q$, conditionining on what the next action is before we go back to following $\pi$.

<details>
<summary>Question - what do you think is the formula relating V and Q to each other?</summary>

$V_\pi(s)$ is the value at a given state according to policy $\pi$, but this must be the average of all the values of taking action $a$ at this state, weighted by the probability that they're taken (under $\pi$). So we have:

$$
V_\pi(s) = \sum_a \pi(a \mid s) Q_\pi(s, a)
$$

Note, this also gives a slightly different form of the Q-value Bellman equation, which will be useful for policy iteration later on:

$$
Q_\pi(s,a) = \sum_{s',r} p(s',r \mid s,a) \left( r + \gamma V_\pi(s')\right)
$$

</details>

### Optimal policies

We say that two policies $\pi_1$ and $\pi_2$ are **equivalent** if $\forall s \in S. V_{\pi_1}(s) = V_{\pi_2}(s)$. A policy $\pi_1$ is **better** than $\pi_2$ (denoted $\pi_1 \geq \pi_2$) if
$\forall s \in S. V_{\pi_1}(s) \geq V_{\pi_2}(s)$.

An **optimal** policy (denoted $\pi^*$) is a policy that is better than all other policies. There may be more than one optimal policy, so we refer to any of them as $\pi^*$, with the understanding that since all optimal policies have the same value $V_{\pi^*}$ for all states, it doesn't actually matter which is chosen.

It is possible to prove that, for any environment, an optimal policy exists, but we won't go into detail on this proof today.

### Theory Exercises: 3-state environment

(These are mostly to check your understanding of the readings on RL.
Feel free to skip ahead if you're already happy with the definition.)

Consider the following environment: There are two actions $A = \{a_L, a_R\}$, three states $S = \{s_0, s_L, s_R\}$ and three rewards $R = \{0,1,2\}$. The environment is deterministic, and can be represented by the following transition diagram:

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/markov-diagram.png" width="400">

The edges represent the state transitions given an action, as well as the reward received. For example, in state $s_0$, taking action $a_L$ means the new state is $s_L$, and the reward received is $+1$. (The transitions for $s_L$ and $s_R$ are independent of action taken.)

This gives us effectively two choices of deterministic policies, $\pi_L(s_0) = s_L$ and $\pi_R(s_0) = s_R$. (It is irrelevant what those policies do in the other states.)

### Exercise - compute the value $V_{\pi}(s_0)$ for $\pi = \pi_L$ and $\pi = \pi_R$.

```c
Difficulty: 🔴🔴🔴⚪⚪
Importance: 🔵🔵🔵⚪⚪

You should spend up to 15-25 minutes on this exercise.

This is a good exercise to build your intuition for these formulas and expressions.
```

Which policy is better? Does the answer depend on the choice of discount factor $\gamma$? If so, how?

<details>
<summary>Answer</summary>

Following the first policy, this gives
$$
V_{\pi_L}(s_0) = 1 + \gamma V_{\pi_L}(s_L) = 1 + \gamma(0 + \gamma V_{\pi_L}(s_0)) = 1 +\gamma^2 V_{\pi_L}(s_0)
$$
Rearranging, this gives
$$
V_{\pi_L}(s_0) = \frac{1}{1-\gamma^2}
$$
Following the second policy, this gives
$$
V_{\pi_R}(s_0) = 0 + \gamma V_{\pi_R}(s_R) = \gamma(2 + \gamma V_{\pi_R}(s_0)) = 2 \gamma + \gamma^2 V_{\pi_R}(s_0)
$$
Rearranging, this gives
$$
V_{\pi_R}(s_0) = \frac{2\gamma}{1-\gamma^2}
$$
Therefore,
$$
\pi^* = \begin{cases}
\pi_L & \gamma < 1/2 \\
\pi_L \text{ or } \pi_R & \gamma = 1/2 \\
\pi_R & \gamma > 1/2
\end{cases}
$$
which makes sense, an agent that discounts heavily ($\gamma < 1/2$) is shortsighted,
and will choose the reward 1 now, over the reward 2 later.
</details>

## Tabular RL, Known Environments

For the moment, we focus on environments for which the agent has access to $p$, the function describing the underlying dynamics of the environment, which will allow us to solve the Bellman equation explicitly. While unrealistic, it means we can explicitly solve the Bellman equation. Later on we will remove this assumption and treat the environment as a black box from which the agent can sample from.

We will simplify things a bit further, and assume that the environment samples states from a probability distribution $T(\cdot | s, a)$ conditioned on the current state $s$ and the action $a$ the policy $\pi$ chose in that state.

Normally, the reward is also considered to be a stochastic function of both state and action $r \sim  R(\cdot \mid s,a)$, but we will assume the reward is a deterministic function $R(s,a,s')$ of the current state, action and next state, and offload the randomness in the rewards to the next state $s'$ sampled from $T$.

This (together with assuming $\pi$ is deterministic) gives a simpler recursive form of the value function

$$
V_\pi(s) = \sum_{s'} T(s' \mid s, a) \Big( R(s,a,s') + \gamma V_\pi (s') \Big)
\text{ where } a = \pi(s)
$$

and in $Q$-value form:

$$
\begin{aligned}
Q_\pi(s,a) &= \sum_{s'} T(s' \mid s,a) \left( R(s, a, s') + \gamma \sum_{a'} \pi(a' \mid s') Q_\pi (s', a') \right) \\ &= \sum_{s'} T(s' \mid s,a) \left( R(s, a, s') + \gamma V_\pi(s') \right)
\end{aligned}
$$

Below, we've provided a simple environment for a gridworld taken from [Russell and Norvig](http://aima.cs.berkeley.edu/). The agent can choose one of four actions: `up` (0), `right` (1), `down` (2) and `left` (3), encoded as numbers. The observation is just the state that the agent is in (encoded as a number from 0 to 11).

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gridworld.png" width="300">

The result of choosing an action from an empty cell will move the agent in that direction, unless they would bump into a wall or walk outside the grid, in which case the next state is unchanged. Both the terminal states "trap" the agent, and any movement from one of the terminal states leaves the agent in the same state, and no reward is received. The agent receives a small penalty for each move $r = -0.04$ to encourage them to move to a terminal state quickly, unless the agent is moving into a terminal state, in which case they recieve reward either $+1$ or $-1$ as appropriate.

Lastly, the environment is slippery, and the agent only has a 70\% chance of moving in the direction chosen, with a 10% chance each of attempting to move in the other three cardinal directions.

<details>
<summary>There are two possible routes to +1, starting from the cell immediately to the <b>right of START</b>. Which one do you think the agent will prefer? How do you think changing the penalty value r=-0.04 to something else might affect this?
</summary>

The two routes are moving clockwise and anticlockwise around the center obstacle.

The clockwise route requires more steps, which (assuming $r<0$) will be penalised more.

But the anticlockwise route has a different disadvantage. The agent has a 70% probability of slipping, and the anticlockwise route brings them into closer proximity with the $-1$ terminal state.

We can guess that there is a certain threshold value $r^* < 0$ such that the optimal policy is clockwise when $r<r^*$ (because movement penalty is large, making the anticlockwise route less attractive), and anticlockwise when $r>r^*$.

You can test this out (and find the approximate value of $r^*$) when you run your RL agent!
</details>

Provided is a class that allows us to define environments with known dynamics. The only parts you should be concerned
with are

* `.num_states`, which gives the number of states,
* `.num_actions`, which gives the number of actions
* `.T`, a 3-tensor of shape  `(num_states, num_actions, num_states)` representing the probability $T(s_{next} \mid s, a)$ = `T[s,a,s_next]`
* `.R`, the reward function, encoded as a vector of shape `(num_states, num_actions, num_states)` that returns the reward $R(s,a,s_{next})$ = `R[s,a,s_next]` associated with entering state $s_{next}$ from state $s$ by taking action $a$.

This environment also provides two additional parameters which we will not use now, but need for part 3 where the environment is treated as a black box, and agents learn from interaction.
* `.start`, the state with which interaction with the environment begins. By default, assumed to be state zero.
* `.terminal`, a list of all the terminal states (with which interaction with the environment ends). By default, terminal states are empty.

```python
class Environment:
    def __init__(self, num_states: int, num_actions: int, start=0, terminal=None):
        self.num_states = num_states
        self.num_actions = num_actions
        self.start = start
        self.terminal = np.array([], dtype=int) if terminal is None else terminal
        (self.T, self.R) = self.build()

    def build(self):
        '''
        Constructs the T and R tensors from the dynamics of the environment.

        Returns:
            T : (num_states, num_actions, num_states) State transition probabilities
            R : (num_states, num_actions, num_states) Reward function
        '''
        num_states = self.num_states
        num_actions = self.num_actions
        T = np.zeros((num_states, num_actions, num_states))
        R = np.zeros((num_states, num_actions, num_states))
        for s in range(num_states):
            for a in range(num_actions):
                (states, rewards, probs) = self.dynamics(s, a)
                (all_s, all_r, all_p) = self.out_pad(states, rewards, probs)
                T[s, a, all_s] = all_p
                R[s, a, all_s] = all_r
        return (T, R)

    def dynamics(self, state: int, action: int) -> Tuple[Arr, Arr, Arr]:
        '''
        Computes the distribution over possible outcomes for a given state
        and action.

        Args:
            state  : int (index of state)
            action : int (index of action)

        Returns:
            states  : (m,) all the possible next states
            rewards : (m,) rewards for each next state transition
            probs   : (m,) likelihood of each state-reward pair
        '''
        raise NotImplementedError()

    def render(pi: Arr):
        '''
        Takes a policy pi, and draws an image of the behavior of that policy, if applicable.

        Args:
            pi : (num_actions,) a policy

        Returns:
            None
        '''
        raise NotImplementedError()

    def out_pad(self, states: Arr, rewards: Arr, probs: Arr):
        '''
        Args:
            states  : (m,) all the possible next states
            rewards : (m,) rewards for each next state transition
            probs   : (m,) likelihood of each state-reward pair

        Returns:
            states  : (num_states,) all the next states
            rewards : (num_states,) rewards for each next state transition
            probs   : (num_states,) likelihood of each state-reward pair (including zero-prob outcomes.)
        '''
        out_s = np.arange(self.num_states)
        out_r = np.zeros(self.num_states)
        out_p = np.zeros(self.num_states)
        for i in range(len(states)):
            idx = states[i]
            out_r[idx] += rewards[i]
            out_p[idx] += probs[i]
        return (out_s, out_r, out_p)
```

For example, here is the toy environment from the theory exercises earlier (the **3-state environment**) implemented in this format.

*Note - don't spend too much time reading this code and the plots generated below; this is just designed to illustrate how we can implement a particular environment on top of our class above by defining certain environment dynamics. The environment you'll actually be working with is the gridworld environment described above.*

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/markov-diagram.png" width="400">

```python
class Toy(Environment):
    def dynamics(self, state: int, action: int):
        '''
        Sets up dynamics for the toy environment:
            - In state s_L, we move right & get +0 reward regardless of action
            - In state s_R, we move left & get +2 reward regardless of action
            - In state s_0, we can move left & get +1, or right & get +0
        '''
        (SL, S0, SR) = (0, 1, 2)
        LEFT = 0
        num_states = 3
        num_actions = 2
        assert 0 <= state < self.num_states and 0 <= action < self.num_actions
        if state == S0:
            if action == LEFT:
                (next_state, reward) = (SL, 1)
            else:
                (next_state, reward) = (SR, 0)
        elif state == SL:
            (next_state, reward) = (S0, 0)
        elif state == SR:
            (next_state, reward) = (S0, 2)
        return (np.array([next_state]), np.array([reward]), np.array([1]))

    def __init__(self):
        super().__init__(num_states=3, num_actions=2)
```

Given a definition for the `dynamics` function, the `Environment` class automatically generates `T` and `R` for us.

The code below plots transition probabilities & rewards. Intuitively, the plot of transition probabilities shows us (for each possible current state $s_L$) what are the transition probabilities to the next state $s_{next}$ for each possible action we could take, and the reward plot shows us the corresponding rewards.

```python
toy = Toy()

actions = ["a_L", "a_R"]
states = ["s_L", "S_0", "S_R"]

imshow(
    toy.T, # dimensions (s, a, s_next)
    title="Transition probabilities T(s_next | s, a) for toy environment",
    facet_col=0, facet_labels=[f"Current state is s = {s}" for s in states], y=actions, x=states,
    labels = {"x": "Next state (s_next)", "y": "Action at current state (a)", "color": "Transition<br>Probability"},
    text_auto = ".2f", border=True, width=1200, height=450,
)

imshow(
    toy.R, # dimensions (s, a, s_next)
    title="Rewards R(s, a, s_next) for toy environment",
    facet_col=0, facet_labels=[f"Current state is s = {s}" for s in states], y=actions, x=states,
    labels = {"x": "Next state (s_next)", "y": "Action at current state (a)", "color": "Reward"},
    text_auto = ".2f", border=True, width=1200, height=450,
)
```

Output produced by this:

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rl-results.png" width="900">#

A few notes to explain these plots:

* Because the environment is deterministic, for a given state $s$ and action $a$ the transition probability is always 1 (it's certain what the next state $s_{next}$ will be).
* Some of the values don't make sense, for instance if we're in state $s_L$ then action $a_L$ is not in the possible action space. The interpretation of this plot is "if we're in state $s_L$, then whatever action we choose, we're moving right regardless" (and vice-versa for being in state $s_R$).
* We still have $R(s, a, s_{next})$ defined for illegal states (e.g. ones where $s_{next} = s$). But this is fine, because these rewards will never appear in our formula, since their transition probabilities are zero.

We also provide an implementation of the gridworld environment above.

The `dynamics` function sets up the dynamics of the Gridworld environment: if the agent takes an action, there is a 70% chance the action is successful, and 10% chance the agent moves in one of the other 3 directions randomly. The rest of the code here deals with edge cases like hitting a wall or being in a terminal state. It's not important to fully read through & understand how this method works.

We include a definition of `render`, which given a policy, prints out a grid showing the direction the policy will try to move in from each cell.

```python
class Norvig(Environment):
    def dynamics(self, state: int, action: int) -> Tuple[Arr, Arr, Arr]:
        def state_index(state):
            assert 0 <= state[0] < self.width and 0 <= state[1] < self.height, print(state)
            pos = state[0] + state[1] * self.width
            assert 0 <= pos < self.num_states, print(state, pos)
            return pos

        pos = self.states[state]
        move = self.actions[action]
        if state in self.terminal or state in self.walls:
            return (np.array([state]), np.array([0]), np.array([1]))
        out_probs = np.zeros(self.num_actions) + 0.1
        out_probs[action] = 0.7
        out_states = np.zeros(self.num_actions, dtype=int) + self.num_actions
        out_rewards = np.zeros(self.num_actions) + self.penalty
        new_states = [pos + x for x in self.actions]
        for (i, s_new) in enumerate(new_states):
            if not (0 <= s_new[0] < self.width and 0 <= s_new[1] < self.height):
                out_states[i] = state
                continue
            new_state = state_index(s_new)
            if new_state in self.walls:
                out_states[i] = state
            else:
                out_states[i] = new_state
            for idx in range(len(self.terminal)):
                if new_state == self.terminal[idx]:
                    out_rewards[i] = self.goal_rewards[idx]
        return (out_states, out_rewards, out_probs)

    def render(self, pi: Arr):
        assert len(pi) == self.num_states
        emoji = ["⬆️", "➡️", "⬇️", "⬅️"]
        grid = [emoji[act] for act in pi]
        grid[3] = "🟩"
        grid[7] = "🟥"
        grid[5] = "⬛"
        print("".join(grid[0:4]) + "\n" + "".join(grid[4:8]) + "\n" + "".join(grid[8:]))

    def __init__(self, penalty=-0.04):
        self.height = 3
        self.width = 4
        self.penalty = penalty
        num_states = self.height * self.width
        num_actions = 4
        self.states = np.array([[x, y] for y in range(self.height) for x in range(self.width)])
        self.actions = np.array([[0, -1], [1, 0], [0, 1], [-1, 0]])
        self.dim = (self.height, self.width)
        terminal = np.array([3, 7], dtype=int)
        self.walls = np.array([5], dtype=int)
        self.goal_rewards = np.array([1.0, -1])
        super().__init__(num_states, num_actions, start=8, terminal=terminal)


# Example use of `render`: print out a random policy
norvig = Norvig()
pi_random = np.random.randint(0, 4, (12,))
norvig.render(pi_random)
```

The output produced by this code shows the direction of the preferred policy from each cell (which is just a random policy in this case):

⬇️⬇️⬆️🟩<br>
⬇️⬛⬆️🟥<br>
⬅️⬇️➡️➡️

## Policy Evaluation

At the moment, we would like to determine the value function $V_\pi$ of some policy $\pi$. **We will assume policies are deterministic**, and encode policies as a lookup table from states to actions (so $\pi$ will be a vector of shape `(num_states,)`, where each element is an integer `a` in the range `0 <= a < num_actions`, representing one of the possible actions to choose for that state.)

Firstly, we will use the Bellman equation as an update rule: Given a current estimate $\hat{V}_\pi$ of the value function $V_{\pi}$, we can obtain a better estimate by using the Bellman equation, sweeping over all states.
$$
\forall s. \hat{V}_\pi(s) \leftarrow \sum_{s'} T(s' \,|\, s, a) \left( R(s,a,s') + \gamma \hat{V}_\pi(s') \right) \;\text{ where } a = \pi(s)
$$
Recall that the true value function $V_\pi$ satisfies this as an equality, i.e. it is a fixed point of the iteration.

(Also, remember how we defined our objects: `T[s, a, s']` $= T(s' \,|\, a, s)$, and `R[s, a, s']` $= R(s, a, s')$.)

We continue looping this update rule until the result stabilizes: $\max_s |\hat{V}^{new}(s) - \hat{V}^{old}(s)| < \epsilon$ for some small $\epsilon > 0$. Use $\hat{V}_\pi(s) = 0$ as your initial guess.

### Exercise - implement `policy_eval_numerical`

```c
Difficulty: 🔴🔴🔴⚪⚪
Importance: 🔵🔵🔵⚪⚪

You should spend up to 15-30 minutes on this exercise.
```

The `policy_eval_numerical` function takes in a deterministic policy $\pi$, and transition and reward matrices $T$ and $R$ (stored as `env.T` and `env.R`), and computes the value function $V_\pi$ using the Bellman equation as an update rule. It keeps iterating until it reaches `max_iterations`, or the result stabilizes - which ever comes first.

```python
def policy_eval_numerical(env: Environment, pi: Arr, gamma=0.99, eps=1e-8, max_iterations=10_000) -> Arr:
    '''
    Numerically evaluates the value of a given policy by iterating the Bellman equation
    Args:
        env: Environment
        pi : shape (num_states,) - The policy to evaluate
        gamma: float - Discount factor
        eps  : float - Tolerance
        max_iterations: int - Maximum number of iterations to run
    Outputs:
        value : float (num_states,) - The value function for policy pi
    '''
    # YOUR CODE HERE
    pass


tests.test_policy_eval(policy_eval_numerical, exact=False)
```

<details>
<summary>Solution</summary>

```python
def policy_eval_numerical(env: Environment, pi: Arr, gamma=0.99, eps=1e-08, max_iterations=10_000) -> Arr:
    '''
    Numerically evaluates the value of a given policy by iterating the Bellman equation
    Inputs:
        env: Environment
        pi : shape (num_states,) - The policy to evaluate
        gamma: float - Discount factor
        eps  : float - Tolerance
    Outputs:
        value : float (num_states,) - The value function for policy pi
    '''
    # Indexing T into an array of shape (num_states, num_states)
    states = np.arange(env.num_states)
    actions = pi
    transition_matrix = env.T[states, actions, :]
    # Same thing with R
    reward_matrix = env.R[states, actions, :]
    
    # Iterate until we get convergence
    V = np.zeros_like(pi)
    for i in range(max_iterations):
        V_new = einops.einsum(transition_matrix, reward_matrix + gamma * V, "s s_prime, s s_prime -> s")
        if np.abs(V - V_new).max() < eps:
            print(f"Converged in {i} steps.")
            return V_new
        V = V_new
    print(f"Failed to converge in {max_iterations} steps.")
    return V
```

</details>

## Exact Policy Evaluation

Essentially what we are doing in the previous step is numerically solving the Bellman equation. Since the Bellman equation essentially gives us a set of simultaneous equations, we can solve it explicitly rather than iterating the Bellman update rule.

> *Note - it's not absolutely essential to closely follow & intuitively understand all of this notation. The main things are (1) understand what the objects in the final formula given at the end represent (i.e. $\mathbf{v}$, $P^\pi$, and $\mathbf{r}^\pi$), and (2) understand why we can deterministically solve for the value of a policy in this particular case, when we might not be able to in general.*

<details>
<summary>Question - can you explain why we can solve for the value of a given policy directly in this case, but we might not be able to in others?</summary>

The key conditions allowing us to solve directly are:

* Finite state space & action space
* Transition probabilities & rewards are known

These will not hold in many cases we'll look at later - especially the second one, because our agent will only have partial observability of its environment.

</details>

Given a policy $\pi$, consider $\mathbf{v} \in \mathbb{R}^{|S|}$ to be a vector representing the value function $V_\pi$ for each state.
$$
\mathbf{v} = [V_\pi(s_1), \ldots, V_\pi(s_{N})]
$$
and recall the Bellman equation:
$$
\mathbf{v}_i = \sum_{s'} T(s' \mid s_i,\pi(s)) \left( R(s,\pi(s),s') + \gamma V_\pi(s') \right)
$$
We can define two matrices $P^\pi$ and $R^\pi$, both of shape `(num_states, num_states)`
as
$$
P^\pi_{i,j} = T(j \,|\, \pi(i), i) \quad R^\pi_{i,j} = R(i, \pi(i), j)
$$
$P^\pi$ can be thought of as a probability transition matrix from current state to next state, and $R^\pi$ is the reward function given current state and next state, assuming actions are chosen by $\pi$.
$$
\mathbf{v}_i = \sum_{j} P^\pi_{i,j} \left( R^\pi_{i,j} + \gamma \mathbf{v}_j \right)
$$
$$
\mathbf{v}_i = \sum_{j} P^\pi_{i,j}  R^\pi_{i,j} +  \gamma \sum_{j} P^\pi_{i,j} \mathbf{v}_j
$$
We can define $\mathbf{r}^\pi_i = \sum_{j} P^\pi_{i,j}  (R^\pi)_{i,j} = (P^\pi  (R^\pi)^T)_{ii}$
$$
\mathbf{v}_i = \mathbf{r}_i^\pi + \gamma (P^\pi \mathbf{v})_i
$$
A little matrix algebra, and we obtain:
$$
\mathbf{v} = (I - \gamma P^\pi)^{-1} \mathbf{r}^\pi
$$
which gives us a closed form solution for the value function $\mathbf{v}$.

Question (optional) - is the inverse $(I - \gamma P^\pi)^{-1}$ guaranteed to exist?

<details>
<summary>Answer</summary>

Yes, for linear algebra-based reasons.

</details>

### Exercise - implement `policy_eval_exact`

```c
Difficulty: 🔴🔴🔴⚪⚪
Importance: 🔵🔵🔵⚪⚪

You should spend up to 10-30 minutes on this exercise.
```

<details>
<summary>Help - I'm not sure where to start.</summary>

You can define the matrix $P^\pi$ indexing `env.T` with the states and corresponding actions `pi` (i.e. you have a 2D array whose elements are $T(s' \mid s, \pi(s))$ for all $s, s'$). Same for defining $R^\pi$ from the reward matrix.

</details>

```python
def policy_eval_exact(env: Environment, pi: Arr, gamma=0.99) -> Arr:
    '''
    Finds the exact solution to the Bellman equation.
    '''
    # YOUR CODE HERE
    pass


tests.test_policy_eval(policy_eval_exact, exact=True)
```

<details>
<summary>Solution</summary>

```python
def policy_eval_exact(env: Environment, pi: Arr, gamma=0.99) -> Arr:
    '''
    Finds the exact solution to the Bellman equation.
    '''
    # SOLUTION
    states = np.arange(env.num_states)
    actions = pi
    transition_matrix = env.T[states, actions, :]
    reward_matrix = env.R[states, actions, :]

    r = einops.einsum(transition_matrix, reward_matrix, "s s_next, s s_next -> s")

    mat = np.eye(env.num_states) - gamma * transition_matrix

    return np.linalg.solve(mat, r)
```

</details>

## Policy Improvement

So, we now have a way to compute the value of a policy. What we are really interested in is finding better policies. One way we can do this is to compare how well $\pi$ performs on a state versus the value obtained by choosing another action instead, that is, the Q-value. If there is an action $a'$ for which $Q_{\pi}(s,a') > V_\pi(s) \equiv Q_\pi(s, \pi(s))$, then we would prefer that $\pi$ take action $a'$ in state $s$ rather than whatever action $\pi(s)$ currently is. Recall that we could write out our Bellman equation like this:

$$
Q_\pi(s,a) = \sum_{s'} T(s' \mid s,a) \left( R(s, a, s') + \gamma V_\pi(s') \right)
$$

which gives us an update rule for our policy $\pi$, given that we've evaluated $V_\pi$ using our closed-form expression from earlier:

1. For each possible state $s$ and action $a$, calculate the thing on the right hand side of this expression
2. Take argmax over $a$, and this gives us the new action that we should take in state $s$.

Or in other words:

$$
\pi^\text{better}(s) = \text{argmax}_a \sum_{s'} T(s' \mid s, a) (R(s,a,s') + \gamma V_{\pi}(s'))
$$

### Exercise - implement `policy_improvement`

```c
Difficulty: 🔴🔴🔴⚪⚪
Importance: 🔵🔵🔵🔵⚪

You should spend up to 10-25 minutes on this exercise.
```

The `policy_improvement` function takes in a value function $V$, and transition and reward matrices $T$ and $R$ (stored as `env.T` and `env.R`), and returns a new policy $\pi^\text{better}$.

```python
def policy_improvement(env: Environment, V: Arr, gamma=0.99) -> Arr:
    '''
    Args:
        env: Environment
        V  : (num_states,) value of each state following some policy pi
    Outputs:
        pi_better : vector (num_states,) of actions representing a new policy obtained via policy iteration
    '''
    # YOUR CODE HERE
    pass


tests.test_policy_improvement(policy_improvement)
```

<details>
<summary>Solution</summary>

Shortest solution:

```python
def policy_improvement(env: Environment, V: Arr, gamma=0.99) -> Arr:
    '''
    Args:
        env: Environment
        V  : (num_states,) value of each state following some policy pi
    Outputs:
        pi_better : vector (num_states,) of actions representing a new policy obtained via policy iteration
    '''
    # SOLUTION
    q_values_for_every_state_action_pair = sum([
        einops.einsum(env.T, env.R, "s a s_next, s a s_next -> s a"),
        gamma * einops.einsum(env.T, V, "s a s_next, s_next -> s a")
    ]) # shape [states actions]

    pi_better = q_values_for_every_state_action_pair.argmax(axis=1)

    return pi_better
```

Alternate solution which puts things in terms of `transition_matrix` and `reward_matrix`, i.e. the notation we introduced earlier:

```
def policy_improvement(env: Environment, V: Arr, gamma=0.99) -> Arr:
    '''
    Args:
        env: Environment
        V  : (num_states,) value of each state following some policy pi
    Outputs:
        pi_better : vector (num_states,) of actions representing a new policy obtained via policy iteration
    '''
    states = np.arange(env.num_states)
    transition_matrix = env.T[states, :, :]
    reward_matrix = env.R[states, :, :]
    
    q_values_for_every_state_action_pair = einops.einsum(
        transition_matrix,
        reward_matrix + gamma * V,
        "s a s_prime, s a s_prime -> s a"
    )
    pi_better = q_values_for_every_state_action_pair.argmax(-1)

    return pi_better
```

</details>

Putting these together, we now have an algorithm to find the optimal policy for an environment.

$$
\pi_0 \overset{E}{\to} V_{\pi_0}
\overset{I}{\to} \pi_1 \overset{E}{\to} V_{\pi_1}
\overset{I}{\to} \pi_2 \overset{E}{\to} V_{\pi_2} \overset{I}{\to}  \ldots
$$

We alternate policy evaluation ($\overset{E}{\to}$) and policy improvement ($\overset{I}{\to}$), with each $I$ step being a monotonic improvement, until the policy no longer changes ($\pi_n = \pi_{n+1}$), at which point we have an optimal policy, as our current policy will satisfy the optimal Bellman equations:

$$
V_{\pi^*} = \text{argmax}_a \sum_{s'} T(s' \mid s,a) (R(s,a,s') + \gamma V_{\pi^*}(s'))
$$

### Exercise - implement `find_optimal_policy`

```c
Difficulty: 🔴🔴🔴⚪⚪
Importance: 🔵🔵🔵🔵⚪

You should spend up to 15-30 minutes on this exercise.
```

Implement the `find_optimal_policy` function below. This should iteratively do the following:

* find the correct value function $V_{\pi}$ for your given policy $\pi$ using your `policy_eval_exact` function,
* update your policy using `policy_improvement`,

where we start from any arbitrary value of $\pi_0$ (we've picked all zeros).

A few things to note:

* Don't forget that policies should be of `dtype=int`, rather than floats! $\pi$ in this case represents **the deterministic action you take in each state**.
* Since the optimal policy is not unique, the automated tests will merely check that your optimal policy has the same value function as the optimal policy found by the solution.

```python
def find_optimal_policy(env: Environment, gamma=0.99, max_iterations=10_000):
    '''
    Args:
        env: environment
    Outputs:
        pi : (num_states,) int, of actions represeting an optimal policy
    '''
    pi = np.zeros(shape=env.num_states, dtype=int)
    # YOUR CODE HERE
    pass
    


tests.test_find_optimal_policy(find_optimal_policy)

penalty = -0.04
norvig = Norvig(penalty)
pi_opt = find_optimal_policy(norvig, gamma=0.99)
norvig.render(pi_opt)
```

<details>
<summary>Solution</summary>

```python
def find_optimal_policy(env: Environment, gamma=0.99, max_iterations=10_000):
    '''
    Args:
        env: environment
    Outputs:
        pi : (num_states,) int, of actions represeting an optimal policy
    '''
    pi = np.zeros(shape=env.num_states, dtype=int)
    # SOLUTION

    for i in range(max_iterations):
        V = policy_eval_exact(env, pi, gamma)
        pi_new = policy_improvement(env, V, gamma)
        if np.array_equal(pi_new, pi):
            return pi_new
        else:
            pi = pi_new
    else:
        print(f"Failed to converge after {max_iterations} steps.")
        return pi
```


</details>

Once you've passed the tests, you should play around with the `penalty` value for the gridworld environment and see how this affects the optimal policy found. Which squares change their optimal strategy when the penalty takes different negative values (e.g. -0.04, -0.1, or even -1)? Can you see why this happens?

<details>
<summary>Click this dropdown to read the answer.</summary>

The two possible routes to the end square are moving clockwise and anticlockwise around the center obstacle.

The clockwise route requires more steps, which (assuming $r<0$) will be penalised more. The anticlockwise route has a different disadvantage. The agent has a 70% probability of slipping, and the anticlockwise route brings them into closer proximity with the $-1$ terminal state.

You should find a certain threshold value $r^* < 0$ such that the optimal policy is clockwise when $r<r^*$ (because movement penalty is large, making the anticlockwise route less attractive), and anticlockwise when $r>r^*$.

Lastly, for a very large negative value of $r$, you should see the model moving to the red square as soon as possible if it's closer than the green square, since even taking a penalty of $-1$ for hitting red is better than taking more time-based penalties to get to green!

</details>

## Bonus

- Implement and test your policy evaluation method on other environments.
- Complete some exercises in Chapters 3 and 4 of Sutton and Barto.
- Modify the tabular RL solvers to allow stochastic policies or to allow $\gamma=1$ on episodic environments (may need to change how environments are defined.)



""", unsafe_allow_html=True)

