import streamlit as st

def section():

    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#introduction'>Introduction</a></li>
    <li class='margtop'><a class='contents-el' href='#problem-setup'>Problem Setup</a></li>
    <li class='margtop'><a class='contents-el' href='#summary-of-the-algorithm'>Summary of the algorithm</a></li>
    <li class='margtop'><a class='contents-el' href='#notation'>Notation</a></li>
    <li class='margtop'><a class='contents-el' href='#content-learning-objectives'>Content & Learning Objectives</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#1-periodicity-fourier-basis'>Periodicity & Fourier basis</a></li>
        <li><a class='contents-el' href='#2-circuits-feature-analysis'>Circuits & Feature Analysis</a></li>
        <li><a class='contents-el' href='#3-analysis-during-training'>Analysis During Training</a></li>
        <li><a class='contents-el' href='#4-bonus'>Bonus</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#setup'>Setup</a></li>
</ul>""", unsafe_allow_html=True)

    st.markdown(
r"""
# [1.8] Grokking and Modular Arithmetic

### Colab: [**exercises**](https://colab.research.google.com/drive/1wg20amCB7n_myEgHYXMnltLiqH3f9aRN?usp=sharing) | [**solutions**](https://colab.research.google.com/drive/1tg4TyTVOWVRRjmTHj6Af7ewHAnFEe39Z?usp=sharing)

Please send any problems / bugs on the `#errata` channel in the [Slack group](https://join.slack.com/t/arena-uk/shared_invite/zt-28h0xs49u-ZN9ZDbGXl~oCorjbBsSQag), and ask any questions on the dedicated channels for this chapter of material.

You can toggle dark mode from the buttons on the top-right of this page.

Links to other chapters: [**(0) Fundamentals**](https://arena3-chapter0-fundamentals.streamlit.app/), [**(2) RL**](https://arena3-chapter2-rl.streamlit.app/).
                
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/wheel3-2.png" width="350">

                
## Introduction

Our goal for these exercises is to reverse-engineer a one-layer transformer trained on modular addition! It turns out that the circuit responsible for this involves discrete Fourier transforms and trigonometric identities. This is one of the most interesting circuits for solving an algorithmic task that has been fully reverse-engineered thus far.
                
We will also go deeper than just analysing the model's final learned solution; we'll also look at the model's training over time, and find evidence of **grokking** - when the model rapidly goes from a high test loss to near zero. We'll investigate why this grokking happens, and ways it can be predicted.

These exercises are adapted from the [original notebook](https://colab.research.google.com/drive/1F6_1_cWXE5M7WocUcpQWp3v8z4b1jL20) by Neel Nanda and Tom Lierberum (and to a lesser extent the [accompanying paper](https://arxiv.org/abs/2301.05217)). We'll mainly be focusing on mechanistic analysis of this toy model, rather than replicating the grokking results (these may come in later exercises).


## Problem Setup

The model we will be reverse-engineering today is a one-layer transformer, with no layer norm and learned positional embeddings. $d_{model} = 128$, $n_{heads} = 4$, $d_{head}=32$, $d_{mlp}=512$. 

The task this model was trained on is addition modulo the prime $p = 113$. The input format is a sequence of three tokens `[x, y, =]`, with $d_{vocab}=114$ (integers from $0$ to $p - 1$ and $=$). The prediction for the next token after `=` should be the token corresponding to $x + y \pmod{p}$.

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/basic_schematic.png" width="480">

It was trained with full batch training, with 0.3 of the total data as training data. It is trained with AdamW, with $lr=10^{-3}$ and very high weight decay ($wd=1$).


## Summary of the algorithm

Broadly, the algorithm works as follows:

* Given two tokens $x, y \in \{0, 1, \ldots, p - 1\}$, map these to $\sin(\omega x)$, $\cos(\omega x)$, $\sin(\omega y)$, $\cos(\omega y)$, where $\omega = \omega_k = \frac{2k\pi}{p}, k \in \mathbb{N}$.
    * In other words, we throw away most frequencies, and only keep a handful of **key frequencies** corresponding to specific values of $k$.
* Calcuates the quadratic terms:
    $$
    \begin{align*}
    \cos(\omega x) &\cos(\omega y)\\
    \sin(\omega x) &\sin(\omega y)\\
    \cos(\omega x) &\sin(\omega y)\\
    \sin(\omega x) &\cos(\omega y)
    \end{align*}
    $$
    in hacky ways (using attention and ReLU). This also allows us to compute the following linear combinations:
    $$
    \begin{align*}
    \cos(\omega (x+y)) &= \cos(\omega x) \cos(\omega y) - \sin(\omega x) \sin(\omega y)\\
    \sin(\omega (x+y)) &= \sin(\omega x) \cos(\omega y) + \cos(\omega x) \sin(\omega y)
    \end{align*}
    $$

* Computes our output logit vector, s.t. each element $\text{logits}[z]$ is a linear combination of terms of the form:
    $$
    \cos(\omega (x + y - z)) = \cos(\omega (x+y)) \cos(\omega z) + \sin(\omega (x+y)) \sin(\omega z)
    $$
    which is a linear combination of the terms we computed above.
* These values (for different $k$) will be added together to get our final output.
    * There is [constructive interference](https://en.wikipedia.org/wiki/Wave_interference) at $z^* = x + y \; (\operatorname{mod} p)$, and destructive interference everywhere else - hence we get accurate predictions.


## Notation
    
A few words about notation we'll use in these exercises, to help remove ambiguity:

* $x$ and $y$ will always refer to the two inputs to the model. We'll also sometimes use the terminology $t_0$ and $t_1$, which are the one-hot encodings of these inputs.
    * The third input token, `=` will always be referred to as $t_2$. Unlike $t_0$ and $t_1$, this token is always the same in every input sequence.
    * $t$ will refer to the matrix of all three one-hot encoded tokens, i.e. it has size $(3, d_{vocab})$. Here, we have $d_{vocab} = p + 1$ (since we have all the numbers from $0$ to $p - 1$, and the token `=`.)

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/tensor.png" width="280">

* $z$ will always refer to the output of the model. For instance, when we talk about the model "computing $\cos(\omega (x + y - z))$", this means that the vector of output logits is the sequence:

    $$
    (\cos(\omega (x + y - z)))_{z = 0, 1, ..., p-1}
    $$
    Note that we discard the logit for the `=` sign.


* We are keeping TransformerLens' convention of left-multiplying matrices. For instance:
    * the embedding matrix $W_E$ has shape $(d_{vocab}, d_{model})$,
    * $t_0 ^T W_E \in \mathbb{R}^{d_{model}}$ is the embedding of the first token,
    * and $t W_E \in \mathbb{R}^{3 \times d_{model}}$ is the embedding of all three tokens.



## Content & Learning Objectives

#### 1️⃣ Periodicity & Fourier basis

This section gets you acquainted with the toy model. You'll do some initial investigations, and see that the activations are highly periodic. You'll also learn how to use the Fourier basis to represent periodic functions.

> ##### Learning Objectives
> 
> * Understand the problem statement, the model architecture, and the corresponding and functional form of any possible solutions.
> * Learn about the Fourier basis (1D and 2D), and how it can be used to represent arbitrary functions.
> * Understand that periodic functions are sparse in the Fourier basis, and how this relates to the model's weights.

#### 2️⃣ Circuits & Feature Analysis

In this section, you'll apply your understanding of the Fourier basis and the periodicity of the model's weights to break down the exact algorithm used by the model to solve the task. You'll verify your hypotheses in several different ways.

> ##### Learning Objectives
> 
> * Apply your understanding of the 1D and 2D Fourier bases to show that the activtions / effective weights of your model are highly sparse in the Fourier basis.
> * Turn these observations into concrete hypotheses about the model's algorithm.
> * Verify these hypotheses using statistical methods, and interventions like ablation.
> * Fully understand the model's algorithm, and how it solves the task.

#### 3️⃣ Analysis During Training

In this section, you'll have a look at how the model evolves during the course of training. This section is optional, and the observations we make are more speculative than the rest of the material.

> ##### Learning Objectives
> 
> * Understand the idea of tracking metrics over time, and how this can inform when certain circuits are forming.
> * Investigate and interpret the evolution over time of the singular values of the model's weight matrices.
> * Investigate the formation of other capabilities in the model, like commutativity.

#### 4️⃣ Bonus

Finally, we conclude with a discussion of these exercises, and some thoughts on future directions it could be taken.


## Setup

```python
import torch as t
from torch import Tensor
import torch.nn.functional as F
import numpy as np
from pathlib import Path
import os
import sys
import plotly.express as px
import plotly.graph_objects as go
from functools import *
import gdown
from typing import List, Tuple, Union, Optional
from fancy_einsum import einsum
import einops
from jaxtyping import Float, Int
from tqdm import tqdm
from transformer_lens import utils, ActivationCache, HookedTransformer, HookedTransformerConfig
from transformer_lens.hook_points import HookPoint
from transformer_lens.components import LayerNorm

# Make sure exercises are in the path
chapter = r"chapter1_transformer_interp"
exercises_dir = Path(f"{os.getcwd().split(chapter)[0]}/{chapter}/exercises").resolve()
section_dir = exercises_dir / "part8_grokking_and_modular_arithmetic"
if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))

root = (section_dir / 'Grokking' / 'saved_runs').resolve()
large_root = (section_dir / 'Grokking' / 'large_files').resolve()

from part8_grokking_and_modular_arithmetic.my_utils import *
import part8_grokking_and_modular_arithmetic.tests as tests

device = t.device("cuda" if t.cuda.is_available() else "cpu")

t.set_grad_enabled(False)

MAIN = __name__ == "__main__"
```

""", unsafe_allow_html=True)

