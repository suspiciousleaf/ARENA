import streamlit as st

def section():
    st.sidebar.markdown(r"""
## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#steering-model-behaviour'>Steering model behaviour</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-replicate-the-steering-vector-results'><b>Exercise</b> - replicate the steering vector results</a></li>
    </ul></li>
</ul>""", unsafe_allow_html=True)
    
    st.markdown(
r"""
# Steering Vectors in GPT2-XL

Here, we discuss a different but related set of research: Alex Turner's work on steering vectors. This also falls under the umbrella of "interventions in the residual stream using vectors found with forward pass (non-SGD) based methods in order to alter behaviour", but it has a different setup, objectives, and approach.

> ### Learning Objectives
>
> * Understand the goals & main results from Alex Turner et al's work on steering vectors
> * Reproduce the changes in behaviour described in their initial post

## Steering model behaviour

In the final non-bonus exercise of the previous section, we touched on the idea of using function vectors to induce behavioural changes in the model's completions, rather than specifically making it solve zero-shot or corrupted prompts with the right completion. In these next exercises, we'll explore this kind of work in more detail. We'll be primarily using Turner et al's work on [Steering GPT-2-XL by adding an activation vector](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector).

Summary of the way in which this work differs from the function vector work we've done so far:

* Function vectors focused on the model performing a particular function (e.g. mapping a word to its opposite), whereas this work focuses on behavioural changes (e.g. completing a prompt which has negative tone in a positive way).
* Function vectors work looked at very large models (our exercises used Pythia-7B, the smallest model which was examined in the function vectors paper). This particular steering vectors post focused on the smaller models GPT2-Small (85m) and GPT2-XL (1.5B). We'll be focusing on GPT2-XL.
* The second half of our function vectors work identified important attention heads and focused on their outputs, rather than just adding to the residual stream directly. In this steering vector setup, we'll go back to the simpler method of adding directly into the residual stream.

Despite these differences, much of the work which was done here overlaps with function vector work, since they both fall into the broader category of *"finding vectors using forward-pass-based methods (i.e. not with SGD) and using them to intervene on models during forward passes & change the model's output"*. This description would also include the following:

* [Inference-time intervention](https://www.lesswrong.com/posts/kuQfnotjkQA4Kkfou/inference-time-intervention-eliciting-truthful-answers-from), which focuses on inducing the behavioural change of "making the model tell the truth". It also looks at other non-forward-pass-based techniques for finding an intervention vector, e.g. CCS and linear probing, although it concludes that forward-pass-based methods similar to the ones we've been using so far work the best.
* [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/abs/2312.06681), which can be thought of as an extension of the GPT2-XL steering vector work to larger models, specifically Llama 2 13B. It also takes more of a high-level evals framework; measuring the model's change in attributes such as sycophancy, myopia, and power-seeking (finding that these attributes can be increased or decreased by adding the appropriate vectors).

We'll discuss some of this work more in the bonus section, but for now, let's get on with the exercises!

First, we'll load in GPT2-XL, then we'll replicate some of the examples in the main post.

```python
model = LanguageModel('gpt2-xl')
tokenizer = model.tokenizer
```

### Exercise - replicate the steering vector results

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´ðŸ”´
Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª

You should spend up to 30-50 minutes on this exercise.
```

Replicate the results in the LessWrong post [Steering GPT-2-XL by adding an activation vector](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#fnrefcvnfx3e6sfu); specifically the "demonstrations of additions that work well" section.

We've provided a function template as well as some example code to run; your main job will be to fill in the function. This will be like a hybrid of several previous exercises (with most similarity to the function `calculate_and_intervene_with_h`), although there will be a few methodological differences.

This is the last exercise in this set, and hopefully it'll provide an opportunity to draw together all the threads of what you've learned so far!

### Caching

This is a different kind of causal intervention than we performed in previous sections. Rather than adding a single vector to the final sequence position at each token generation, we're adding a slice of vectors to the first sequence positions of the original prompt (see tables like in [this section](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#1__Love___Hate) for an illustration). How do you think this will affect our function? Should we still cache? Should we be using `.generate()` or `.forward()`? If using `.generate()`, do we need to call `invoker.next()` ?

<details>
<summary>Click this dropdown for answers to the questions above.</summary>

Rather than adding to each final sequence position for every token generated, we just add the vectors once, to the end of the prompt. This means that:

- We can still use caching (because the values we cache shouldn't be different in subsequent token generations),
- We should be using `.generate()` (because we're doing multi-token generation),
- We don't need to call `invoker.next()` (because we only intervene once, and our intervention will be cached & applied to all subsequent tokens which are generated).

Again, if any of this is confusing then please ask a TA or message in the Slack channel.

</details>

### Padding

The [tables](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#1__Love___Hate) show the activations being added on the left (i.e. the sequences are padded on the right), but by default padding is applied on the left. There are 2 possible ways you can get around this:

1. Right-pad the input sequences manually, i.e. use something like `len(tokenizer.tokenize(prompt))` to see how long each of the prompts is, and add copies of `tokenizer.pad_token` to the end of each sequence.
2. Don't manually pad the input sequences, instead slice the sequences you add to the original prompt from the right side of the activation addition sequences, rather than from the left side.

The solutions use (2), but you can use either of these methods.

### Sampling

Following the post, we'll use top-p sampling with probability 0.3 to generate our sequences. We'll also use a small frequency penalty to penalize repetition (so the model gets stuck in loops less). If you've done earlier exercises in this section then you might have implemented `freq_penalty` during sampling; this is supported by TransformerLens models, but HuggingFace uses the somewhat similar `repetition_penalty` (default value is 1.0 indicating no penalty, values higher than 1.0 apply a penalty to repeated tokens).

We apply these sampling methods by passing keyword arguments into the `generate` method:

```python
{
    "do_sample": True, # necessary whenever we're sampling rather than doing greedy decoding
    "top_p": 0.3,
    "repetition_penalty": 1.1,
}
```

Note that the sequences are generated stochastically rather than greedily - this means we'll get different results if we input multiple different copies of the same sequence. We've given you the `n_comparisons` argument in the function below, i.e. you should generate this many steered *and* this many unsteered completions.

### Other tips / notes

We recommend starting with example #9 (the "talking about weddings" one). It seems quite robust to the exact conditions of the forward pass, unlike the `Love - Hate` example. You can use any of the template cells we've given you below.

We've given you a `use_bos` argument; if this is True then you should append `tokenizer.bos_token` to the start of all the prompts. This is just to be true to the LessWrong post's implementation; it won't change behaviour much. For convenience, we've implemented this part of the function for you.

```python
def calculate_and_apply_steering_vector(
    model: LanguageModel,
    prompt: str,
    activation_additions: List[Tuple[int, float, str]],
    n_tokens: int,
    n_comparisons: int = 1,
    use_bos: bool = True,
) -> Tuple[List[str], List[str]]:
    '''
    Performs the steering vector experiments described in the LessWrong post.

    Args:
        prompt: str
            The original prompt, which we'll be doing activation steering on.

        activation_additions: List[Tuple[int, float, str]], each tuple contains:
            layer - the layer we're applying these steering vectors to
            coefficient - the value we're multiplying it by
            prompt - the prompt we're inputting
            e.g. activation_additions[0] = [6, 5.0, " Love"] means we add the " Love" vector at layer 6, scaled by 5x

        n_tokens: int
            Number of tokens which will be generated for each completion

        n_comparisons: int
            Number of sequences generated in this function (i.e. we generate `n_comparisons` which are unsteered, and
            the same number which are steered).

    Returns:
        unsteered_completions: List[str]
            List of length `n_comparisons`, containing all the unsteered completions.
        
        steered_completions: List[str]
            List of length `n_comparisons`, containing all the steered completions.
    '''
    # Add the BOS token manually, if we're including it
    if use_bos:
        bos = model.tokenizer.bos_token
        prompt = bos + prompt
        activation_additions = [[layer, coeff, bos + p] for layer, coeff, p in activation_additions]
    
    # YOUR CODE HERE - fill in the rest of the function
```

<details>
<summary>Solution</summary>

```python
sampling_kwargs = {
    "do_sample": True,
    "top_p": 0.3,
    "repetition_penalty": 1.1,
}

def calculate_and_apply_steering_vector(
    model: LanguageModel,
    prompt: str,
    activation_additions: List[Tuple[int, float, str]],
    n_tokens: int,
    n_comparisons: int = 1,
    use_bos: bool = True,
) -> Tuple[List[str], List[str]]:
    '''
    Performs the steering vector experiments described in the LessWrong post.

    Args:
        prompt: str
            The original prompt, which we'll be doing activation steering on.

        activation_additions: List[Tuple[int, float, str]], each tuple contains:
            layer - the layer we're applying these steering vectors to
            coefficient - the value we're multiplying it by
            prompt - the prompt we're inputting
            e.g. activation_additions[0] = [6, 5.0, " Love"] means we add the " Love" vector at layer 6, scaled by 5x

        n_tokens: int
            Number of tokens which will be generated for each completion

        n_comparisons: int
            Number of sequences generated in this function (i.e. we generate `n_comparisons` which are unsteered, and
            the same number which are steered).

    Returns:
        unsteered_completions: List[str]
            List of length `n_comparisons`, containing all the unsteered completions.
        
        steered_completions: List[str]
            List of length `n_comparisons`, containing all the steered completions.
    '''
    # Add the BOS token manually, if we're including it
    if use_bos:
        bos = model.tokenizer.bos_token
        prompt = bos + prompt
        activation_additions = [[layer, coeff, bos + p] for layer, coeff, p in activation_additions]

    # Get the (layers, coeffs, prompts) in an easier form to use, also calculate the prompt lengths & check they're all the same
    act_add_layers, act_add_coeffs, act_add_prompts = zip(*activation_additions)
    act_add_seq_lens = [len(tokenizer.tokenize(p)) for p in act_add_prompts]
    assert len(set(act_add_seq_lens)) == 1, "All activation addition prompts must be the same length."
    assert act_add_seq_lens[0] <= len(tokenizer.tokenize(prompt)), "All act_add prompts should be shorter than original prompt."

    # Get the prompts we'll intervene on (unsteered and steered)
    steered_prompts = [prompt for _ in range(n_comparisons)]
    unsteered_prompts = [prompt for _ in range(n_comparisons)]

    with model.generate(max_new_tokens=n_tokens, remote=True, **sampling_kwargs) as generator:

        # Run the act_add prompts (i.e. the contrast pairs), and extract their activations
        with generator.invoke(act_add_prompts) as invoker:
            # Get all the prompts from the activation additions, and put them in a list
            # (note, we slice from the end of the sequence because of left-padding)
            act_add_vectors = [
                model.transformer.h[layer].output[0][i, -seq_len:]
                for i, (layer, seq_len) in enumerate(zip(act_add_layers, act_add_seq_lens))
            ]

        # Forward pass on unsteered prompts (no intervention, no activations saved - we only need the completions)
        with generator.invoke(steered_prompts) as invoker:
            pass

        # Forward pass on steered prompts (we add in the results from the act_add prompts)
        with generator.invoke(unsteered_prompts) as invoker:
            # For each act_add prompt, add the vector to residual stream, at the start of the sequence
            for i, (layer, coeff, seq_len) in enumerate(zip(act_add_layers, act_add_coeffs, act_add_seq_lens)):
                model.transformer.h[layer].output[0][:, :seq_len] += act_add_vectors[i] * coeff

    # Decode steered & unsteered completions (discarding the sequences we only used for extracting activations) & return results
    unsteered_completions = tokenizer.batch_decode(generator.output[-2*n_comparisons: -n_comparisons])
    steered_completions = tokenizer.batch_decode(generator.output[-n_comparisons:])
    return unsteered_completions, steered_completions
```
</details>

To test your function, use any of the following code snippets (as mentioned, we recommend starting with the weddings example, since the results tend to be pretty robust).

<details>
<summary>Function call for weddings example (#9)</summary>

```python
unsteered_completions, steered_completions = calculate_and_apply_steering_vector(
    model,
    prompt = "I went up to my friend and said",
    activation_additions = [
        (20, +4.0, "I talk about weddings constantly  "),
        (20, -4.0, "I do not talk about weddings constantly"),
    ],
    n_tokens = 50,
    n_comparisons = 3,
    use_bos = False,
)

table = Table("Unsteered", "Steered", title="Completions", show_lines=True)
for usc, sc in zip(unsteered_completions, steered_completions):
    table.add_row(usc, sc)
rprint(table)
```

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/completions-wedding.png" width="600">

</details>

<details>
<summary>Function call for love / hate example (#1)</summary>

```python
unsteered_completions, steered_completions = calculate_and_apply_steering_vector(
    model,
    prompt = "I hate you because",
    activation_additions = [
        (6, +5.0, "Love "),
        (6, -5.0, "Hate"),
    ],
    n_tokens = 50,
    n_comparisons = 3,
    use_bos = True,
)

table = Table("Unsteered", "Steered", title="Completions", show_lines=True)
for usc, sc in zip(unsteered_completions, steered_completions):
    table.add_row(usc, sc)
rprint(table)
```

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/completions-bad.png" width="600">

</details>

<details>
<summary>Function call for Eiffel Tower example (#6)</summary>

```python
unsteered_completions, steered_completions = calculate_and_apply_steering_vector(
    model,
    prompt = "To see the eiffel tower, people flock to",
    activation_additions = [
        (24, +10.0, "The Eiffel Tower is in Rome"),
        (24, -10.0, "The Eiffel Tower is in France"),
    ],
    n_tokens = 50,
    n_comparisons = 3,
    use_bos = False,
)

table = Table("Unsteered", "Steered", title="Completions", show_lines=True)
for usc, sc in zip(unsteered_completions, steered_completions):
    table.add_row(usc, sc)
rprint(table)
```

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/completions-eiffel.png" width="600">

</details>

""", unsafe_allow_html=True)