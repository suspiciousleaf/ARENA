import streamlit as st

def section():
    st.sidebar.markdown(r"""
## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#icl-task'>ICL Task</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-optional-generate-your-own-antonym-pairs'><b>Exercise</b> (optional) - generate your own antonym pairs</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#icl-dataset'>ICL Dataset</a></li>
    <li class='margtop'><a class='contents-el' href='#task-encoding-vector'>Task-encoding vector</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-forward-pass-on-antonym-dataset'><b>Exercise</b> - forward pass on antonym dataset</a></li>
        <li><a class='contents-el' href='#exercise-intervene-with-h'><b>Exercise</b> - intervene with h</a></li>
        <li><a class='contents-el' href='#exercise-combine-the-last-two-functions'><b>Exercise</b> - combine the last two functions</a></li>
        <li><a class='contents-el' href='#exercise-compute-change-in-accuracy'><b>Exercise</b> - compute change in accuracy</a></li>
    </ul></li>
</ul>""", unsafe_allow_html=True)
    
    st.markdown(
r"""
# Task-encoding hidden states

We'll begin with the following question, posed by the Function Vectors paper:

> *When a transformer processes an ICL (in-context-learning) prompt with exemplars demonstrating task $T$, do any hidden states encode the task itself?*

We'll prove that the answer is yes, by constructing a vector $h$ from a set of ICL prompts for the **antonym task**, and intervening with our vector to make our model produce antonyms on zero-shot prompts.

This will require you to learn how to perform causal interventions with `nnsight`, not just save activations.

(Note - this section structurally follows section 2.1 of the function vectors paper).


> ### Learning Objectives
>
> * Understand how `nnsight` can be used to perform causal interventions, and perform some yourself
> * Reproduce the "h-vector results" from the function vectors paper; that the residual stream does contain a vector which encodes the task and can induce task behaviour on zero-shot prompts

## ICL Task

### Exercise (optional) - generate your own antonym pairs

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª
Importance: ðŸ”µðŸ”µâšªâšªâšª

If you choose to do this exercise, you should spend up to 10-30 minutes on it - depending on your familiarity with the OpenAI Python API.
```

We've provided you two options for the antonym dataset you'll use in these exercises.

1. Firstly, we've provided you a list of word pairs, in the file `data/antonym_pairs.txt`.
2. Secondly, if you want to run experiments like the ones in this paper, it can be good practice to learn how to generate prompts from GPT-4 or other models (this is how we generated the data for this exercise).

If you just want to use the provided list of words, skip this exercise and run the code below to load in the dataset from the text file. Alternatively, if you want to generate your own dataset, you can fill in the function `generate_dataset` below, which should query GPT-4 and get a list of antonym pairs.

See [here](https://platform.openai.com/docs/guides/gpt/chat-completions-api) for a guide to using the chat completions API, if you haven't already used it. Use the two dropdowns below (in order) for some guidance.

<details>
<summary>Getting started #1</summary>

Here is a recommended template:

```python
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": antonym_task},
        {"role": "assistant", "content": start_of_response},
    ]
)
```

where `antonym_task` explains the antonym task, and `start_of_respose` gives the model a prompt to start from (e.g. "Sure, here are some antonyms: ..."), to guide its subsequent behaviour.

</details>

<details>
<summary>Getting started #2</summary>

Here is an template you might want to use for the actual request:

```python
example_antonyms = "old: young, top: bottom, awake: asleep, future: past, "

response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": f"Give me {N} examples of antonym pairs. They should be obvious, i.e. each word should be associated with a single correct antonym."},
        {"role": "assistant", "content": f"Sure! Here are {N} pairs of antonyms satisfying this specification: {example_antonyms}"},
    ]
)
```

where `N` is the function argument. Note that we've provided a few example antonyms, and appended them to the start of GPT4's completion. This is a classic trick to guide the rest of the output (in fact, it's commonly used in adversarial attacks).

</details>

Note - it's possible that not all the antonyms returned will be solvable by GPT-J. In this section, we won't worry too much about this. When it comes to testing out our zero-shot intervention, we'll make sure to only use cases where GPT-J can actually solve it.

```python
openai.api_key = "insert-your-key-here!"

def generate_antonym_dataset(N: int):
    '''
    Generates 100 pairs of antonyms, in the form of a list of 2-tuples.
    '''

    assert openai.api_key != "insert your key here!", "Please insert your own key before running this function!"

    # YOUR CODE HERE - fill in this function (optional)


if openai.api_key != "insert-your-key-here!":
    ANTONYM_PAIRS = generate_antonym_dataset(100)

    # Save the word pairs in a text file
    with open(section_dir / "data" / "my_antonym_pairs.txt", "w") as f:
        for word_pair in ANTONYM_PAIRS:
            f.write(f"{word_pair[0]} {word_pair[1]}\n")
```

If you don't want to do this exercise, you can skip it and run the code below to load in the dataset from the text file.

```python
# Load the word pairs from the text file
with open(section_dir / "data" / "antonym_pairs.txt", "r") as f:
    ANTONYM_PAIRS = [line.split() for line in f.readlines()]

print(ANTONYM_PAIRS[:10])
```
    
<details>
<summary>Solution</summary>

```python
def generate_antonym_dataset(N: int): 
    '''
    Generates 100 pairs of antonyms, in the form of a list of 2-tuples.
    '''

    assert openai.api_key != "insert your key here!", "Please insert your own key before running this function!"

    t0 = time.time()

    # Define a few examples (for our dataset, and for our prompt)
    example_antonyms = "old: young, top: bottom, awake: asleep, future: past, "

    # Use openai's api to generate examples. We prepend the example antonyms to the assistant's response, to both
    # make sure the query is successful, and so that the assistant returns words in the same syntax as the examples.
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": f"Give me {N} examples of antonym pairs. They should be obvious, i.e. each word should be associated with a single correct antonym."},
            {"role": "assistant", "content": f"Sure! Here are {N} pairs of antonyms satisfying this specification: {example_antonyms}"},
        ]
    )
    # Add our examples to the response
    response_text: str = example_antonyms + response["choices"][0]["message"]["content"]

    # Create word pairs, by splitting on commas and colons
    word_pairs = [word_pair.split(": ") for word_pair in response_text.strip(".\n").split(", ")]

    print(f"Finished in {time.time()-t0:.2f} seconds.")

    return word_pairs
```

</details>

## ICL Dataset

To handle this list of word pairs, we've given you some helpful classes.

Firstly, there's the `ICLSequence` class, which takes in a list of word pairs and contains methods for constructing a prompt (and completion) from these words. Run the code below to see how it works.

```python
class ICLSequence:
    '''
    Class to store a single antonym sequence.

    Uses the default template "Q: {x}\nA: {y}" (with separate pairs split by "\n\n").
    '''
    def __init__(self, word_pairs: List[List[str]]):
        self.word_pairs = word_pairs
        self.x, self.y = zip(*word_pairs)

    def __len__(self):
        return len(self.word_pairs)

    def __getitem__(self, idx: int):
        return self.word_pairs[idx]

    def prompt(self):
        '''Returns the prompt, which contains all but the second element in the last word pair.'''
        p = "\n\n".join([f"Q: {x}\nA: {y}" for x, y in self.word_pairs])
        return p[:-len(self.completion())]

    def completion(self):
        '''Returns the second element in the last word pair (with padded space).'''
        return " " + self.y[-1]

    def __str__(self):
        '''Prints a readable string representation of the prompt & completion (indep of template).'''
        return f"{', '.join([f'({x}, {y})' for x, y in self[:-1]])}, {self.x[-1]} ->".strip(", ")


word_list = [["hot", "cold"], ["yes", "no"], ["in", "out"], ["up", "down"]]
seq = ICLSequence(word_list)

print("Tuple-representation of the sequence:")
print(seq)
print("\nActual prompt, which will be fed into the model:")
print(seq.prompt())
```

Secondly, we have the `ICLDataset` class. This is also fed a word pair list, and it has methods for generating batches of prompts and completions. It can generate both clean prompts (where each pair is actually an antonym pair) and corrupted prompts (where the answers for each pair are randomly chosen from the dataset).

```python
class ICLDataset:
    '''
    Dataset to create antonym pair prompts, in ICL task format. We use random seeds for consistency
    between the corrupted and clean datasets.

    Inputs:
        word_pairs:
            list of ICL task, e.g. [["old", "young"], ["top", "bottom"], ...] for the antonym task
        size:
            number of prompts to generate
        n_prepended:
            number of antonym pairs before the single-word ICL task
        bidirectional:
            if True, then we also consider the reversed antonym pairs
        corrupted:
            if True, then the second word in each pair is replaced with a random word
        seed:
            random seed, for consistency & reproducibility
    '''

    def __init__(
        self,
        word_pairs: List[List[str]],
        size: int,
        n_prepended: int,
        bidirectional: bool = True,
        seed: int = 0,
        corrupted: bool = False,
    ):
        assert n_prepended+1 <= len(word_pairs), "Not enough antonym pairs in dataset to create prompt."

        self.word_pairs = word_pairs
        self.word_list = [word for word_pair in word_pairs for word in word_pair]
        self.size = size
        self.n_prepended = n_prepended
        self.bidirectional = bidirectional
        self.corrupted = corrupted
        self.seed = seed

        self.seqs = []
        self.prompts = []
        self.completions = []

        # Generate the dataset (by choosing random antonym pairs, and constructing `ICLSequence` objects)
        for n in range(size):
            np.random.seed(seed + n)
            random_pairs = np.random.choice(len(self.word_pairs), n_prepended+1, replace=False)
            random_orders = np.random.choice([1, -1], n_prepended+1)
            if not(bidirectional): random_orders[:] = 1
            word_pairs = [self.word_pairs[pair][::order] for pair, order in zip(random_pairs, random_orders)]
            if corrupted:
                for i in range(len(word_pairs) - 1):
                    word_pairs[i][1] = np.random.choice(self.word_list)
            seq = ICLSequence(word_pairs)

            self.seqs.append(seq)
            self.prompts.append(seq.prompt())
            self.completions.append(seq.completion())

    def create_corrupted_dataset(self):
        '''Creates a corrupted version of the dataset (with same random seed).'''
        return ICLDataset(self.word_pairs, self.size, self.n_prepended, self.bidirectional, corrupted=True, seed=self.seed)

    def __len__(self):
        return self.size

    def __getitem__(self, idx: int):
        return self.seqs[idx]
```

You can see how this dataset works below. **Note that the correct completions have a prepended space**, because this is how the antonym prompts are structured - the answers are tokenized as `"A: answer" -> ["A", ":", " answer"]`. Forgetting prepended spaces is a classic mistake when working with transformers!

```python
dataset = ICLDataset(ANTONYM_PAIRS, size=10, n_prepended=2, corrupted=False)

table = Table("Prompt", "Correct completion")
for seq, completion in zip(dataset.seqs, dataset.completions):
    table.add_row(str(seq), repr(completion))

rprint(table)
```

Compare this output to what it looks like when `corrupted=True`. Each of the pairs before the last one has their second element replaced with a random one (but the last pair is unchanged).

```python
dataset = ICLDataset(ANTONYM_PAIRS, size=10, n_prepended=2, corrupted=True)

table = Table("Prompt", "Correct completion")
for seq, completions in zip(dataset.seqs, dataset.completions):
    table.add_row(str(seq), repr(completions))

rprint(table)
```

<details>
<summary>Aside - the <code>rich</code> library</summary>

The `rich` library is a helpful little library to display outputs more clearly in a Python notebook or terminal. It's not necessary for this workshop, but it's a nice little tool to have in your toolbox.

The most important function is `rich.print` (usually imported as `rprint`). This can print basic strings, but it also supports the following syntax for printing colors:

```python
rprint("[green]This is green text[/], this is default color")
```

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rprint-1.png" width="350">

and for making text bold / underlined:

```python
rprint("[u dark_orange]This is underlined[/], and [b cyan]this is bold[/].")
```

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rprint-2.png" width="350">

It can also print tables:

```python
from rich.table import Table

table = Table("Col1", "Col2", title="Title") # title is optional
table.add_row("A", "a")
table.add_row("B", "b")

rprint(table)
```

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rprint-3.png" width="150">

The text formatting (bold, underlined, colors, etc) is also supported within table cells.

</details>

## Task-encoding vector

### Exercise - forward pass on antonym dataset

```c
Difficulty: ðŸ”´ðŸ”´âšªâšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 10-15 minutes on this exercise.
```

You should fill in the `calculate_h` function below. It should:

* Run a forward pass on the model with the dataset prompts (i.e. the `.prompts` attribute), using the `nnsight` syntax we've demonstrated previously,
* Run a forward pass on the model, using the `nnsight` syntax we've demonstrated previously,
* Return a tuple of the model's output (i.e. a list of its string-token completions, one for each prompt in the batch) and the residual stream value at the end of layer `layer` (e.g. if `layer = -1`, this means the final value of the residual stream before we convert into logits).

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/h-intervention-1.png" width="900">

You should only return the residual stream values for the very last sequence position in each prompt, i.e. the last `:` token (where the model makes the antonym prediction), and same for the completions.

<details>
<summary>Help - I'm not sure how to run (and index into) a batch of inputs.</summary>

If we pass a list of strings to the `generator.invoke` function, this will be tokenized with padding automatically.

The type of padding which is applied is **left padding**, meaning if you index at sequence position `-1`, this will get the final token in the prompt for all prompts in the list, even if the prompts have different lengths.

</details>

```python
def calculate_h(model: LanguageModel, dataset: ICLDataset, layer: int = -1) -> Tuple[List[str], Tensor]:
    '''
    Averages over the model's hidden representations on each of the prompts in `dataset` at layer `layer`, to produce
    a single vector `h`.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset whose prompts `dataset.prompts` you're extracting the activations from (at the last seq pos)
        layer: int
            the layer you're extracting activations from

    Returns:
        completions: List[str]
            list of model completion strings (i.e. the strings the model predicts to follow the last token)
        h: Tensor
            average hidden state tensor at final sequence position, of shape (d_model,)
    '''
    pass


tests.test_calculate_h(calculate_h, model)
```

<details>
<summary>Solution</summary>

```python
def calculate_h(model: LanguageModel, dataset: ICLDataset, layer: int = -1) -> Tuple[List[str], Tensor]:
    '''
    Averages over the model's hidden representations on each of the prompts in `dataset` at layer `layer`, to produce
    a single vector `h`.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset whose prompts `dataset.prompts` you're extracting the activations from (at the last seq pos)
        layer: int
            the layer you're extracting activations from

    Returns:
        completions: List[str]
            list of model completion strings (i.e. the strings the model predicts to follow the last token)
        h: Tensor
            average hidden state tensor at final sequence position, of shape (d_model,)
    '''
    with model.forward(remote=REMOTE) as runner:
        with runner.invoke(dataset.prompts) as invoker:

            hidden_states = model.transformer.h[layer].output[0][:, -1]
            h = hidden_states.mean(dim=0).save()

            logits = model.lm_head.output[:, -1]
            token_ids = logits.argmax(dim=-1).save()

    completions = model.tokenizer.batch_decode(token_ids.value)

    return completions, h.value
```
</details>

Once you've passed the tests, run the cell below. We've provided you with a helper function, which displays the model's output on the antonym dataset (and highlights the examples where the model's prediction is correct). Note, we're using the `repr` function, because a lot of the completions are line breaks, and this helps us see them more clearly!

If you've constructed your antonyms dataset well, you should find that the model's completion is correct most of the time, and most of its mistakes are either copying (e.g. predicting `wet -> wet` rather than `wet -> dry`) or understandable completions which shouldn't really be considered mistakes (e.g. predicting `right -> left` rather than `right -> wrong`). If we were being rigorous, we'd want to filter this dataset to make sure it only contains examples where the model can correctly perform the task - but for these exercises, we won't worry about this.

```python
def display_model_completions_on_antonyms(
    model: LanguageModel,
    dataset: ICLDataset,
    completions: List[str],
    num_to_display: int = 20,
) -> None:
    table = Table("Prompt (tuple representation)", "Model's completion\n(green=correct)", "Correct completion", title="Model's antonym completions")

    for i in range(min(len(completions), num_to_display)):

        # Get model's completion, and correct completion
        completion = completions[i]
        correct_completion = dataset.completions[i]
        correct_completion_first_token = model.tokenizer.tokenize(correct_completion)[0].replace('Ä ', ' ')
        seq = dataset.seqs[i]

        # Color code the completion based on whether it's correct
        is_correct = (completion == correct_completion_first_token)
        completion = f"[b green]{repr(completion)}[/]" if is_correct else repr(completion)

        table.add_row(str(seq), completion, repr(correct_completion))

    rprint(table)


# Get uncorrupted dataset
dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=2)

# Getting it from layer 12, cause the graph suggested this was where there was high accuracy
model_completions, h = calculate_h(model, dataset, layer=12)

# Displaying the output
display_model_completions_on_antonyms(model, dataset, model_completions)
```

### Exercise - intervene with $h$

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª
Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª

You should spend up to 10-15 minutes on this exercise.
```

You should fill in the function `intervene_with_h` below. This will involve:

* Defining a zero-shot dataset, i.e. with no prepended antonym pairs,
* Run two forward passes (within the same context manager):
    * One with no intervention (i.e. `h` is unchanged),
    * One with an intervention on `h` (i.e. the residual stream value is set to `h`, at the layer which `h` was taken from).
* Return the completions for no intervention and intervention cases respectively (see docstring).

The diagram below shows how all of this should work, when combined with the `calculate_h` function.

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/h-intervention-2.png" width="950">

Hint - you can use `tokenizer.batch_decode` to turn a list of tokens into a list of strings.

<details>
<summary>Help - I'm not sure how best to get both the no-intervention and intervention completions.</summary>

You can use `with runner.invoke...` more than once within the same context manager, in order to add to your batch. This will eventually give you output of shape (2*N, seq_len), which can then be indexed and reshaped to get the completions in the no intervention & intervention cases respectively.

</details>

<details>
<summary>Help - I'm not sure how to intervene on the hidden state.</summary>

First, you can define the tensor of hidden states (i.e. using `.output[0]`, like you've done before).

Then, you can add to this tensor directly (or add to some indexed version of it). You can use inplace operations (i.e. `tensor += h`) or redefining the tensor (i.e. `tensor = tensor + h`); either work.

</details>

```python
def intervene_with_h(
    model: LanguageModel,
    zero_shot_dataset: ICLDataset,
    h: Tensor,
    layer: int,
) -> Tuple[List[str], List[str]]:
    '''
    Extracts the vector `h` using previously defined function, and intervenes by adding `h` to the
    residual stream of a set of generated zero-shot prompts.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        h: Tensor
            the `h`-vector we'll be adding to the residual stream
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        completions_zero_shot: List[str]
            list of string completions for the zero-shot prompts, without intervention
        completions_intervention: List[str]
            list of string completions for the zero-shot prompts, with h-intervention
    '''
    pass


tests.test_intervene_with_h(intervene_with_h, model, h, ANTONYM_PAIRS, REMOTE)
```

<details>
<summary>Solution</summary>

```python
def intervene_with_h(
    model: LanguageModel,
    zero_shot_dataset: ICLDataset,
    h: Tensor,
    layer: int,
) -> Tuple[List[str], List[str]]:
    '''
    Extracts the vector `h` using previously defined function, and intervenes by adding `h` to the
    residual stream of a set of generated zero-shot prompts.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        h: Tensor
            the `h`-vector we'll be adding to the residual stream
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        completions_zero_shot: List[str]
            list of string completions for the zero-shot prompts, without intervention
        completions_intervention: List[str]
            list of string completions for the zero-shot prompts, with h-intervention
    '''
    with model.forward(remote=REMOTE) as runner:

        # First, run a forward pass where we don't intervene, just save token id completions
        with runner.invoke(zero_shot_dataset.prompts) as invoker:
            token_completions_zero_shot = model.lm_head.output[:, -1].argmax(dim=-1).save()

        # Next, run a forward pass on the zero-shot prompts where we do intervene
        with runner.invoke(zero_shot_dataset.prompts) as invoker:
            # Add the h-vector to the residual stream, at the last sequence position
            hidden_states = model.transformer.h[layer].output[0]
            hidden_states[:, -1] += h
            # Also save completions
            token_completions_intervention = model.lm_head.output[:, -1].argmax(dim=-1).save()

    # Decode to get the string tokens
    completions_zero_shot = model.tokenizer.batch_decode(token_completions_zero_shot.value)
    completions_intervention = model.tokenizer.batch_decode(token_completions_intervention.value)

    return completions_zero_shot, completions_intervention
```

</details>

Once you've passed these tests, you should run the code below to calculate completions for your function.

**Note, it's very important that we set a different random seed for the zero shot dataset, otherwise we'll be intervening on examples which were actually in the dataset we used to compute $h$!**

```python
layer = 12
dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)
zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)

# Run previous function to get h-vector
h = calculate_h(model, dataset, layer=layer)[1]

# Run new function to intervene with h-vector
completions_zero_shot, completions_intervention = intervene_with_h(model, zero_shot_dataset, h, layer=layer)

print("\nZero-shot completions: ", completions_zero_shot)
print("Completions with intervention: ", completions_intervention)
```

Next, run the code below to visualise your completions in a table. If you've done this correctly, you should see:

* ~0% correct completions on the zero-shot prompt with no intervention, because the model usually just copies the first and only word in the prompt
* ~25% correct completions on the zero-shot prompt with intervention

```python
def display_model_completions_on_h_intervention(
    dataset: ICLDataset,
    completions: List[str],
    completions_intervention: List[str],
    num_to_display: int = 20,
) -> None:
    table = Table("Prompt", "Model's completion\n(no intervention)", "Model's completion\n(intervention)", "Correct completion", title="Model's antonym completions")

    for i in range(min(len(completions), num_to_display)):

        completion_ni = completions[i]
        completion_i = completions_intervention[i]
        correct_completion = dataset.completions[i]
        correct_completion_first_token = tokenizer.tokenize(correct_completion)[0].replace('Ä ', ' ')
        seq = dataset.seqs[i]

        # Color code the completion based on whether it's correct
        is_correct = (completion_i == correct_completion_first_token)
        completion_i = f"[b green]{repr(completion_i)}[/]" if is_correct else repr(completion_i)

        table.add_row(str(seq), repr(completion_ni), completion_i, repr(correct_completion))

    rprint(table)


display_model_completions_on_h_intervention(zero_shot_dataset, completions_zero_shot, completions_intervention)
```

### Exercise - combine the last two functions

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 10-15 minutes on this exercise.
```

One great feature of the `nnsight` library is its ability to parallelize forward passes and perform complex interventions within a single context manager.

In the code above, we had one function to extract the hidden states from the model, and another function where we intervened with those hidden states. But we can actually do both at once: we can compute $h$ within our forward pass, and then intervene with it on a different forward pass (using our zero-shot prompts), all within the same `model.forward` context manager. In other words, **we'll be using `with runner.invoke...` three times** in this context manager.

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/h-intervention-3.png" width="1000">

You should fill in the `calculate_h_and_intervene` function below, to do this. Mostly, this should involve combining your `calculate_h` and `intervene_with_h` functions, and wrapping the forward passes in the same context manager (plus a bit of code rewriting).

Your output should be exactly the same as before (since the `ICLDataset` class is deterministic), hence we've not provided test functions in this case - you can just compare the table you get to the one before! However, this time around your code should run twice as fast, because you're batching the operations of "compute $h$" and "intervene with $h$" together into a single forward pass.

<details>
<summary>Help - I'm not sure how to use the <code>h</code> vector inside the context manager.</summary>

You extract `h` the same way as before, but you don't need to save it, or ever reference its `.value` attribute. It is kept as a proxy. You can still use it later in the context manager, just like it actually was a tensor.

You shouldn't have to `.save()` anything inside your context manager, other than the token completions.

</details>

```python
def calculate_h_and_intervene(
    model: LanguageModel,
    dataset: ICLDataset,
    zero_shot_dataset: ICLDataset,
    layer: int,
) -> Tuple[List[str], List[str]]:
    '''
    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,
    all within the same forward pass. Returns the completions from this intervention.

    Inputs:
        model: LanguageModel
            the model we're using to generate completions
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the `h`-vector
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        completions_zero_shot: List[str]
            list of string completions for the zero-shot prompts, without intervention
        completions_intervention: List[str]
            list of string completions for the zero-shot prompts, with h-intervention
    '''
    pass


dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)
zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)

completions_zero_shot, completions_intervention = calculate_h_and_intervene(model, dataset, zero_shot_dataset, layer=layer)

display_model_completions_on_h_intervention(zero_shot_dataset, completions_zero_shot, completions_intervention)
```

<details>
<summary>Solution</summary>

```python
def calculate_h_and_intervene(
    model: LanguageModel,
    dataset: ICLDataset,
    zero_shot_dataset: ICLDataset,
    layer: int,
) -> Tuple[List[str], List[str]]:
    '''
    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,
    all within the same forward pass. Returns the completions from this intervention.

    Inputs:
        model: LanguageModel
            the model we're using to generate completions
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the `h`-vector
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        completions_zero_shot: List[str]
            list of string completions for the zero-shot prompts, without intervention
        completions_intervention: List[str]
            list of string completions for the zero-shot prompts, with h-intervention
    '''
    with model.forward(remote=REMOTE) as runner:

        # Run on the clean prompts, to get the h-vector
        with runner.invoke(dataset.prompts) as invoker:
            # Define h (we don't need to save it, cause we don't need it outside `runner:`)
            hidden_states = model.transformer.h[layer].output[0]
            h = hidden_states[:, -1].mean(dim=0)

        # First, run a forward pass where we don't intervene
        with runner.invoke(zero_shot_dataset.prompts) as invoker:
            token_completions_zero_shot = model.lm_head.output[:, -1].argmax(dim=-1).save()

        # Next, run a forward pass on the zero-shot prompts where we do intervene
        with runner.invoke(zero_shot_dataset.prompts) as invoker:
            # Add the h-vector to the residual stream, at the last sequence position
            hidden_states = model.transformer.h[layer].output[0]
            hidden_states[:, -1] += h
            # Also save completions
            token_completions_intervention = model.lm_head.output[:, -1].argmax(dim=-1).save()

    # Decode to get the string tokens
    completions_zero_shot = model.tokenizer.batch_decode(token_completions_zero_shot.value)
    completions_intervention = model.tokenizer.batch_decode(token_completions_intervention.value)

    return completions_zero_shot, completions_intervention
```

</details>

### Exercise - compute change in accuracy

```c
Difficulty: ðŸ”´ðŸ”´âšªâšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 10-20 minutes on this exercise.
```

So far, all we've done is look at the most likely completions, and see what fraction of the time these were correct. But our forward pass doesn't just give us token completions, it gives us logits too!

You should now rewrite the `calculate_h_and_intervene` function so that, rather than returning two lists of string completions, it returns two lists of floats containing the **logprobs assigned by the model to the correct antonym** in the no intervention / intervention cases respectively.

<details>
<summary>Help - I don't know how to get the correct logprobs from the logits.</summary>

First, apply log softmax to the logits, to get logprobs.

Second, you can use `tokenizer(dataset.completions)["input_ids"]` to get the token IDs of the correct completions. (Gotcha - some words might be tokenized into multiple tokens, so make sure you're just picking the first token ID for each completion.)

Note - we recommend doing all this inside the context manager, then saving and returning just the correct logprobs not all the logits (this means less to download from the server!).

</details>

```python
def calculate_h_and_intervene_logprobs(
    model: LanguageModel,
    dataset: ICLDataset,
    zero_shot_dataset: ICLDataset,
    layer: int,
) -> Tuple[List[float], List[float]]:
    '''
    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,
    all within the same forward pass. Returns the logprobs on correct tokens from this intervention.

    Inputs:
        model: LanguageModel
            the model we're using to generate completions
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the `h`-vector
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        correct_logprobs: List[float]
            list of correct-token logprobs for the zero-shot prompts, without intervention
        correct_logprobs_intervention: List[float]
            list of correct-token logprobs for the zero-shot prompts, with h-intervention
    '''
    pass
```

<details>
<summary>Solution</summary>

```python
def calculate_h_and_intervene_logprobs(
    model: LanguageModel,
    dataset: ICLDataset,
    zero_shot_dataset: ICLDataset,
    layer: int,
) -> Tuple[List[float], List[float]]:
    '''
    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,
    all within the same forward pass. Returns the logprobs on correct tokens from this intervention.

    Inputs:
        model: LanguageModel
            the model we're using to generate completions
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the `h`-vector
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        correct_logprobs: List[float]
            list of correct-token logprobs for the zero-shot prompts, without intervention
        correct_logprobs_intervention: List[float]
            list of correct-token logprobs for the zero-shot prompts, with h-intervention
    '''
    # Get correct completions from `dataset`, to be used for indexing into the logprobs
    correct_completion_ids = [toks[0] for toks in tokenizer(zero_shot_dataset.completions)["input_ids"]]

    with model.forward(remote=REMOTE) as runner:

        # Run on the clean prompts, to get the h-vector
        with runner.invoke(dataset.prompts) as invoker:
            # Define h (we don't need to save it, cause we don't need it outside `runner:`)
            hidden_states = model.transformer.h[layer].output[0]
            h = hidden_states[:, -1].mean(dim=0)

        # First, run a forward pass where we don't intervene
        with runner.invoke(zero_shot_dataset.prompts) as invoker:
            # We save correct-token logprobs, not all logits - this means less for us to download!
            logprobs = model.lm_head.output[:, -1].log_softmax(dim=-1)
            correct_logprobs_zero_shot = logprobs[t.arange(len(zero_shot_dataset)), correct_completion_ids].save()

        # Next, run a forward pass on the zero-shot prompts where we do intervene
        with runner.invoke(zero_shot_dataset.prompts) as invoker:
            # Add the h-vector to the residual stream, at the last sequence position
            hidden_states = model.transformer.h[layer].output[0]
            hidden_states[:, -1] += h
            # We save correct-token logprobs, not all logits - this means less for us to download!
            logprobs = model.lm_head.output[:, -1].log_softmax(dim=-1)
            correct_logprobs_intervention = logprobs[t.arange(len(zero_shot_dataset)), correct_completion_ids].save()

    return correct_logprobs_zero_shot.value.tolist(), correct_logprobs_intervention.value.tolist()
```

</details>

When you run the code below this function, it will display the log-probabilities (highlighting green when they increase from the zero-shot case). You should find that in every sequence, the logprobs on the correct token increase in the intervention. This helps make something clear - **even if the maximum-likelihood token doesn't change, this doesn't mean that the intervention isn't having a significant effect.**

```python
def display_model_logprobs_on_h_intervention(
    dataset: ICLDataset,
    correct_logprobs_zero_shot: List[float],
    correct_logprobs_intervention: List[float],
    num_to_display: int = 20,
) -> None:
    table = Table(
        "Zero-shot prompt", "Model's logprob\n(no intervention)", "Model's logprob\n(intervention)", "Change in logprob",
        title="Model's antonym logprobs, with zero-shot h-intervention\n(green = intervention improves accuracy)"
    )

    for i in range(min(len(correct_logprobs_zero_shot), num_to_display)):

        logprob_ni = correct_logprobs_zero_shot[i]
        logprob_i = correct_logprobs_intervention[i]
        delta_logprob = logprob_i - logprob_ni
        zero_shot_prompt = f"{dataset[i].x[0]:>8} -> {dataset[i].y[0]}"

        # Color code the logprob based on whether it's increased with this intervention
        is_improvement = (delta_logprob >= 0)
        delta_logprob = f"[b green]{delta_logprob:+.2f}[/]" if is_improvement else f"{delta_logprob:+.2f}"

        table.add_row(zero_shot_prompt, f"{logprob_ni:.2f}", f"{logprob_i:.2f}", delta_logprob)

    rprint(table)


dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)
zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)

correct_logprobs_zero_shot, correct_logprobs_intervention = calculate_h_and_intervene_logprobs(model, dataset, zero_shot_dataset, layer=layer)

display_model_logprobs_on_h_intervention(zero_shot_dataset, correct_logprobs_zero_shot, correct_logprobs_intervention)
```

""", unsafe_allow_html=True)