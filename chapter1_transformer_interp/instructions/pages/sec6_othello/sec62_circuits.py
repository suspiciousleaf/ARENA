import streamlit as st


def section():

    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#probing-across-layers'>Probing Across Layers</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-compute-attn-and-mlp-contributions'><b>Exercise</b> - compute attn and mlp contributions</a></li>
        <li><a class='contents-el' href='#exercise-repeat-this-for-the-"blank"-probe'><b>Exercise</b> - repeat this for the "blank" probe</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#reading-off-neuron-weights'>Reading off neuron weights</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-calculate-neuron-input-weights'><b>Exercise</b> - calculate neuron input weights</a></li>
        <li><a class='contents-el' href='#how-much-variance-does-the-probe-explain'>How much variance does the probe explain?</a></li>
        <li><a class='contents-el' href='#more-neurons'>More Neurons</a></li>
        <li><a class='contents-el' href='#recap-of-this-section'>Recap of this section</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#activation-patching'>Activation Patching</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#setting-up-our-patching'>Setting up our patching</a></li>
        <li><a class='contents-el' href='#exercise-create-a-patching-metric'><b>Exercise</b> - create a patching metric</a></li>
        <li><a class='contents-el' href='#exercise-write-a-patching-function'><b>Exercise</b> - write a patching function</a></li>
        <li><a class='contents-el' href='#recap-of-this-section'>Recap of this section</a></li>
</ul>""", unsafe_allow_html=True)

    st.markdown(
r"""

# Looking for modular circuits


> ### Learning objectives
>
> - Learn how to use our linear probe across layers
> - Apply activation patching at a given sequence position to test hypotheses about our model


## Probing Across Layers


The probe's inputs are accumulated in the residual stream over the six layers - the residual stream is the sum of the output of each previous head and neuron. We can therefore analyse which previous model components most contribute to the overall probe computation, and use this to identify the end of the world model computing circuit.


Let's analyse move 20 in game 1, where we can see that the probe has perfect accuracy after layer 6.


```python
game_index = 1
move = 20
layer = 6

plot_single_board(focus_games_string[game_index, :move+1])
plot_probe_outputs(layer, game_index, move)
```

We now plot the contributions of the attention and MLP layers to the `my_probe` direction. Strikingly, we see that the MLP layers are important for the vertical stripe that was just taken by the opponent, but that most of the rest seems to be done by the attention layers. 


### Exercise - compute attn and mlp contributions

```yaml
Difficulty: 🔴🔴🔴⚪⚪
Importance: 🔵🔵🔵🔵⚪

You should spend up to 10-20 minutes on this exercise.

This is an important exercise to be able to do - logit attribution is one of the most important tools in the interpretability toolbox.
```

Below, you should define `attn_contributions` and `mlp_contributions`. You should do this by taking the batched dot product of the vectors *written* to the residual stream in each layer (from layer `0` to layer `layer` inclusive), and the probe direction "my vs their" which you computed earlier (i.e. `my_probe`).

Note, we're looking for the marginal contribution to the probe direction from each of our components, not the accumulated residual stream. This is because we want to see which components have a strong effect on the output.

<details>
<summary>Hint - what activation names to use?</summary>

You should be using `attn_out` and `mlp_out`.

Calculating each of these two contributions will require taking an `einsum` with the activations and your probe.
</details>

<details>
<summary>Hint - what dimensions to multiply over?</summary>

`my_probe` has shape `(d_model=512, rows=8, cols=8)`. You should multiply the residual stream at a given layer and `my_probe` over the `d_model` dimension (since your probe represents directions in the residual stream). Your output (for a given game index and move) will have shape `(rows=8, cols=8)`, and will represent the amount by which that component of your model writes to the residual stream in the probe directions.
</details>


```python
def plot_contributions(contributions, component: str):
    imshow(
        contributions,
        facet_col=0,
        y=list("ABCDEFGH"),
        facet_labels=[f"Layer {i}" for i in range(7)],
        title=f"{component} Layer Contributions to my vs their (Game {game_index} Move {move})",
        aspect="equal",
        width=1400,
        height=350
    )

def calculate_attn_and_mlp_probe_score_contributions(
    focus_cache: ActivationCache, 
    my_probe: Float[Tensor, "d_model rows cols"],
    layer: int,
    game_index: int, 
    move: int
) -> Tuple[Float[Tensor, "layers rows cols"], Float[Tensor, "layers rows cols"]]:
    
    pass


attn_contributions, mlp_contributions = calculate_attn_and_mlp_probe_score_contributions(focus_cache, my_probe, layer, game_index, move)

plot_contributions(attn_contributions, "Attention")
plot_contributions(mlp_contributions, "MLP")
```

<details>
<summary>Solution</summary>


```python
def calculate_attn_and_mlp_probe_score_contributions(
    focus_cache: ActivationCache, 
    my_probe: Float[Tensor, "d_model rows cols"],
    layer: int,
    game_index: int, 
    move: int
) -> Tuple[Float[Tensor, "layers rows cols"], Float[Tensor, "layers rows cols"]]:
    
    # SOLUTION
    attn_contributions = t.stack([    
        einops.einsum(
            focus_cache["attn_out", l][game_index, move], my_probe, 
            "d_model, d_model rows cols -> rows cols",
        )
        for l in range(layer+1)
    ])
    mlp_contributions = t.stack([
        einops.einsum(
            focus_cache["mlp_out", l][game_index, move], my_probe, 
            "d_model, d_model rows cols -> rows cols",
        )
        for l in range(layer+1)])

    return (attn_contributions, mlp_contributions)
```
</details>


Next, you should plot overall probe scores (i.e. from the accumulated residual stream by the end of layer `layer`). This should mostly be a copy-and-paste job from above (but with a different activation name).


```python
def calculate_accumulated_probe_score(
    focus_cache: ActivationCache, 
    my_probe: Float[Tensor, "d_model rows cols"],
    layer: int,
    game_index: int, 
    move: int
) -> Float[Tensor, "rows cols"]:
    
    pass


overall_contribution = calculate_accumulated_probe_score(focus_cache, my_probe, layer, game_index, move)

imshow(
    overall_contribution, 
    title=f"Overall Probe Score after Layer {layer} for<br>my vs their (Game {game_index} Move {move})",
)
```

<details>
<summary>Solution</summary>


```python
def calculate_accumulated_probe_score(
    focus_cache: ActivationCache, 
    my_probe: Float[Tensor, "d_model rows cols"],
    layer: int,
    game_index: int, 
    move: int
) -> Float[Tensor, "rows cols"]:
    
    # SOLUTION
    return einops.einsum(
        focus_cache["resid_post", layer][game_index, move], my_probe, 
        "d_model, d_model rows cols -> rows cols",
    )
```
</details>


### Exercise - repeat this for the "blank" probe

```yaml
Difficulty: 🔴⚪⚪⚪⚪
Importance: 🔵🔵🔵⚪⚪

This just involves copying code and changing the probe name!
```

Make exactly the same plots, but using `blank_probe` instead of `my_probe`. What do you notice, and why?

<details>
<summary>Answer</summary>

It's very easy to tell whether a cell is blank or not (unlike telling whether a cell is black or white, since this requires an understanding of the piece-flipping rules). Most of the logic is implemented by the zeroth attention layer.
</details>


## Reading off neuron weights


Another cool consequence of having a linear probe is having an interpretable set of directions in the residual stream. This means that we can read off the meaning of any neuron's input and output weights, in terms of the set of directions given by the probe. 

Let's start with neuron L5N1393, which seemed interesting from my initial investigations.

Firstly, we'll compute the normalized version of the probes (normalization happens over the `d_model` dimension, i.e. `blank_probe_normalized` has shape `(d_model, row, col)` and the `[:, i, j]`-th entry is the residual stream probe direction for the `i, j`-th cell in the board).


```python
# Scale the probes down to be unit norm per cell
blank_probe_normalised = blank_probe / blank_probe.norm(dim=0, keepdim=True)
my_probe_normalised = my_probe / my_probe.norm(dim=0, keepdim=True)
# Set the center blank probes to 0, since they're never blank so the probe is meaningless
blank_probe_normalised[:, [3, 3, 4, 4], [3, 4, 3, 4]] = 0.
```

### Exercise - calculate neuron input weights

```yaml
Difficulty: 🔴🔴🔴⚪⚪
Importance: 🔵🔵🔵🔵⚪

You shouldn't spend more than 10-25 minutes on this exercise.
```

The function `calculate_neuron_input_weights` below takes `layer` and `neuron`. It should return a tensor of shape `(row, col)`, where the `[i, j]`-th entry is the projection of this neuron's input weights onto the probe direction corresponding to the `i, j`-th cell in the board.

The function `calculate_neuron_output_weights` is very similar, but returns the projection of the neuron's output weights onto the probe direction.

Recall that, when we talk about a neuron's input and output weights, we're referring to the following decomposition:

$$
\begin{align}
f(x) &= f(x^T W^{in}) W^{out} \\
&= \sum_n f(x^T W^{in}_{[:, n]}) W^{out}_{[n, :]}
\end{align}
$$

where $x$ is a vector in the residual stream, $W^{in}$ is the input weight matrix, $W^{out}$ is the output weight matrix, $f$ is the activation function, and $\sum_n$ represents a sum over neurons.

You'll first write the helper function `get_w_in` and `get_w_out`, which returns the (normalized) vectors $W^{in}_{[:, n]}$ and $W^{out}_{[n, :]}$ for a given neuron. Then, you'll implement `calculate_neuron_input_weights` using this helper function.

Why do we normalize before projecting onto the probe direction? The reason we do this is because we don't care about the scale factor - you could double the magnitude of the output vector and half that of the corresponding input vector, and (ignoring biases) the result would be the same. Instead, we care about how much the input direction of our model's weights aligns with the probe direction we found in the residual stream. The fact that we've also normalized our probes means that we'll be plotting the **cosine similarity** of vectors.

*Note - remember to use `clone()` and `detach()` if you're indexing into a model's weights and performing operations on it. You use `clone()` because you don't want to modify the model's weights, and `detach()` because you don't want to compute gradients through the model's weights.*


```python
def get_w_in(
    model: HookedTransformer,
    layer: int,
    neuron: int,
    normalize: bool = False,
) -> Float[Tensor, "d_model"]:
    '''
    Returns the input weights for the given neuron.

    If normalize is True, the weights are normalized to unit norm.
    '''
    pass


def get_w_out(
    model: HookedTransformer,
    layer: int,
    neuron: int,
    normalize: bool = False,
) -> Float[Tensor, "d_model"]:
    '''
    Returns the input weights for the given neuron.

    If normalize is True, the weights are normalized to unit norm.
    '''
    pass


def calculate_neuron_input_weights(
    model: HookedTransformer, 
    probe: Float[Tensor, "d_model row col"], 
    layer: int, 
    neuron: int
) -> Float[Tensor, "rows cols"]:
    '''
    Returns tensor of the input weights for the given neuron, at each square on the board,
    projected along the corresponding probe directions.

    Assume probe directions are normalized. You should also normalize the model weights.
    '''
    pass


def calculate_neuron_output_weights(
    model: HookedTransformer, 
    probe: Float[Tensor, "d_model row col"], 
    layer: int, 
    neuron: int
) -> Float[Tensor, "rows cols"]:
    '''
    Returns tensor of the output weights for the given neuron, at each square on the board,
    projected along the corresponding probe directions.

    Assume probe directions are normalized. You should also normalize the model weights.
    '''
    pass


tests.test_calculate_neuron_input_weights(calculate_neuron_input_weights, model)
tests.test_calculate_neuron_output_weights(calculate_neuron_output_weights, model)
```

<details>
<summary>Solution</summary>


```python
def get_w_in(
    model: HookedTransformer,
    layer: int,
    neuron: int,
    normalize: bool = False,
) -> Float[Tensor, "d_model"]:
    '''
    Returns the input weights for the neuron in the list, at each square on the board.

    If normalize is True, the weights are normalized to unit norm.
    '''
    # SOLUTION
    w_in = model.W_in[layer, :, neuron].detach().clone()
    if normalize: w_in /= w_in.norm(dim=0, keepdim=True)
    return w_in

def get_w_out(
    model: HookedTransformer,
    layer: int,
    neuron: int,
    normalize: bool = False,
) -> Float[Tensor, "d_model"]:
    '''
    Returns the output weights for the neuron in the list, at each square on the board.

    If normalize is True, the weights are normalized to unit norm.
    '''
    # SOLUTION
    w_out = model.W_out[layer, neuron, :].detach().clone()
    if normalize: w_out /= w_out.norm(dim=0, keepdim=True)
    return  w_out

def calculate_neuron_input_weights(
    model: HookedTransformer, 
    probe: Float[Tensor, "d_model row col"], 
    layer: int, 
    neuron: int
) -> Float[Tensor, "rows cols"]:
    '''
    Returns tensor of the input weights for the given neuron, at each square on the board,
    projected along the corresponding probe directions.

    Assume probe directions are normalized. You should also normalize the model weights.
    '''
    # SOLUTION
    w_in = get_w_in(model, layer, neuron, normalize=True)

    return einops.einsum(
        w_in, probe,
        "d_model, d_model row col -> row col",
    )

def calculate_neuron_output_weights(
    model: HookedTransformer, 
    probe: Float[Tensor, "d_model row col"], 
    layer: int, 
    neuron: int
) -> Float[Tensor, "rows cols"]:
    '''
    Returns tensor of the output weights for the given neuron, at each square on the board,
    projected along the corresponding probe directions.

    Assume probe directions are normalized. You should also normalize the model weights.
    '''
    # SOLUTION
    w_out = get_w_out(model, layer, neuron, normalize=True)

    return einops.einsum(
        w_out, probe,
        "d_model, d_model row col -> row col",
    )
```
</details>


Now, let's examine neuron `1393` in more detail. Can you interpret what it's doing (if you haven't already read this from the post)?


```python
layer = 5
neuron = 1393

w_in_L5N1393_blank = calculate_neuron_input_weights(model, blank_probe_normalised, layer, neuron)
w_in_L5N1393_my = calculate_neuron_input_weights(model, my_probe_normalised, layer, neuron)

imshow(
    t.stack([w_in_L5N1393_blank, w_in_L5N1393_my]),
    facet_col=0,
    y=[i for i in "ABCDEFGH"],
    title=f"Input weights in terms of the probe for neuron L{layer}N{neuron}",
    facet_labels=["Blank In", "My In"],
    width=750,
)
```

<details>
<summary>Answer - what this neuron is doing.</summary>

It seems to represent `(C0==BLANK) & (D1==THEIRS) & (E2==MINE)`.

This is useful for the model, because if all three of these conditions hold, then `C0` is a legal move (because it flips `D1`).
</details>


### How much variance does the probe explain?

We can also look at what fraction of the neuron's input and output weights are captured by the probe (because the vector was scaled to have unit norm, looking at the squared norm of its projection gives us this answer).

We see that the input weights are well explained by this, while the output weights are only somewhat well explained by this. 


```python
w_in_L5N1393 = get_w_in(model, layer, neuron, normalize=True)
w_out_L5N1393 = get_w_out(model, layer, neuron, normalize=True)

U, S, Vh = t.svd(t.cat([
    my_probe.reshape(cfg.d_model, 64),
    blank_probe.reshape(cfg.d_model, 64)
], dim=1))

# Remove the final four dimensions of U, as the 4 center cells are never blank and so the blank probe is meaningless there
probe_space_basis = U[:, :-4]

print("Fraction of input weights in probe basis:", (w_in_L5N1393 @ probe_space_basis).norm().item()**2)
print("Fraction of output weights in probe basis:", (w_out_L5N1393 @ probe_space_basis).norm().item()**2)
```

<details>
<summary>Help - I don't understand what's going on here.</summary>

`U` is a matrix of shape `(512, 128)`. This is best seen as a collection of 128 vectors of size `d_model=512`, which collectively span all the residual stream directions for both probes. Note that the number of vectors is significantly smaller than the dimensionality of the residual stream (they only span 128/512 = 1/4 of the dimensions), so for a randomly chosen vector we generally don't expect it to overlap with the probe directions.

In the print statements, we're projecting our input and output weights onto these vector spaces and finding the norm. The fact that this is much higher than would be expected if the vector was chosen randomly is pretty strong evidence that this neuron is mostly reading from the residual stream along the directions which our probe has identified, in other words **it's using the information located by our "blank" and "mine" probes.** The fact that this *isn't* true for the output weights suggests that the neuron is doing something different downstream (i.e. it's not just contributing to the residual stream in the directions identified by the probes).
</details>


### More Neurons


Lets try this on the layer 4 neurons with the top standard deviation (of activations), and look at how their output weights affect the my probe direction.


```python
layer = 3
top_layer_3_neurons = focus_cache["post", layer][:, 3:-3].std(dim=[0, 1]).argsort(descending=True)[:10]

heatmaps_blank = []
heatmaps_my = []

for neuron in top_layer_3_neurons:
    neuron = neuron.item()
    heatmaps_blank.append(calculate_neuron_output_weights(model, blank_probe_normalised, layer, neuron))
    heatmaps_my.append(calculate_neuron_output_weights(model, my_probe_normalised, layer, neuron))

imshow(
    t.stack(heatmaps_blank),
    facet_col=0,
    y=[i for i in "ABCDEFGH"],
    title=f"Cosine sim of Output weights and the 'blank color' probe for top layer {layer} neurons",
    facet_labels=[f"L3N{n.item()}" for n in top_layer_3_neurons],
    width=1600, height=300,
)

imshow(
    t.stack(heatmaps_my),
    facet_col=0,
    y=[i for i in "ABCDEFGH"],
    title=f"Cosine sim of Output weights and the 'my color' probe for top layer {layer} neurons",
    facet_labels=[f"L3N{n.item()}" for n in top_layer_3_neurons],
    width=1600, height=300,
)
```

These look interesting, but they don't seem to have super interpretable patterns in the same way that our previous neuron did. Can you think of a different statistical measure which might capture this better?

<details>
<summary>Answer</summary>

If you guessed [**kurtosis**](https://en.wikipedia.org/wiki/Kurtosis), you're exactly right! Try replacing your code with this:

```python
def kurtosis(tensor: Tensor, reduced_axes, fisher=True):
    '''
    Computes the kurtosis of a tensor over specified dimensions.
    '''
    return (((tensor - tensor.mean(dim=reduced_axes, keepdim=True)) / tensor.std(dim=reduced_axes, keepdim=True))**4).mean(dim=reduced_axes, keepdim=False) - fisher*3


top_layer_3_neurons = einops.reduce(focus_cache["post", layer][:, 3:-3], "game move neuron -> neuron", reduction=kurtosis).argsort(descending=True)[:10]
```

What kinds of results do you get here? Can you interpret any of the neurons you see? Are they more or less interpretable than the ones returned when you took the max over std dev?
</details>


How does this change when we plot them for layer 4?


```python
layer = 4
top_layer_4_neurons = focus_cache["post", layer][:, 3:-3].std(dim=[0, 1]).argsort(descending=True)[:10]

heatmaps_blank = []
heatmaps_my = []

for neuron in top_layer_4_neurons:
    neuron = neuron.item()
    heatmaps_blank.append(calculate_neuron_output_weights(model, blank_probe_normalised, layer, neuron))
    heatmaps_my.append(calculate_neuron_output_weights(model, my_probe_normalised, layer, neuron))

imshow(
    t.stack(heatmaps_blank),
    facet_col=0,
    y=[i for i in "ABCDEFGH"],
    title=f"Cosine sim of Output weights and the blank color probe for top layer 4 neurons",
    facet_labels=[f"L4N{n.item()}" for n in top_layer_4_neurons],
    width=1600, height=300,
)

imshow(
    t.stack(heatmaps_my),
    facet_col=0,
    y=[i for i in "ABCDEFGH"],
    title=f"Cosine sim of Output weights and the my color probe for top layer 4 neurons",
    facet_labels=[f"L4N{n.item()}" for n in top_layer_4_neurons],
    width=1600, height=300,
)
```

Why is the blank output weights of the layer 4 neurons so striking?!

A cell can *only* be legal to play in if it is blank (obviously). Since calculating blankness is easy (you just check whether a move was played), the model should be able to do this in a single layer - this is what we see above.

**Question - if this is true, then what observation should we expect when we compare the neuron output weights to the unembedding weights?**

Think about this before you read on.

<details>
<summary>Answer</summary>


If this is true, then we should expect the cosine similarity of output weights and the unembedding weights to exhibit the same heatmap pattern as we see above.

In other words, these neurons which are firing strongly on blank cells are also directly writing to the residual stream, having their output used in the unembedding to increase the logit score for the blank cells which they are detecting.

</details>

Let's test this out. Because of indexing (avoiding the middle 4 squares) it will be a bit messy; we'll use our previous trick of using `.flatten()` then indexing with `stoi_indices`.


```python
layer = 4
top_layer_4_neurons = focus_cache["post", layer][:, 3:-3].std(dim=[0, 1]).argsort(descending=True)[:10]
W_U_norm = model.W_U / model.W_U.norm(dim=0, keepdim=True)
W_U_norm = W_U_norm[:, 1:] # Get rid of the passing/dummy first element
heatmaps_unembed = []

for neuron in top_layer_4_neurons:
    neuron = neuron.item()
    w_out = get_w_out(model, layer, neuron)
    # Fill in the `state` tensor with cosine sims, while skipping the middle 4 squares
    state = t.zeros((8, 8), device=device)
    state.flatten()[stoi_indices] = w_out @ W_U_norm
    heatmaps_unembed.append(state)

imshow(
    t.stack(heatmaps_unembed),
    facet_col=0,
    y=[i for i in "ABCDEFGH"],
    title=f"Cosine sim of Output weights and the unembed for top layer 4 neurons",
    facet_labels=[f"L4N{n.item()}" for n in top_layer_4_neurons],
    width=1600, height=300,
)
```

Great! We've validated our hypothesis that the neurons in this layer are directly computing "blankness" and feeding this output directly into the unembedding, in order to raise the logit score for blank cells (since being blank is necessary for being legal to play in).

In section 3️⃣, we'll do more of this kind of direct logit attribution.


### Recap of this section

We did the following:

* Defined helper functions `get_w_in`, `get_w_out` which returned particular MLP weight vectors for a given neuron.
* Defined functions `calculate_neuron_input_weights` and `calculate_neuron_output_weights` which returned the projection of a neuron's input and output weights onto the directions of a given probe (e.g. `my_probe` or `blank_probe`).
* Discovered that some neurons have very interpretable input weights, for example:
    * After comparing the input weights of `L5N1393` to the probe directions, we found that it was detecting a particular diagonal line of blank-theirs-mine, since this indicates that the blank square is legal.
        * This is interesting, and not necessarily something we could have predicted beforehand.
    * After looking at the input weights of some layer 4 neurons, we found that they were detecting whether a square was blank (since this is necessary for legality), and their output weights seemed to be directly used in the embedding to increase log probs for those blank squares.
        * This is not as interesting, because the "check if cell" is blank operation is pretty easy.


## Activation Patching


A valuable technique for tracking down the action of various circuits is activation patching. This was first introduced in [David Bau and Kevin Meng's excellent ROME paper](https://rome.baulab.info/), there called causal tracing.

The setup of activation patching is to take two runs of the model on two different inputs, the clean run and the corrupted run. The clean run outputs the correct answer and the corrupted run does not. The key idea is that we give the model the corrupted input, but then **intervene** on a specific activation and **patch** in the corresponding activation from the clean run (ie replace the corrupted activation with the clean activation), and then continue the run. And we then measure how much the output has updated towards the correct answer.

By carefully choosing clean and corrupted inputs that differ in one key detail, we can isolate which model components capture and depend on this detail.

The ability to localise is a key move in mechanistic interpretability - if the computation is diffuse and spread across the entire model, it is likely much harder to form a clean mechanistic story for what's going on. But if we can identify precisely which parts of the model matter, we can then zoom in and determine what they represent and how they connect up with each other, and ultimately reverse engineer the underlying circuit that they represent.


The diagrams below demonstrate activation patching on an abstract neural network (the nodes represent activations, and the arrows between them are weight connections).

A regular forward pass on the clean input looks like:

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/simpler-patching-1c.png" width="300">

And activation patching from a corrupted input (green) into a forward pass for the clean input (black) looks like:

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/simpler-patching-2c.png" width="440">

where the dotted line represents patching in a value (i.e. during the forward pass on the clean input, we replace node $D$ with the value it takes on the corrupted input). Nodes $H$, $G$ and $F$ are colored orange, to represent that they now follow a distribution which is not the same as clean or corrupted.


We can patch into a transformer in many different ways (e.g. values of the residual stream, the MLP, or attention heads' output - see below). We can also get even more granular by patching at particular sequence positions (not shown in diagram).

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/simpler-patching-examples.png" width="840">


### Setting up our patching

Before we patch, we need to decide what our clean and corrupted datasets will be, and create a metric for evaluating a set of logits.

Setting up clean and corrupted moves that result in similar games is non-trivial, so here we take a game and just change the most recent move from `E0` to `C0`. This turns `F0` (as a move for white to play) from legal to illegal, so let's make that logit our patching metric.

It also makes sense to have the metric be a linear function of the logit difference. This is enough to uniquely specify a metric.


First, we can plot the original and corrupted boards, to visualize this:


```python
game_index = 4
move = 20

plot_single_board(focus_games_string[game_index, :move+1], title="Original Game (black plays E0)")
plot_single_board(focus_games_string[game_index, :move].tolist()+[16], title="Corrupted Game (blank plays C0)")
```

Next, let's define our clean and corrupted inputs as token strings, to be fed into our model:


```python
clean_input = focus_games_int[game_index, :move+1].clone()
corrupted_input = focus_games_int[game_index, :move+1].clone()
corrupted_input[-1] = to_int("C0")
print("Clean:     ", ", ".join(int_to_label(corrupted_input)))
print("Corrupted: ", ", ".join(int_to_label(clean_input)))
```

```python
clean_logits, clean_cache = model.run_with_cache(clean_input)
corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_input)

clean_log_probs = clean_logits.log_softmax(dim=-1)
corrupted_log_probs = corrupted_logits.log_softmax(dim=-1)
```

### Exercise - create a patching metric

```yaml
Difficulty: 🔴🔴⚪⚪⚪
Importance: 🔵🔵🔵🔵⚪

You shouldn't spend more than 10-15 minutes on this exercise.
```

Finally, we'll create a patching metric. This is a function we'll apply to our output logits, in order to measure how much they've changed (in some important way) from their clean values.

We want our patching metric to satisfy the following conditions:

* Should have value **one** when the logits are the same as in the clean distribution.
* Should have value **zero** when the logits are the same as in the corrupted distribution.
    * *Note - we sometimes use the opposite convention. Either might be justified, depending on the context. Here, you should think of a value of 1 meaning "100% of performance is preserved", and 0 as "performance is gone".*
* Should be a **linear** function of the log probs.
    * **Important note** - this is *not* the same as being a linear function of the logits. Can you see why?
* Should just be a function of the logits for the `F0` token, at the final game move (since this is the only move that changes between clean and corrupted).
    * Note - you can index into the `d_vocab` dimension of logits using the `f0_index` variable defined below.

This should be enough for you to uniquely define your patching metric. Also, note that it should return a **scalar tensor** (this is important for the transformerlens patching functions to work).


```python
f0_index = to_int("F0")
clean_f0_log_prob = clean_log_probs[0, -1, f0_index]
corrupted_f0_log_prob = corrupted_log_probs[0, -1, f0_index]

print("Clean log prob", clean_f0_log_prob.item())
print("Corrupted log prob", corrupted_f0_log_prob.item(), "\n")
    
def patching_metric(patched_logits: Float[Tensor, "batch=1 seq=21 d_vocab=61"]):
    '''
    Function of patched logits, calibrated so that it equals 0 when performance is 
    same as on corrupted input, and 1 when performance is same as on clean input.

    Should be linear function of the logits for the F0 token at the final move.
    '''
    pass


tests.test_patching_metric(patching_metric, clean_log_probs, corrupted_log_probs)
```

<details>
<summary>Solution</summary>


```python
def patching_metric(patched_logits: Float[Tensor, "batch=1 seq=21 d_vocab=61"]):
    '''
    Function of patched logits, calibrated so that it equals 0 when performance is 
    same as on corrupted input, and 1 when performance is same as on clean input.

    Should be linear function of the logits for the F0 token at the final move.
    '''
    # SOLUTION
    patched_log_probs = patched_logits.log_softmax(dim=-1)
    return (patched_log_probs[0, -1, f0_index] - corrupted_f0_log_prob) / (clean_f0_log_prob - corrupted_f0_log_prob)
```
</details>


### Exercise - write a patching function

```yaml
Difficulty: 🔴🔴🔴🔴⚪
Importance: 🔵🔵🔵🔵⚪

This exercise is very important; getting it right shows you understand activation patching.
```

Below, you should fill in the functions `patch_attn_layer_output` and `patch_mlp_layer_output`.

To do this, you'll have to use TransformerLens hooks. A quick refresher on how they work:

* Hook functions take arguments `tensor: t.Tensor` and `hook: HookPoint`. 
* The function `model.run_with_hooks` takes arguments:
    * The tokens to run (as first argument)
    * `fwd_hooks` - a list of `(hook_name, hook_fn)` tuples. Remember that you can use `utils.get_act_name` to get hook names.

Tips:
* It's good practice to have `model.reset_hooks()` at the start of functions which add and run hooks. This is because sometimes hooks fail to be removed (if they cause an error while running). There's nothing more frustrating than fixing a hook error only to get the same error message, not realising that you've failed to clear the broken hook!
* The `HookPoint` object has method `.layer()` and attribute `.name` which can be useful in your hook functions.


```python
def patch_final_move_output(
    activation: Float[Tensor, "batch seq d_model"], 
    hook: HookPoint,
    clean_cache: ActivationCache,
) -> Float[Tensor, "batch seq d_model"]:
    '''
    Hook function which patches activations at the final sequence position.

    Note, we only need to patch in the final sequence position, because the
    prior moves in the clean and corrupted input are identical (and this is
    an autoregressive model).
    '''
    pass

def get_act_patch_resid_pre(
    model: HookedTransformer, 
    corrupted_input: Float[Tensor, "batch pos"], 
    clean_cache: ActivationCache, 
    patching_metric: Callable[[Float[Tensor, "batch seq d_vocab"]], Float[Tensor, ""]]
) -> Float[Tensor, "2 n_layers"]:
    '''
    Returns an array of results, corresponding to the results of patching at
    each (attn_out, mlp_out) for all layers in the model.
    '''
    pass


```

<details>
<summary>Solution</summary>


```python
def patch_final_move_output(
    activation: Float[Tensor, "batch seq d_model"], 
    hook: HookPoint,
    clean_cache: ActivationCache,
) -> Float[Tensor, "batch seq d_model"]:
    '''
    Hook function which patches activations at the final sequence position.

    Note, we only need to patch in the final sequence position, because the
    prior moves in the clean and corrupted input are identical (and this is
    an autoregressive model).
    '''
    # SOLUTION
    activation[0, -1, :] = clean_cache[hook.name][0, -1, :]
    return activation

def get_act_patch_resid_pre(
    model: HookedTransformer, 
    corrupted_input: Float[Tensor, "batch pos"], 
    clean_cache: ActivationCache, 
    patching_metric: Callable[[Float[Tensor, "batch seq d_model"]], Float[Tensor, ""]]
) -> Float[Tensor, "2 n_layers"]:
    '''
    Returns an array of results, corresponding to the results of patching at
    each (attn_out, mlp_out) for all layers in the model.
    '''
    # SOLUTION
    model.reset_hooks()
    results = t.zeros(2, model.cfg.n_layers, device=device, dtype=t.float32)
    hook_fn = partial(patch_final_move_output, clean_cache=clean_cache)

    for i, activation in enumerate(["attn_out", "mlp_out"]):
        for layer in tqdm(range(model.cfg.n_layers)):
            patched_logits = model.run_with_hooks(
                corrupted_input, 
                fwd_hooks = [(utils.get_act_name(activation, layer), hook_fn)], 
            )
            results[i, layer] = patching_metric(patched_logits)

    return results
```
</details>


```python
patching_results = get_act_patch_resid_pre(model, corrupted_input, clean_cache, patching_metric)

line(patching_results, title="Layer Output Patching Effect on F0 Log Prob", line_labels=["attn", "mlp"], width=750)
```

<details>
<summary>Spoiler - what results you should get</summary>

We can see that most layers just don't matter! But MLP0, MLP5, MLP6 and Attn7 do! My next steps would be to get more fine grained and to patch in individual neurons and see how far I can zoom in on *why* those layers matter - ideally at the level of understanding how these neurons compose with each other and the changed the embedding, using the fact that most of the model just doesn't matter here. And then to compare this data to the above techniques for understanding neurons. If you want to go off and explore this, that would be a great exercise at this point (or to return to at the end of the exercises).

It's not surprising that the attention layers are fairly unimportant - attention specialises in moving information between token positions, we've only changed the information at the current position! (Attention does have the ability to operate on the current position, but that's not the main thing it does).
</details>


### Recap of this section

We did the following:

* Learned how **activation patching** worked.
* Constructed the following datasets for patching:
    * Clean distribution = unaltered game,
    * Corrupted distribution = game with a single move flipped (changing th legality of a square),
* Looked at the effect on patching at the **output of attention and MLP layers** to see which ones changed the output significantly.
    * We found a handful of the MLP layers, and the final attention layer, mattered.
        * Attention layers mostly not mattering was unsurprising, since attention's main job is to move around information rather than operate on it.
    * If we wanted, we could get more granular at this point, and explore which neurons in these layers had a significant effect.

""", unsafe_allow_html=True)
