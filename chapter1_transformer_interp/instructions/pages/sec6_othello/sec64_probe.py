import streamlit as st

def section():

    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#training-setup'>Training setup</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-fill-in-the-missing-code-below'><b>Exercise</b> - fill in the missing code below</a></li>
    </ul></li>
</ul>""", unsafe_allow_html=True)

    st.markdown(
r"""
# Training a Probe

> ### Learning objectives
> 
> - Learn how to set up and train a linear probe

In this final section, we'll return to the linear probe from earlier, but discuss how you might go about training it from scratch.

We won't be doing a full-scale training run here, instead we'll just look at a small example involving the `board_seqs_int_small.npy` datasets we've already used (the actual probe we've been using in these exercises was trained using a larger `board_seqs_int.pth` dataset).

Note - if you're an ARENA participant who did the material on model training during the first week (or in the "transformer from scratch" material), this should all be familiar to you. I'd recommend doing that section before this one, unless you already have experience writing standard ML training loops.

One important thing to make clear: we're not actually training our transformer model here. If this was a standard training loop, we'd run our model in training mode and update its gradients in a way which reduces the cross entropy loss between its logit output and the true black/white/blank labels. Instead, we're **running our model in inference mode, caching its residual stream values, applying our probe to these values, and then updating the weights of our probe in a way which reduces the cross entropy loss between our probe's output and the true mine/theirs/blank labels.**

```python
imshow(
    focus_states[0, :16],
    facet_col=0,
    facet_col_wrap=8,
    facet_labels=[f"Move {i}" for i in range(1, 17)],
    title="First 16 moves of first game",
    color_continuous_scale="Greys",
)
```

## Training setup

We'll first create a **dataclass** to store our probe training args. This is a great way to keep all our variables in one place (and also it works well with VSCode's autocompletion features!). Also, note a cool feature of dataclasses - you can define attributes in terms of previous attributes (e.g. see the `length` attribute).

We've also included a `setup_linear_probe` method, which will give us a randomly initialized probe with appropriately normalized weights.


```python
@dataclass
class ProbeTrainingArgs():

    # Which layer, and which positions in a game sequence to probe
    layer: int = 6
    pos_start: int = 5
    pos_end: int = model.cfg.n_ctx - 5
    length: int = pos_end - pos_start
    alternating: Tensor = t.tensor([1 if i%2 == 0 else -1 for i in range(length)], device=device)

    # Game state (options are blank/mine/theirs)
    options: int = 3
    rows: int = 8
    cols: int = 8

    # Standard training hyperparams
    max_epochs: int = 8
    num_games: int = 50000

    # Hyperparams for optimizer
    batch_size: int = 256
    lr: float = 1e-4
    betas: Tuple[float, float] = (0.9, 0.99)
    wd: float = 0.01

    # Saving & logging
    probe_name: str = "main_linear_probe"
    wandb_project: Optional[str] = 'othellogpt-probe'
    wandb_name: Optional[str] = None

    # The modes are "black to play / odd moves", "white to play / even moves", and "all moves"
    modes = 3

    # Code to get randomly initialized probe
    def setup_linear_probe(self, model: HookedTransformer):
        linear_probe = t.randn(
            self.modes, model.cfg.d_model, self.rows, self.cols, self.options, requires_grad=False, device=device
        ) / np.sqrt(model.cfg.d_model)
        linear_probe.requires_grad = True
        return linear_probe
```

A reminder of what some of these mean:
* `modes` refers to "black to play/odd moves", "white to play/even moves", and "all moves". In the previous exercises, we only ever used "black to play" (this choice didn't really matter, since the model is detecting "my/their color" rather than "black/white").
* `options` (for our linear probe) refers to "empty", "black" and "white". After we've trained it, we'll create a version with "empty", "theirs" and "mine".

Now for our main block of code - we'll write a class for training our linear probe.


### Exercise - fill in the missing code below

```yaml
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 30-40 minutes on this exercise.

There are several steps to this exercise, so after trying for some time you're recommended to look at the solution.
```

We've just left the `training_step` function incomplete, so that's the one you need to fill in. It should return the loss which you backpropagate on (you can see the code which actually performs the backprop algorithm below).

The `batch` object in your training step is a tuple of `(games_int, state_stack_one_hot)`. The `games_int` object has shape `(batch_size=256, full_game_len=60)`, and is the thing you feed into your model. The `state_stack_one_hot` object has shape `(batch_size=256, game_len, rows=8, cols=8, options=3)`, and used when calculating your loss (i.e. it's the correct labels for your game, but in one-hot format). Note that `game_len` is less than `full_game_len=60`, because we're removing the first and last 5 games.

* Run the model in inference mode, and get the residual stream (post attn and MLP).
    * Note - remember to only use `games_int[:, :-1]` as input, since we don't have a label for the final move.
        * Hint - you might want to use the `name_filter` argument of `run_with_cache` so you only get `resid_post`.
    * Also, we only want to take the slice `pos_start: pos_end` of the cached values.
* Multiply the residual stream values with the linear probe values and sum over the hidden dimension.
    * The result should be a tensor of shape `(modes=3, batch, seq_pos, rows=8, cols=8, options=3)`.
* Convert this tensor of logits into a tensor of log probabilities (over `options=3`).
* Multiply this tensor by `state_stack_one_hot` and sum over the final dimensions (the `options` dimension).
    * This is equivalent to indexing along the `options` dimension to choose the log prob corresponding to the correct move (remember that cross entropy loss is negative log prob).
* Take the mean over the batch dimension.
* Calculate `loss_even`, `loss_odd` and `loss_all`.
    * These are the average negative log probs for the correct move, for each of the three modes: even moves, odd moves, and all moves aggregated.
    * Make sure you index these correctly (e.g. `loss_even` should be the `mode=0`-th dimension of your linear probe tensor).
    * Note - this is because the zeroth mode ("black to play / odd moves") are the even moves in the moves vector, if we're indexing from zero.
* Return the sum of these three losses.

Once you've finished implementing this function, you can run the code below. You'll know it's working if your training loss decreases at a reasonable rate (it should start at around 300, and will probably drop down to somewhere between 30 and 100 by the end of your training loop).



```python
class LinearProbeTrainer:
    def __init__(self, model: HookedTransformer, args: ProbeTrainingArgs):
        self.model = model
        self.args = args
        self.linear_probe = args.setup_linear_probe(model)

    def training_step(self, indices: Int[Tensor, "game_idx"]) -> t.Tensor:

        # Get the game sequences and convert them to state stacks
        games_int = board_seqs_int[indices.cpu()]
        games_str = board_seqs_string[indices.cpu()]
        state_stack = t.stack([t.tensor(seq_to_state_stack(game_str.tolist())) for game_str in games_str])
        state_stack = state_stack[:, self.args.pos_start: self.args.pos_end, :, :]
        state_stack_one_hot = state_stack_to_one_hot(state_stack).to(device)
        batch_size = self.args.batch_size
        game_len = self.args.length

        # games_int = tensor of game sequences, each of length 60
        # This is the input to our model
        assert isinstance(games_int, Int[Tensor, f"batch={batch_size} full_game_len=60"])

        # state_stack_one_hot = tensor of one-hot encoded states for each game
        # We'll multiply this by our probe's estimated log probs along the `options` dimension, to get probe's estimated log probs for the correct option
        assert isinstance(state_stack_one_hot, Int[Tensor, f"batch={batch_size} game_len={game_len} rows=8 cols=8 options=3"])

        pass


    def shuffle_training_indices(self):
        '''
        Returns the tensors you'll use to index into the training data.
        '''
        n_indices = self.args.num_games - (self.args.num_games % self.args.batch_size)
        full_train_indices = t.randperm(self.args.num_games)[:n_indices]
        full_train_indices = einops.rearrange(full_train_indices, "(batch_idx game_idx) -> batch_idx game_idx", game_idx=self.args.batch_size)
        return full_train_indices


    def train(self):
        
        self.step = 0
        wandb.init(project=args.wandb_project, name=args.wandb_name, config=args)

        optimizer = t.optim.AdamW([self.linear_probe], lr=self.args.lr, betas=self.args.betas, weight_decay=self.args.wd)

        for epoch in range(args.max_epochs):
            full_train_indices = trainer.shuffle_training_indices()
            progress_bar = tqdm(full_train_indices)
            for indices in progress_bar:
                loss = trainer.training_step(indices)
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()
                progress_bar.set_description(f"Loss = {loss:.4f}")

        wandb.finish()


args = ProbeTrainingArgs()
trainer = LinearProbeTrainer(model, args)
trainer.train()
```

<details>
<summary>Solution</summary>

You can see the wandb report for this code [here](https://api.wandb.ai/links/callum-mcdougall/bw7ot1rg).

```python
class LinearProbeTrainer:
    def __init__(self, model: HookedTransformer, args: ProbeTrainingArgs):
        self.model = model
        self.args = args
        self.linear_probe = args.setup_linear_probe(model)

    def training_step(self, indices: Int[Tensor, "game_idx"]) -> t.Tensor:

        # Get the game sequences and convert them to state stacks
        games_int = board_seqs_int[indices.cpu()]
        games_str = board_seqs_string[indices.cpu()]
        state_stack = t.stack([t.tensor(seq_to_state_stack(game_str.tolist())) for game_str in games_str])
        state_stack = state_stack[:, self.args.pos_start: self.args.pos_end, :, :]
        state_stack_one_hot = state_stack_to_one_hot(state_stack).to(device)
        batch_size = self.args.batch_size
        game_len = self.args.length

        # games_int = tensor of game sequences, each of length 60
        # This is the input to our model
        assert isinstance(games_int, Int[Tensor, f"batch={batch_size} full_game_len=60"])

        # state_stack_one_hot = tensor of one-hot encoded states for each game
        # We'll multiply this by our probe's estimated log probs along the `options` dimension, to get probe's estimated log probs for the correct option
        assert isinstance(state_stack_one_hot, Int[Tensor, f"batch={batch_size} game_len={game_len} rows=8 cols=8 options=3"])

        # SOLUTION
        with t.inference_mode():
            _, cache = model.run_with_cache(
                games_int[:, :-1].to(device),
                return_type=None,
                names_filter=lambda name: name.endswith("resid_post")
            )
            resid_post = cache["resid_post", self.args.layer][:, self.args.pos_start: self.args.pos_end]

        probe_out = einops.einsum(
            resid_post,
            self.linear_probe,
            "batch pos d_model, modes d_model rows cols options -> modes batch pos rows cols options",
        )

        probe_log_probs = probe_out.log_softmax(-1)
        probe_correct_log_probs = einops.reduce(
            probe_log_probs * state_stack_one_hot,
            "modes batch pos rows cols options -> modes pos rows cols",
            "mean"
        ) * self.args.options # Multiply to correct for the mean over options
        loss_even = -probe_correct_log_probs[0, 0::2].mean(0).sum() # note that "even" means odd in the game framing, since we offset by 5 moves lol
        loss_odd = -probe_correct_log_probs[1, 1::2].mean(0).sum()
        loss_all = -probe_correct_log_probs[2, :].mean(0).sum()
        loss = loss_even + loss_odd + loss_all

        wandb.log({"loss_even": loss_even.item(), "loss_odd": loss_odd.item(), "loss_all": loss_all.item(), "loss": loss.item()}, step=self.step)
        self.step += 1

        return loss


    def shuffle_training_indices(self):
        '''
        Returns the tensors you'll use to index into the training data.
        '''
        n_indices = self.args.num_games - (self.args.num_games % self.args.batch_size)
        full_train_indices = t.randperm(self.args.num_games)[:n_indices]
        full_train_indices = einops.rearrange(full_train_indices, "(batch_idx game_idx) -> batch_idx game_idx", game_idx=self.args.batch_size)
        return full_train_indices


    def train(self):
        
        self.step = 0
        wandb.init(project=args.wandb_project, name=args.wandb_name, config=args)

        optimizer = t.optim.AdamW([self.linear_probe], lr=self.args.lr, betas=self.args.betas, weight_decay=self.args.wd)

        for epoch in range(args.max_epochs):
            full_train_indices = trainer.shuffle_training_indices()
            progress_bar = tqdm(full_train_indices)
            for indices in progress_bar:
                loss = trainer.training_step(indices)
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()
                progress_bar.set_description(f"Loss = {loss:.4f}")

        wandb.finish()
```
</details>


Finally, let's make the same accuracy plot from before, and see how well it works. (Note - if you're having trouble training your model, you can take some of this code and adapt it to include in your training function above, i.e. logging accuracy as well as training loss.)

```python
black_to_play_index = 0
white_to_play_index = 1
blank_index = 0
their_index = 1
my_index = 2

# Creating values for linear probe (converting the "black/white to play" notation into "me/them to play")
my_linear_probe = t.zeros(cfg.d_model, rows, cols, options, device=device)
my_linear_probe[..., blank_index] = 0.5 * (trainer.linear_probe[black_to_play_index, ..., 0] + trainer.linear_probe[white_to_play_index, ..., 0])
my_linear_probe[..., their_index] = 0.5 * (trainer.linear_probe[black_to_play_index, ..., 1] + trainer.linear_probe[white_to_play_index, ..., 2])
my_linear_probe[..., my_index] = 0.5 * (trainer.linear_probe[black_to_play_index, ..., 2] + trainer.linear_probe[white_to_play_index, ..., 1])

# Getting the probe's output, and then its predictions
probe_out = einops.einsum(
    focus_cache["resid_post", 6], my_linear_probe,
    "game move d_model, d_model row col options -> game move row col options"
)
probe_out_value = probe_out.argmax(dim=-1)

# Getting the correct answers in the odd cases
correct_middle_odd_answers = (probe_out_value == focus_states_flipped_value[:, :-1])[:, 5:-5:2]
accuracies_odd = einops.reduce(correct_middle_odd_answers.float(), "game move row col -> row col", "mean")

# Getting the correct answers in all cases
correct_middle_answers = (probe_out_value == focus_states_flipped_value[:, :-1])[:, 5:-5]
accuracies = einops.reduce(correct_middle_answers.float(), "game move row col -> row col", "mean")

plot_square_as_board(
    1 - t.stack([accuracies_odd, accuracies], dim=0), 
    title="Average Error Rate of Linear Probe", 
    facet_col=0, facet_labels=["Black to Play moves", "All Moves"], 
    zmax=0.25, zmin=-0.25
)
```

If you did this correctly, you should get comparable performance to the original probes we used above (maybe not quite as good, depending on how long you trained for). As a reference, when I trained according to the default values in `args`, my loss came down to about 38 (starting at 300) and error rate was below 5% everywhere (and was largest on the edges and corners).

With your trained probe, can you reproduce some of our results with the probes you trained? Can you find any interesting neurons, and figure out what they're doing?

""", unsafe_allow_html=True)

