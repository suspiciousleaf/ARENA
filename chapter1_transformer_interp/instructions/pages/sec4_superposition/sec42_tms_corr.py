import streamlit as st

def section():
    st.sidebar.markdown(r"""
## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#superposition-with-correlation'>Superposition with correlation</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-generate-correlated-batch'><b>Exercise</b> - implement <code>generate_correlated_batch</code></a></li>
        <li><a class='contents-el' href='#exercise-generate-more-correlated-feature-plots'><b>Exercise</b> - generate more correlated feature plots</a></li>
    </ul></li>
</ul>""", unsafe_allow_html=True)
    
    st.markdown(
r"""
# TMS: Correlated / Anticorrelated Features

## Superposition with correlation

One major thing we haven't considered in our experiments is **correlation**. We could guess that superposition is even more common when features are **anticorrelated** (for a similar reason as why it's more common when features are sparse). Most real-world features are anticorrelated (e.g. the feature "this is a sorted Python list" and "this is some text in an edgy teen vampire romance novel" are probably anticorrelated - that is, unless you've been reading some pretty weird fanfics).

In this section, you'll define a new data-generating function for correlated features, and run the same experiments as in the first section.

### Exercise - implement `generate_correlated_batch`

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª
Importance: ðŸ”µðŸ”µâšªâšªâšª

You should spend up to 20-40 minutes on this exercise.

The exercise itself is not conceptually important, and it is a bit fiddly / delicate, so you should definitely look at the solutions if you get stuck. Understanding the results and why they occur is more important than the implementation!
```

You should now fill in the three methods `generate_correlated_features`, `generate_anticorrelated_features` and `generate_uncorrelated_features` in the `Model` class, which are created to generate correlated / anticorrelated data. A summary of what you will have to do:

* The `generate_correlated_features` function returns a tensor of shape `(batch_size, n_instances, n_features)`, where `n_features = 2 * n_correlated_pairs`, and each feature is correlated with the next feature.
    * In other words, each `output[i, j, 2k]` and `output[i, j, 2k+1]` are correlated: one is non-zero iff the other is non-zero.
    * Hint - create a tensor of random seeds of shape `(batch_size, n_instances, n_correlated_pairs)`, then use `einops.repeat`.
* The `generate_anticorrelated_features` function returns a tensor of shape `(batch_size, n_instances, n_features)`, where `n_features = 2 * n_anticorrelated_pairs`, and each feature is anticorrelated with the next feature.
    * In other words, each `output[i, j, 2k]` and `output[i, j, 2k+1]` are anticorrelated: one is non-zero iff the other is zero.
    * Hint - create a tensor of random seeds of shape `(batch_size, n_instances, n_anticorrelated_pairs)` which determines whether one of the features in a given pair is zero or not, and then for the non-zero features you can use another random seed tensor to determine whether the first or the second feature is non-zero.
    * Note - if `p` is the feature probability, then you should have the probability of any given pair of anticorrelated features being present as `2p` - this is so that any single feature has probability `2p * (1/2) = p` of being present.
* The `generate_uncorrelated_features` function returns a tensor of shape `(batch_size, n_instances, n_uncorrelated)`.
    * This should be exactly the same function as your current version of `generate_batch`.

For these functions you can assume that the `model.feature_probability` is the same for all features (although it might vary across instances). Since `model.feature_probability` automatically gets broadcasted to shape `(n_instances, n_features)` during intialization, you can handle this by just indexing the probability for the first feature with e.g. `[:, 0]` to get a vector of length `n_instances`.

For more details, you can read the [experimental details in Anthropic's paper](https://transformer-circuits.pub/2022/toy_model/index.html#geometry-correlated-setup), where they describe how they setup correlated and anticorrelated sets.

You should fill in the functions below. Note that we've also given you a new `generate_batch` function, which calculates how many correlated / anticorrelated / uncorrelated features we need, and creates a full batch by concatenating the outputs of these three functions.

```python
def generate_correlated_features(self: Model, batch_size, n_correlated_pairs) -> Float[Tensor, "batch_size instances features"]:
    '''
    Generates a batch of correlated features.
    Each output[i, j, 2k] and output[i, j, 2k + 1] are correlated, i.e. one is present iff the other is present.
    '''
    pass


def generate_anticorrelated_features(self: Model, batch_size, n_anticorrelated_pairs) -> Float[Tensor, "batch_size instances features"]:
    '''
    Generates a batch of anti-correlated features.
    Each output[i, j, 2k] and output[i, j, 2k + 1] are anti-correlated, i.e. one is present iff the other is absent.
    '''
    pass


def generate_uncorrelated_features(self: Model, batch_size, n_uncorrelated) -> Float[Tensor, "batch_size instances features"]:
    '''
    Generates a batch of uncorrelated features.
    '''
    pass


def generate_batch(self: Model, batch_size):
    '''
    Generates a batch of data, with optional correslated & anticorrelated features.
    '''
    n_uncorrelated = self.cfg.n_features - 2 * self.cfg.n_correlated_pairs - 2 * self.cfg.n_anticorrelated_pairs
    data = []
    if self.cfg.n_correlated_pairs > 0:
        data.append(self.generate_correlated_features(batch_size, self.cfg.n_correlated_pairs))
    if self.cfg.n_anticorrelated_pairs > 0:
        data.append(self.generate_anticorrelated_features(batch_size, self.cfg.n_anticorrelated_pairs))
    if n_uncorrelated > 0:
        data.append(self.generate_uncorrelated_features(batch_size, n_uncorrelated))
    batch = t.cat(data, dim=-1)
    return batch


Model.generate_correlated_features = generate_correlated_features
Model.generate_anticorrelated_features = generate_anticorrelated_features
Model.generate_uncorrelated_features = generate_uncorrelated_features
Model.generate_batch = generate_batch
```

<details>
<summary>Solution</summary>

```python
def generate_correlated_features(self: Model, batch_size, n_correlated_pairs) -> Float[Tensor, "batch_size instances features"]:
    '''
    Generates a batch of correlated features.
    Each output[i, j, 2k] and output[i, j, 2k + 1] are correlated, i.e. one is present iff the other is present.
    '''
    feat = t.rand((batch_size, self.cfg.n_instances, 2 * n_correlated_pairs), device=self.W.device)
    feat_set_seeds = t.rand((batch_size, self.cfg.n_instances, n_correlated_pairs), device=self.W.device)
    feat_set_is_present = feat_set_seeds <= self.feature_probability[:, [0]]
    feat_is_present = einops.repeat(feat_set_is_present, "batch instances features -> batch instances (features pair)", pair=2)
    return t.where(feat_is_present, feat, 0.0)


def generate_anticorrelated_features(self: Model, batch_size, n_anticorrelated_pairs) -> Float[Tensor, "batch_size instances features"]:
    '''
    Generates a batch of anti-correlated features.
    Each output[i, j, 2k] and output[i, j, 2k + 1] are anti-correlated, i.e. one is present iff the other is absent.
    '''
    feat = t.rand((batch_size, self.cfg.n_instances, 2 * n_anticorrelated_pairs), device=self.W.device)
    feat_set_seeds = t.rand((batch_size, self.cfg.n_instances, n_anticorrelated_pairs), device=self.W.device)
    first_feat_seeds = t.rand((batch_size, self.cfg.n_instances, n_anticorrelated_pairs), device=self.W.device)
    feat_set_is_present = feat_set_seeds <= 2 * self.feature_probability[:, [0]]
    first_feat_is_present = first_feat_seeds <= 0.5
    first_feats = t.where(feat_set_is_present & first_feat_is_present, feat[:, :, :n_anticorrelated_pairs], 0.0)
    second_feats = t.where(feat_set_is_present & (~first_feat_is_present), feat[:, :, n_anticorrelated_pairs:], 0.0)
    return einops.rearrange(t.concat([first_feats, second_feats], dim=-1), "batch instances (pair features) -> batch instances (features pair)", pair=2)


def generate_uncorrelated_features(self: Model, batch_size, n_uncorrelated) -> Float[Tensor, "batch_size instances features"]:
    '''
    Generates a batch of uncorrelated features.
    '''
    feat = t.rand((batch_size, self.cfg.n_instances, n_uncorrelated), device=self.W.device)
    feat_seeds = t.rand((batch_size, self.cfg.n_instances, n_uncorrelated), device=self.W.device)
    feat_is_present = feat_seeds <= self.feature_probability[:, [0]]
    return t.where(feat_is_present, feat, 0.0)


def generate_batch(self: Model, batch_size):
    '''
    Generates a batch of data, with optional correslated & anticorrelated features.
    '''
    n_uncorrelated = self.cfg.n_features - 2 * self.cfg.n_correlated_pairs - 2 * self.cfg.n_anticorrelated_pairs
    data = []
    if self.cfg.n_correlated_pairs > 0:
        data.append(self.generate_correlated_features(batch_size, self.cfg.n_correlated_pairs))
    if self.cfg.n_anticorrelated_pairs > 0:
        data.append(self.generate_anticorrelated_features(batch_size, self.cfg.n_anticorrelated_pairs))
    if n_uncorrelated > 0:
        data.append(self.generate_uncorrelated_features(batch_size, n_uncorrelated))
    batch = t.cat(data, dim=-1)
    return batch
```

</details>

The code below tests your function, by generating a large number of batches and measuring them statistically.

```python
cfg = Config(
    n_instances = 30,
    n_features = 4,
    n_hidden = 2,
    n_correlated_pairs = 1,
    n_anticorrelated_pairs = 1,
)

feature_probability = 10 ** -t.linspace(0.5, 1, cfg.n_instances).to(device)

model = Model(
    cfg = cfg,
    device = device,
    feature_probability = einops.rearrange(feature_probability, "instances -> instances ()")
)

# Generate a batch of 4 features: first 2 are correlated, second 2 are anticorrelated
batch = model.generate_batch(batch_size=100_000)
corr0, corr1, anticorr0, anticorr1 = batch.unbind(dim=-1)
corr0_is_active = corr0 != 0
corr1_is_active = corr1 != 0
anticorr0_is_active = anticorr0 != 0
anticorr1_is_active = anticorr1 != 0

assert (corr0_is_active == corr1_is_active).all(), "Correlated features should be active together"
assert (corr0_is_active.float().mean(0) - feature_probability).abs().mean() < 0.002, "Each correlated feature should be active with probability `feature_probability`"

assert (anticorr0_is_active & anticorr1_is_active).int().sum().item() == 0, "Anticorrelated features should never be active together"
assert (anticorr0_is_active.float().mean(0) - feature_probability).abs().mean() < 0.002, "Each anticorrelated feature should be active with probability `feature_probability`"
```

We can also visualise these features, in the form of a bar chart. You should see the correlated features always co-occurring, and the anticorrelated features never co-occurring.

<details>
<summary>What you should see when you run the code below</summary>


<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/bar-cooccur.png" width="800">

</details>

```python
# Generate a batch of 4 features: first 2 are correlated, second 2 are anticorrelated
batch = model.generate_batch(batch_size = 1)
correlated_feature_batch, anticorrelated_feature_batch = batch[:, :, :2], batch[:, :, 2:]

# Plot correlated features
plot_correlated_features(correlated_feature_batch, title="Correlated Features: should always co-occur")
plot_correlated_features(anticorrelated_feature_batch, title="Anti-correlated Features: should never co-occur")
```

Now, let's try training our model & visualising features in 2D, when we have 2 pairs of correlated features (matching the [first figure](https://transformer-circuits.pub/2022/toy_model/index.html#geometry-organization) in the Anthropic paper).

```python
cfg = Config(
    n_instances = 5,
    n_features = 4,
    n_hidden = 2,
    n_correlated_pairs = 2,
    n_anticorrelated_pairs = 0,
)

# All same importance, very low feature probabilities (ranging from 5% down to 0.25%)
importance = t.ones(cfg.n_features, dtype=t.float, device=device)
importance = einops.rearrange(importance, "features -> () features")
feature_probability = (400 ** -t.linspace(0.5, 1, cfg.n_instances))
feature_probability = einops.rearrange(feature_probability, "instances -> instances ()")

model = Model(
    cfg = cfg,
    device = device,
    importance = importance,
    feature_probability = feature_probability,
)
model.optimize()

plot_features_in_2d(
    model.W,
    colors = ["blue"] * 2 + ["limegreen"] * 2, # when colors is a list of strings, it's assumed to be the colors of features
    title = "Correlated feature sets are represented in local orthogonal bases",
    subplot_titles = [f"1 - S = {i:.3f}" for i in model.feature_probability[:, 0]],
)
```

### Exercise - generate more correlated feature plots

```c
Difficulty: ðŸ”´ðŸ”´âšªâšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to ~10 minutes on this exercise.

It should just involve changing the parameters in your code above.
```

You should now plot the second and third figures from the paper. You may not get exactly the same results as the paper, but they should still roughly match (e.g. you should see no antipodal pairs in the code above, but you should see at least some when you test the anticorrelated sets, even if not all of them are antipodal). You can look at the solutions colab to see some examples.

<details>
<summary>Question - for the anticorrelated feature plots, you'l have to increase the feature probability to something like ~10%, or else you won't always form antipodal pairs. Why do you think this is?</summary>

If sparsity is small / feature prob is large, then interference between the two pairs of anticorrelated features is a problem. If two features from different pairs are in the same subspace (because they're antipodal) the model is more likely to keep looking for a better solution.

On the other hand, if sparsity is very large / feature probability is close to zero, then the negative effect of interference is much smaller. So the difference in loss between the solutions where the antipodal pairs are / aren't the same as the anticorrelated pairs is much smaller, and the model is more likely to just settle on whichever solution it finds first.

</details>

```python
# YOUR CODE HERE - generate more correlated feature plots
```

See the [Colab notebook](https://colab.research.google.com/drive/1mHKZpkhYAr0WWAQo2Y6pXL08yNfJHOVx?usp=sharing) for some sample code & outputs for these plots.

""", unsafe_allow_html=True)