import streamlit as st

def section():
    st.sidebar.markdown(r"""

## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#introduction'>Introduction</a></li>
    <li class='margtop'><a class='contents-el' href='#content-learning-objectives'>Content & Learning Objectives</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#1-tms-superposition-in-a-nonprivileged-basis'>TMS: Superposition in a Nonprivileged Basis</a></li>
        <li><a class='contents-el' href='#2-tms-correlated-anticorrelated-features'>TMS: Correlated / Anticorrelated Features</a></li>
        <li><a class='contents-el' href='#3-tms-superposition-in-a-privileged-basis'>TMS: Superposition in a Privileged Basis</a></li>
        <li><a class='contents-el' href='#4-feature-geometry'>Feature Geometry</a></li>
        <li><a class='contents-el' href='#5-double-descent-superposition'>Double Descent & Superposition</a></li>
        <li><a class='contents-el' href='#6-sparse-autoencoders-in-toy-models'>Sparse Autoencoders in Toy Models</a></li>
        <li><a class='contents-el' href='#7-sparse-autoencoders-in-languagfe-models'>Sparse Autoencoders in Language Models</a></li>
        <li><a class='contents-el' href='#8-bonus'>Bonus</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#reading-material'>Reading Material</a></li>
    <li class='margtop'><a class='contents-el' href='#questions'>Questions</a></li>
    <li class='margtop'><a class='contents-el' href='#setup'>Setup</a></li>
</ul>""", unsafe_allow_html=True)

    st.markdown(
r"""
# [1.4] Superposition & Sparse Autoencoders

### Colab: [**exercises**](https://colab.research.google.com/drive/1DSqMihSkocF4WtLtazpIpZoUzrlu3Vdb?usp=sharing) | [**solutions**](https://colab.research.google.com/drive/1mHKZpkhYAr0WWAQo2Y6pXL08yNfJHOVx?usp=sharing)

Please send any problems / bugs on the `#errata` channel in the [Slack group](https://join.slack.com/t/arena-uk/shared_invite/zt-28h0xs49u-ZN9ZDbGXl~oCorjbBsSQag), and ask any questions on the dedicated channels for this chapter of material.

You can toggle dark mode from the buttons on the top-right of this page.
                
Links to other chapters: [**(0) Fundamentals**](https://arena3-chapter0-fundamentals.streamlit.app/), [**(2) RL**](https://arena3-chapter2-rl.streamlit.app/).

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/galaxies.png" width="350">

## Introduction

Superposition is a crucially important concept for understanding how transformers work. A definition from Neel Nanda's glossary:

> Superposition is when a model represents more than n features in an $n$-dimensional activation space. That is, features still correspond to directions, but **the set of interpretable directions is larger than the number of dimensions**.

Why should we expect something like this to happen? In general, the world has way more features than the model has dimensions of freedom, and so we can't have a one-to-one mapping between features and values in our model. But the model has to represent these features somehow. Hence, it comes up with techniques for cramming multiple features into fewer dimensions (at the cost of adding noise and interference between features).

<br>

The exercises here are split into three main sections (described in more detail below). The first three sections introduce Anthropic's toy models of superposition, and show how this model can illustrate important concepts about superposition. The next two sections dive deeper into specific lines of research that grew from the toy model setup. The last two sections introduce **sparse autoencoders**, an exciting line of research which hopes to solve the barriers to interpretability which superposition presents.

**These exercises are much longer than any other set in the transformer interpretability chapter**, so if you want to attempt more than one exercise set during the course, we recommend being liberal with the sections you skip (in particular, we recommend skipping sections 4 and 5 at least, unless you really want to dive deep into these topics).

Unlike many other topics in this chapter, there's quite a bit of theory which needs to be understood before we start making inferences from the results of our coding experiments. A key point to make here is that, perhaps more so than any other section in this chapter, we really don't understand superposition that well at all! It's hard to point to the seminal work in this field because we don't really know what the critical new insights will look like. That being said, we hope this material gives you enough directions to pursue when you're finished!

## Content & Learning Objectives

### 1️⃣, 2️⃣, 3️⃣ Toy Models of Superposition: basics

In these sections, you'll be exposed to Anthropic's toy models of superposition. You'll learn about the key ideas of superposition and why it presents a problem for interpreting neural networks, and you'll experiment with different ways of visualising superposition. You'll also see how properties like sparsity and importance of features affect the degree and nature of superposition.

> ##### Learning objectives
>
> - Understand the concept of superposition, and why models need to do it
> - Understand the difference between superposition and polysemanticity
> - Understand the difference between neuron and bottleneck superposition (or computational and representational superposition)
> - Build & train the toy model from Anthropic's paper, replicate the main results
> - See how superposition varies when you change the following characteristics of the features:
>   - Importance
>   - Sparsity
>   - Correlation

<br>

### 4️⃣, 5️⃣ Toy Models of Superposition: extensions

The next two sections take deep dives into some extension topics from the toy models paper: namely the geometry of superposition, and deep double descent. We don't consider these exercises as essential as the previous ones, but they're still very interesting and worth doing if you have time, or just want to dive more deeply into superposition.

> ##### Learning objectives
>
> - Understand the geometric intuitions behind superposition, and how they relate to the more general ideas of superposition in larger models
> - Understand how superposition might relate to double descent, and the idea of memorizing vs generalizing solutions

<br>

### 6️⃣, 7️⃣ Sparse AutoEncoders

In this last section, you'll learn about sparse autoencoders, and how they might help us resolve problems of superposition. You'll first train an SAE on the toy model setup from earlier sections, then conclude by investigating an SAE trained on a real language model.

> ##### Learning objectives
>
> - Learn about sparse autoencoders, and how they might be used to disentangle features represented in superposition
> - Train your own SAEs on the toy models from earlier sections, and visualise the feature reconstruction process
> - Interpret SAEs trained on real language models, and see what kinds of features they can find which are dense in the neuron basis

<br>

### 8️⃣ Bonus

We end with a section of suggested bonus material & paper replications, like usual.

## Reading Material

* [200 COP in MI: Exploring Polysemanticity and Superposition](https://www.alignmentforum.org/posts/o6ptPu7arZrqRCxyz/200-cop-in-mi-exploring-polysemanticity-and-superposition), <b>15 mins</b>
    * Read the post, up to and including "Tips" (although some parts of it might make more sense after you've read the other things here).
* Neel Nanda's [Dynalist notes on superposition](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=3br1psLRIjQCOv2T4RN3V6F2), <b>10 mins</b>
    * These aren't long, you should skim through them, and also use them as a reference during these exercises.
* Anthropic's [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html), <b>20 mins</b>
    * You should read up to & including the "Summary: A Hierarchy of Feature Properties" section.
    * The first few sections ("Key Results", "Definitions and Motivation", and "Empirical Phenomena" are particularly important).
    * We'll also be going through other parts of this paper as we work through the exercises.

## Questions

Here are a set of questions (with some brief answers provided) which you should be able to answer for yourself after reading the above material. Seach for them on Neel's Dynalist notes if you didn't come across them during your reading.

What is a **privileged basis**? Why should we expect neuron activations to be privileged by default? Why *shouldn't* we expect the residual stream to be privileged?

<details>
<summary>Answer</summary>

A privileged basis is one where the **standard basis directions are meaningful** (due to the structure of computation being done on that basis). This doesn't necessarily mean that the basis is interpretable.

**Neurons**

Neuron activations are privileged because of the **elementwise nonlinear function that gets applied**. ReLU is easily described in the standard basis, e.g. in 2D:

$$
\begin{bmatrix} x \\ y \end{bmatrix}
\to
\begin{bmatrix} \max(x, 0) \\ \max(y, 0) \end{bmatrix}
$$

but if you redefine a basis $x' = (x+y)/\sqrt{2}$, $y' = (x-y)/\sqrt{2}$, then describing ReLU in this new basis becomes really messy. More importantly, we now get interference between the components $x'$ and $y'$, i.e. the ReLU is no longer acting on them independently.

$$
\begin{bmatrix} x' \\ y' \end{bmatrix}
\to
\frac{1}{\sqrt{2}} \begin{bmatrix} \max(x, 0) + \max(y, 0) \\ \max(x, 0) - \max(y, 0) \end{bmatrix}
= 
\frac{1}{2} \begin{bmatrix} \max(x'+y', 0) + \max(x'-y', 0) \\ \max(x'+y', 0) - \max(x'-y', 0) \end{bmatrix}
$$

**Residual stream**

The residual stream is not privileged because anything that reads from it and writes to it uses a linear map. As a thought experiment, if we changed all the writing matrices (i.e. $W_{out}$ in the MLP layers and $W_O$ in the attention layers) to $W \to W R$, and all the reading matrices (i.e. $W_{in}$ in the MLP layers and $W_Q$, $W_K$, $W_V$ in the attention layers) to $W \to W R^{-1}$ where $R$ is some arbitrary rotation matrix, then the model's computation would be unchanged. Since the matrix $R$ is arbitrary, it can change the basis in any way it wants, so that basis can't be privileged.

To put this another way - if you claimed "I think the 47th element of the residual stream encoded some special information e.g. the plurality of the noun at that sequence position", I could call bullshit on your claim, because this thought experiment shows that any basis direction could just as easily be rotated & distributed as a linear combination of several different basis directions without fundamentally changing the computation done by the transformer. The same does not apply to neurons, because a rotation / change of basis would change the nature of computation done on them.

**Summary**

**Something is a privileged basis if it is not rotation-independent**, i.e. the nature of computation done on it means that the **basis directions have some special significance.**

Common misconception: privileged basis is equivalent to interpretable basis. This is **NOT true** (although it is the case that a basis must be privileged if the individual basis directions have some interpretable meaning; this is necessary but not sufficient).

</details>

What is the difference between **superposition** and **polysemanticity**?

<details>
<summary>Answer</summary>

Polysemanticity happens when one neuron corresponds to multiple features (see [here](https://distill.pub/2020/circuits/zoom-in/#:~:text=lot%20of%20effort.-,Polysemantic%20Neurons,-This%20essay%20may) for more discussion & examples). If we only had polysemanticity, this wouldn't really be a problem for us (there might exist a basis for features s.t. each basis vector corresponds to a single feature).

Superposition is when there are **more features than dimensions**. So it implies polysemanticity (because we must have dimensions representing more than one feature), but the converse is not true.

</details>


What are the **importance** and **sparsity** of features? Do you expect more or less polysemantic neurons if sparsity is larger?

<details>
<summary>Answer</summary>

**Importance** = how useful is this feature for achieving lower loss?

**Sparsity** = how frequently is it in the input data?

If sparsity is larger, then we expect more polysemantic neurons. This is because a single neuron can afford to represent several different sparse features (usually it'll only be representing one of them at any given time, so there won't be interference).
</details>

How would you define a **feature**?

<details>
<summary>Answer</summary>

There's no single correct answer to this. Many of the definitions are unfortunately circular (e.g. "a feature is a thing which could be represented by a neuron"). A few possible definitions are this one from Neel's [Dynalist notes](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#q=feature):

> A feature is a property of an input to the model, or some subset of that input (eg a token in the prompt given to a language model, or a patch of an image).

or this similar one from Chris Olah's [Distill Circuits Thread](https://distill.pub/2020/circuits/zoom-in/):

> A feature is a a scalar function of the input. In this essay, neural network features are directions, and often simply individual neurons. We claim such features in neural networks are typically meaningful features which can be rigorously studied. A **meaningful feature** is one that genuinely responds to an articulable property of the input, such as the presence of a curve or a floppy ear.
</details>

## Setup

```python
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
import sys
import torch as t
from torch import nn, Tensor
from torch.distributions.categorical import Categorical
from torch.nn import functional as F
from dataclasses import dataclass
import numpy as np
import einops
from jaxtyping import Float, Int
from typing import Optional, Callable, Union, List, Tuple
from functools import partial
from tqdm.notebook import tqdm
from dataclasses import dataclass
from rich import print as rprint
from rich.table import Table
from IPython.display import display, HTML
from pathlib import Path

# Make sure exercises are in the path
chapter = r"chapter1_transformer_interp"
exercises_dir = Path(f"{os.getcwd().split(chapter)[0]}/{chapter}/exercises").resolve()
section_dir = exercises_dir / "part4_superposition_and_saes"
if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))

from plotly_utils import imshow, line, hist
from part4_superposition_and_saes.utils import (
    plot_features_in_2d,
    plot_features_in_Nd,
    plot_features_in_Nd_discrete,
    plot_correlated_features,
    plot_feature_geometry,
    frac_active_line_plot,
)
import part4_superposition_and_saes.tests as tests
import part4_superposition_and_saes.solutions as solutions

device = t.device("cuda" if t.cuda.is_available() else "cpu")

MAIN = __name__ == "__main__"
```

""", unsafe_allow_html=True)